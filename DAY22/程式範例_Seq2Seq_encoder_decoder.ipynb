{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"程式範例_Seq2Seq_encoder_decoder.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ul0OnrdETGgp"},"source":["[連結文字](https://)\n","# 範例 : 實作 Seq2Seq Encoder-Decoder 模型\n","***\n","## [學習目標]\n","\n","用 pytorch 搭建 Seq2Seq Encoder-Decoder 模型\n","\n","## [學習重點]\n","\n","*   資料向量化\n","*   使用 GRU 建構 Encoder: VanillaEncoder\n","*   使用 GRU 建構 Decoder: VanillaDecoder\n","*   搭建 Sequence to Sequence 模型: Seq2Seq\n","*   實作 Trainer 類別\n","*   撰寫訓練函式\n","*   撰寫測試函式\n"]},{"cell_type":"markdown","metadata":{"id":"3zzt1zfsPOLT"},"source":["## 資料向量化\n","\n","一開始我們必須把字元作預處理的向量化，所有的程式已經在 DataHelper.py ，裡面有兩個類別，Vocabulary 和 DataTransformer。Vocabulary 這個類別，的主要功能是給予字元一個 index 的編碼動作:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tdoum2VqU5o-","executionInfo":{"status":"ok","timestamp":1610886541834,"user_tz":-480,"elapsed":1089,"user":{"displayName":"TJ Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD8ZNogaI1a7JvaPCP0sixRAUqrNuHydZOR0qLPQ=s64","userId":"07122505407861381538"}},"outputId":"3e526673-5579-45c4-bfea-7d2f68b7b6df"},"source":["!python DataHelper.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["python3: can't open file 'DataHelper.py': [Errno 2] No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ijh6BkaJp1pM"},"source":["import numpy as np\n","import datetime\n","import os\n","import requests\n","import pandas as pd\n","import re\n","import itertools"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6l076Nmwphvi"},"source":["PAD_ID = 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SWSbKXJyQyMb"},"source":["### Encoder"]},{"cell_type":"code","metadata":{"id":"ii0KBfjxreRj"},"source":["# model/Encoder.py\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class VanillaEncoder(nn.Module):\n","   def __init__(self, vocab_size, embedding_size, output_size):\n","       \"\"\"Define layers for a vanilla rnn encoder\"\"\"\n","       super(VanillaEncoder, self).__init__()\n","       self.vocab_size = vocab_size\n","       self.embedding = nn.Embedding(vocab_size, embedding_size)\n","       self.gru = nn.GRU(embedding_size, output_size)\n","\n","\n","   def forward(self, input_seqs, input_lengths, hidden=None):\n","       embedded = self.embedding(input_seqs)\n","       packed = pack_padded_sequence(embedded, input_lengths)\n","       packed_outputs, hidden = self.gru(packed, hidden)\n","       outputs, output_lengths = pad_packed_sequence(packed_outputs)\n","       return outputs, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CH3YmuyFQ9j_"},"source":["### Decoder"]},{"cell_type":"code","metadata":{"id":"CqzxrX2Vr0j-"},"source":["import random\n","import torch\n","import torch.nn as nn\n","\n","from torch.autograd import Variable\n","\n","class VanillaDecoder(nn.Module):\n","\n","    def __init__(self, hidden_size, output_size, max_length, teacher_forcing_ratio, sos_id, use_cuda):\n","        \"\"\"Define layers for a vanilla rnn decoder\"\"\"\n","        super(VanillaDecoder, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.log_softmax = nn.LogSoftmax(dim=-1)  # work with NLLLoss = CrossEntropyLoss\n","\n","        self.max_length = max_length\n","        self.teacher_forcing_ratio = teacher_forcing_ratio\n","        self.sos_id = sos_id\n","        self.use_cuda = use_cuda\n","\n","    def forward_step(self, inputs, hidden):\n","        # inputs: (time_steps=1, batch_size)\n","        batch_size = inputs.size(1)\n","        embedded = self.embedding(inputs)\n","        embedded.view(1, batch_size, self.hidden_size)  # S = T(1) x B x N\n","        rnn_output, hidden = self.gru(embedded, hidden)  # S = T(1) x B x H\n","        rnn_output = rnn_output.squeeze(0)  # squeeze the time dimension\n","        output = self.log_softmax(self.out(rnn_output))  # S = B x O\n","        return output, hidden\n","\n","    def forward(self, context_vector, targets):\n","\n","        # Prepare variable for decoder on time_step_0\n","        target_vars, target_lengths = targets\n","        batch_size = context_vector.size(1)\n","        decoder_input = Variable(torch.LongTensor([[self.sos_id] * batch_size]))\n","\n","        # Pass the context vector\n","        decoder_hidden = context_vector\n","\n","        max_target_length = max(target_lengths)\n","        decoder_outputs = Variable(torch.zeros(\n","            max_target_length,\n","            batch_size,\n","            self.output_size\n","        ))  # (time_steps, batch_size, vocab_size)\n","\n","        if self.use_cuda:\n","            decoder_input = decoder_input.cuda()\n","            decoder_outputs = decoder_outputs.cuda()\n","\n","        use_teacher_forcing = True if random.random() > self.teacher_forcing_ratio else False\n","\n","        # Unfold the decoder RNN on the time dimension\n","        for t in range(max_target_length):\n","            decoder_outputs_on_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n","            decoder_outputs[t] = decoder_outputs_on_t\n","            if use_teacher_forcing:\n","                decoder_input = target_vars[t].unsqueeze(0)\n","            else:\n","                decoder_input = self._decode_to_index(decoder_outputs_on_t)\n","\n","        return decoder_outputs, decoder_hidden\n","\n","    def evaluate(self, context_vector):\n","        batch_size = context_vector.size(1) # get the batch size\n","        decoder_input = Variable(torch.LongTensor([[self.sos_id] * batch_size]))\n","        decoder_hidden = context_vector\n","\n","        decoder_outputs = Variable(torch.zeros(\n","            self.max_length,\n","            batch_size,\n","            self.output_size\n","        ))  # (time_steps, batch_size, vocab_size)\n","\n","        if self.use_cuda:\n","            decoder_input = decoder_input.cuda()\n","            decoder_outputs = decoder_outputs.cuda()\n","\n","        # Unfold the decoder RNN on the time dimension\n","        for t in range(self.max_length):\n","            decoder_outputs_on_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n","            decoder_outputs[t] = decoder_outputs_on_t\n","            decoder_input = self._decode_to_index(decoder_outputs_on_t)  # select the former output as input\n","\n","        return self._decode_to_indices(decoder_outputs)\n","\n","    def _decode_to_index(self, decoder_output):\n","        \"\"\"\n","        evaluate on the logits, get the index of top1\n","        :param decoder_output: S = B x V or T x V\n","        \"\"\"\n","        value, index = torch.topk(decoder_output, 1)\n","        index = index.transpose(0, 1)  # S = 1 x B, 1 is the index of top1 class\n","        if self.use_cuda:\n","            index = index.cuda()\n","        return index\n","\n","    def _decode_to_indices(self, decoder_outputs):\n","        \"\"\"\n","        Evaluate on the decoder outputs(logits), find the top 1 indices.\n","        Please confirm that the model is on evaluation mode if dropout/batch_norm layers have been added\n","        :param decoder_outputs: the output sequence from decoder, shape = T x B x V \n","        \"\"\"\n","        decoded_indices = []\n","        batch_size = decoder_outputs.size(1)\n","        decoder_outputs = decoder_outputs.transpose(0, 1)  # S = B x T x V\n","\n","        for b in range(batch_size):\n","            top_ids = self._decode_to_index(decoder_outputs[b])\n","            decoded_indices.append(top_ids.data[0].cpu().numpy())\n","        return decoded_indices\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRzW4Iu8RDNw"},"source":["### Sequence to Sequence Model\n","\n"]},{"cell_type":"code","metadata":{"id":"gtcjwvF5ts3T"},"source":["class Seq2Seq(nn.Module):\n","\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, inputs, targets):\n","        input_vars, input_lengths = inputs\n","        encoder_outputs, encoder_hidden = self.encoder.forward(input_vars, input_lengths)\n","        decoder_outputs, decoder_hidden = self.decoder.forward(context_vector=encoder_hidden, targets=targets)\n","        return decoder_outputs, decoder_hidden\n","\n","    def evaluate(self, inputs):\n","        input_vars, input_lengths = inputs\n","        encoder_outputs, encoder_hidden = self.encoder(input_vars, input_lengths)\n","        decoded_sentence = self.decoder.evaluate(context_vector=encoder_hidden)\n","        return decoded_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKJsCJCaRkjo"},"source":["### Hyper Parameters"]},{"cell_type":"code","metadata":{"id":"bycctYBkRj0a"},"source":["use_cuda = True if torch.cuda.is_available() else False\n","\n","# for dataset\n","dataset_path = './Google-10000-English.txt'\n","\n","# for training\n","num_epochs = 10\n","batch_size = 128\n","learning_rate = 1e-3\n","\n","# for model\n","encoder_embedding_size = 256\n","encoder_output_size = 256\n","decoder_hidden_size = encoder_output_size\n","teacher_forcing_ratio = .5\n","# max_length = 20\n","\n","# for logging\n","checkpoint_name = 'auto_encoder.pt'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K8dOyKFeRMiY"},"source":["### Trainer"]},{"cell_type":"code","metadata":{"id":"o9CAB1n7tuzY"},"source":["class Trainer(object):\n","\n","    def __init__(self, model, data_transformer, learning_rate, use_cuda,\n","                 checkpoint_name=checkpoint_name,\n","                 teacher_forcing_ratio=teacher_forcing_ratio):\n","\n","        self.model = model\n","\n","        # record some information about dataset\n","        self.data_transformer = data_transformer\n","        self.vocab_size = self.data_transformer.vocab_size\n","        self.PAD_ID = self.data_transformer.PAD_ID\n","        self.use_cuda = use_cuda\n","\n","        # optimizer setting\n","        self.learning_rate = learning_rate\n","        self.optimizer= torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n","        self.criterion = torch.nn.NLLLoss(ignore_index=self.PAD_ID, reduction='mean')\n","\n","        self.checkpoint_name = checkpoint_name\n","\n","    def train(self, num_epochs, batch_size, pretrained=False):\n","\n","        if pretrained:\n","            self.load_model()\n","\n","        step = 0\n","\n","        for epoch in range(0, num_epochs):\n","            mini_batches = self.data_transformer.mini_batches(batch_size=batch_size)\n","            for input_batch, target_batch in mini_batches:\n","                self.optimizer.zero_grad()\n","                decoder_outputs, decoder_hidden = self.model(input_batch, target_batch)\n","\n","                # calculate the loss and back prop.\n","                cur_loss = self.get_loss(decoder_outputs, target_batch[0])\n","\n","                # logging\n","                step += 1\n","                if step % 50 == 0:\n","                    print(\"Step:\", step, \"char-loss: \", cur_loss.data.numpy())\n","                    self.save_model()\n","                cur_loss.backward()\n","\n","                # optimize\n","                self.optimizer.step()\n","\n","        self.save_model()\n","\n","    def masked_nllloss(self):\n","        # Deprecated in PyTorch 2.0, can be replaced by ignore_index\n","        # define the masked NLLoss\n","        weight = torch.ones(self.vocab_size)\n","        weight[self.PAD_ID] = 0\n","        if self.use_cuda:\n","            weight = weight.cuda()\n","        return torch.nn.NLLLoss(weight=weight).cuda()\n","\n","    def get_loss(self, decoder_outputs, targets):\n","        b = decoder_outputs.size(1)\n","        t = decoder_outputs.size(0)\n","        targets = targets.contiguous().view(-1)  # S = (B*T)\n","        decoder_outputs = decoder_outputs.view(b * t, -1)  # S = (B*T) x V\n","        return self.criterion(decoder_outputs, targets)\n","\n","    def save_model(self):\n","        torch.save(self.model.state_dict(), self.checkpoint_name)\n","        print(\"Model has been saved as %s.\\n\" % self.checkpoint_name)\n","\n","    def load_model(self):\n","        self.model.load_state_dict(torch.load(self.checkpoint_name, map_location='cpu'))\n","        print(\"Pretrained model has been loaded.\\n\")\n","\n","    def tensorboard_log(self):\n","        pass\n","\n","    def evaluate(self, words):\n","        # make sure that words is list\n","        if type(words) is not list:\n","            words = [words]\n","\n","        # transform word to index-sequence\n","        eval_var = self.data_transformer.evaluation_batch(words=words)\n","        decoded_indices = self.model.evaluate(eval_var)\n","        results = []\n","        for indices in decoded_indices:\n","            results.append(self.data_transformer.vocab.indices_to_sequence(indices))\n","        return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cwrF-OPQpt3y"},"source":["import torch\n","import numpy as np\n","\n","from torch.autograd import Variable\n","\n","class Vocabulary(object):\n","\n","    def __init__(self):\n","        self.char2idx = {'SOS': 0, 'EOS': 1, 'PAD': 2, 'UNK': 3}\n","        self.idx2char = {0: 'SOS', 1: 'EOS', 2: 'PAD', 3: 'UNK'}\n","        self.num_chars = 4\n","        self.max_length = 0\n","        self.word_list = []\n","\n","    def build_vocab(self, data_path):\n","        \"\"\"Construct the relation between words and indices\"\"\"\n","        with open(data_path, 'r', encoding='utf-8') as dataset:\n","            for word in dataset:\n","                word = word.strip('\\n')\n","\n","                self.word_list.append(word)\n","                if self.max_length < len(word):\n","                    self.max_length = len(word)\n","\n","                chars = self.split_sequence(word)\n","                for char in chars:\n","                    if char not in self.char2idx:\n","                        self.char2idx[char] = self.num_chars\n","                        self.idx2char[self.num_chars] = char\n","                        self.num_chars += 1\n","\n","    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n","        \"\"\"Transform a char sequence to index sequence\n","            :param sequence: a string composed with chars\n","            :param add_eos: if true, add the <EOS> tag at the end of given sentence\n","            :param add_sos: if true, add the <SOS> tag at the beginning of given sentence\n","        \"\"\"\n","        index_sequence = [self.char2idx['SOS']] if add_sos else []\n","\n","        for char in self.split_sequence(sequence):\n","            if char not in self.char2idx:\n","                index_sequence.append((self.char2idx['UNK']))\n","            else:\n","                index_sequence.append(self.char2idx[char])\n","\n","        if add_eos:\n","            index_sequence.append(self.char2idx['EOS'])\n","\n","        return index_sequence\n","\n","    def indices_to_sequence(self, indices):\n","        \"\"\"Transform a list of indices\n","            :param indices: a list\n","        \"\"\"\n","        sequence = \"\"\n","        for idx in indices:\n","            char = self.idx2char[idx]\n","            if char == \"EOS\":\n","                break\n","            else:\n","                sequence += char\n","        return sequence\n","\n","    def split_sequence(self, sequence):\n","        \"\"\"Vary from languages and tasks. In our task, we simply return chars in given sentence\n","        For example:\n","            Input : alphabet\n","            Return: [a, l, p, h, a, b, e, t]\n","        \"\"\"\n","        return [char for char in sequence]\n","\n","    def __str__(self):\n","        str = \"Vocab information:\\n\"\n","        for idx, char in self.idx2char.items():\n","            str += \"Char: %s Index: %d\\n\" % (char, idx)\n","        return str\n","\n","class DataTransformer(object):\n","\n","    def __init__(self, path, use_cuda):\n","        self.indices_sequences = []\n","        self.use_cuda = use_cuda\n","\n","        # Load and build the vocab\n","        self.vocab = Vocabulary()\n","        self.vocab.build_vocab(path)\n","        self.PAD_ID = self.vocab.char2idx[\"PAD\"]\n","        self.SOS_ID = self.vocab.char2idx[\"SOS\"]\n","        self.vocab_size = self.vocab.num_chars\n","        self.max_length = self.vocab.max_length\n","\n","        self._build_training_set(path)\n","\n","    def _build_training_set(self, path):\n","        # Change sentences to indices, and append <EOS> at the end of all pairs\n","        for word in self.vocab.word_list:\n","            indices_seq = self.vocab.sequence_to_indices(word, add_eos=True)\n","            # input and target are the same in auto-encoder\n","            self.indices_sequences.append([indices_seq, indices_seq[:]])\n","\n","    def mini_batches(self, batch_size):\n","        input_batches = []\n","        target_batches = []\n","\n","        np.random.shuffle(self.indices_sequences)\n","        mini_batches = [\n","            self.indices_sequences[k: k + batch_size]\n","            for k in range(0, len(self.indices_sequences), batch_size)\n","        ]\n","\n","        for batch in mini_batches:\n","            seq_pairs = sorted(batch, key=lambda seqs: len(seqs[0]), reverse=True)  # sorted by input_lengths\n","            input_seqs = [pair[0] for pair in seq_pairs]\n","            target_seqs = [pair[1] for pair in seq_pairs]\n","\n","            input_lengths = [len(s) for s in input_seqs]\n","            in_max = input_lengths[0]\n","            input_padded = [self.pad_sequence(s, in_max) for s in input_seqs]\n","\n","            target_lengths = [len(s) for s in target_seqs]\n","            out_max = target_lengths[0]\n","            target_padded = [self.pad_sequence(s, out_max) for s in target_seqs]\n","\n","            input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)  # time * batch\n","            target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)  # time * batch\n","\n","            if self.use_cuda:\n","                input_var = input_var.cuda()\n","                target_var = target_var.cuda()\n","\n","            yield (input_var, input_lengths), (target_var, target_lengths)\n","\n","    def pad_sequence(self, sequence, max_length):\n","        sequence += [self.PAD_ID for i in range(max_length - len(sequence))]\n","        return sequence\n","\n","    def evaluation_batch(self, words):\n","        \"\"\"\n","        Prepare a batch of var for evaluating\n","        :param words: a list, store the testing data \n","        :return: evaluation_batch\n","        \"\"\"\n","        evaluation_batch = []\n","\n","        for word in words:\n","            indices_seq = self.vocab.sequence_to_indices(word, add_eos=True)\n","            evaluation_batch.append([indices_seq])\n","\n","        seq_pairs = sorted(evaluation_batch, key=lambda seqs: len(seqs[0]), reverse=True)\n","        input_seqs = [pair[0] for pair in seq_pairs]\n","        input_lengths = [len(s) for s in input_seqs]\n","        in_max = input_lengths[0]\n","        input_padded = [self.pad_sequence(s, in_max) for s in input_seqs]\n","\n","        input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)  # time * batch\n","\n","        if self.use_cuda:\n","            input_var = input_var.cuda()\n","\n","        return input_var, input_lengths"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-E2ZUhCmRsoF"},"source":["### Method of Training "]},{"cell_type":"code","metadata":{"id":"UI7ddhvmpqsB"},"source":["def train():\n","    data_transformer = DataTransformer(dataset_path, use_cuda=use_cuda)\n","\n","    # define our models\n","    vanilla_encoder = VanillaEncoder(vocab_size=data_transformer.vocab_size,\n","                                     embedding_size=encoder_embedding_size,\n","                                     output_size=encoder_output_size)\n","\n","    vanilla_decoder = VanillaDecoder(hidden_size=decoder_hidden_size,\n","                                     output_size=data_transformer.vocab_size,\n","                                     max_length=data_transformer.max_length,\n","                                     teacher_forcing_ratio=teacher_forcing_ratio,\n","                                     sos_id=data_transformer.SOS_ID,\n","                                     use_cuda=use_cuda)\n","    if use_cuda:\n","        vanilla_encoder = vanilla_encoder.cuda()\n","        vanilla_decoder = vanilla_decoder.cuda()\n","\n","\n","    seq2seq = Seq2Seq(encoder=vanilla_encoder,\n","                      decoder=vanilla_decoder)\n","\n","    trainer = Trainer(seq2seq, data_transformer, learning_rate, use_cuda)\n","    trainer.train(num_epochs=num_epochs, batch_size=batch_size, pretrained=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uS7ES48QpwD4","executionInfo":{"status":"ok","timestamp":1610886824575,"user_tz":-480,"elapsed":283807,"user":{"displayName":"TJ Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD8ZNogaI1a7JvaPCP0sixRAUqrNuHydZOR0qLPQ=s64","userId":"07122505407861381538"}},"outputId":"8ea5e8f6-b0db-42e1-9905-0bf91f61901e"},"source":["train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Step: 50 char-loss:  2.209176\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 100 char-loss:  1.6808023\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 150 char-loss:  1.4202219\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 200 char-loss:  0.68837035\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 250 char-loss:  0.48758346\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 300 char-loss:  0.6792467\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 350 char-loss:  0.2711418\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 400 char-loss:  0.4148103\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 450 char-loss:  0.32217252\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 500 char-loss:  0.15307778\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 550 char-loss:  0.26541898\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 600 char-loss:  0.2628817\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 650 char-loss:  0.073865265\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 700 char-loss:  0.09037264\n","Model has been saved as auto_encoder.pt.\n","\n","Step: 750 char-loss:  0.085165806\n","Model has been saved as auto_encoder.pt.\n","\n","Model has been saved as auto_encoder.pt.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gcvmAkLyRyLg"},"source":["### Method of Test"]},{"cell_type":"code","metadata":{"id":"mcG1GUIUtzQ4"},"source":["def test():\n","    data_transformer = DataTransformer(dataset_path, use_cuda=use_cuda)\n","\n","    vanilla_encoder = VanillaEncoder(vocab_size=data_transformer.vocab_size,\n","                                     embedding_size=encoder_embedding_size,\n","                                     output_size=encoder_output_size)\n","\n","    vanilla_decoder = VanillaDecoder(hidden_size=decoder_hidden_size,\n","                                     output_size=data_transformer.vocab_size,\n","                                     max_length=data_transformer.max_length,\n","                                     teacher_forcing_ratio=teacher_forcing_ratio,\n","                                     sos_id=data_transformer.SOS_ID,\n","                                     use_cuda=use_cuda)\n","    if use_cuda:\n","        vanilla_encoder = vanilla_encoder.cuda()\n","        vanilla_decoder = vanilla_decoder.cuda()\n","\n","    seq2seq = Seq2Seq(encoder=vanilla_encoder,\n","                      decoder=vanilla_decoder)\n","\n","    trainer = Trainer(seq2seq, data_transformer, learning_rate, use_cuda)\n","    trainer.load_model()\n","\n","    while(True):\n","        testing_word = input('You say: ')\n","        if testing_word == \"exit\":\n","            break\n","        results = trainer.evaluate(testing_word)\n","        print(\"Model says: %s\" % results[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vx4HrUSIt_c0","executionInfo":{"status":"ok","timestamp":1610886937743,"user_tz":-480,"elapsed":396968,"user":{"displayName":"TJ Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD8ZNogaI1a7JvaPCP0sixRAUqrNuHydZOR0qLPQ=s64","userId":"07122505407861381538"}},"outputId":"21cd4619-4956-44c7-9b76-d870c09d2132"},"source":["test()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Pretrained model has been loaded.\n","\n","You say: hello\n","Model says: helloo\n","You say: test\n","Model says: tests\n","You say: exit\n"],"name":"stdout"}]}]}