{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "作業_Seq2Seq_translator_en_de.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrWWUhFlUprI"
      },
      "source": [
        "# 作業 : 實作英文-德文翻譯機器人\n",
        "***\n",
        "## [作業目標]\n",
        "\n",
        "用 pytorch 實作一個英文-德文翻譯機器人\n",
        "\n",
        "## [作業目標]\n",
        "\n",
        "*   語言資料處理\n",
        "*   使用 LSTM 建構 Encoder: EncoderLSTM\n",
        "*   使用 LSTM 建構 Decoder: DecoderLSTM\n",
        "*   搭建 Sequence to Sequence 模型: Seq2Seq\n",
        "*   撰寫訓練函式\n",
        "*   撰寫測試函式\n",
        "\n",
        "## [問題]\n",
        "\n",
        "在 Colab 實際上執行完這個範例後，請改用 BiLSTM 來建構 Encoder 與 Decoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgPfcYR26SxF"
      },
      "source": [
        "## 安裝 spacy\n",
        "\n",
        "We'll also make use of spaCy to tokenize our data. To install spaCy, follow the instructions here making sure to install both the English and German models with:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyd4LijE7vGo"
      },
      "source": [
        "## 引用需要的模組"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RanKHsWTu-rn"
      },
      "source": [
        "import os\n",
        "import spacy\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "from torch import nn, optim\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9h9rmgr74Sk"
      },
      "source": [
        "## 下載英文預料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyskUGjGSr-0",
        "outputId": "95d87640-acc7-40c6-b119-675cc4cacf33"
      },
      "source": [
        "!mkdir ./data\n",
        "!mkdir ./data/multi30k\n",
        "!python -m spacy download en\n",
        "!ls ./data/multi30k -al\n",
        "spacy_english = spacy.load(\"en_core_web_sm\")\n",
        "!ls ./data/multi30k -al"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./data’: File exists\n",
            "mkdir: cannot create directory ‘./data/multi30k’: File exists\n",
            "2021-02-03 12:19:03.986623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Pleaseuse the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Requirement already satisfied: en-core-web-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl#egg=en_core_web_sm==3.0.0 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from en-core-web-sm==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.2)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (53.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.8)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: dataclasses<1.0,>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8)\n",
            "Requirement already satisfied: contextvars<3,>=2.4; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from thinc<8.1.0,>=8.0.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars<3,>=2.4; python_version < \"3.7\"->thinc<8.1.0,>=8.0.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.14)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Feb  3 11:29 .\n",
            "drwxr-xr-x 3 root root 4096 Feb  3 11:29 ..\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Feb  3 11:29 .\n",
            "drwxr-xr-x 3 root root 4096 Feb  3 11:29 ..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBestLOF8L4w"
      },
      "source": [
        "## 下載德語語料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Hjr_6AXTRoz",
        "outputId": "3c1ccca0-f1cb-498c-f722-9325c8ca9b07"
      },
      "source": [
        "!python -m spacy download de\n",
        "spacy_de = spacy.load(\"de_core_news_sm\")\n",
        "!ls ./data/multi30k -al"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-03 12:19:14.823946: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Pleaseuse the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "Requirement already satisfied: de-core-news-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.0.0/de_core_news_sm-3.0.0-py3-none-any.whl#egg=de_core_news_sm==3.0.0 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from de-core-news-sm==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.11.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (8.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (53.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.8.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (20.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: dataclasses<1.0,>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.8)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: contextvars<3,>=2.4; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from thinc<8.1.0,>=8.0.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars<3,>=2.4; python_version < \"3.7\"->thinc<8.1.0,>=8.0.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.14)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Feb  3 11:29 .\n",
            "drwxr-xr-x 3 root root 4096 Feb  3 11:29 ..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK984GbYv8Y-",
        "outputId": "40718ebd-7b39-4562-b7c3-bfbcb1075592"
      },
      "source": [
        "def tokenize_de(text):\n",
        "  return [token.text for token in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_english(text):\n",
        "  return [token.text for token in spacy_english.tokenizer(text)]\n",
        "\n",
        "### Sample Run ###\n",
        "\n",
        "sample_text = \"I love machine learning\"\n",
        "print(tokenize_english(sample_text))\n",
        "\n",
        "german = Field(tokenize=tokenize_de, lower=True,\n",
        "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "english = Field(tokenize=tokenize_english, lower=True,\n",
        "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = (\".en\", \".en\"),\n",
        "                                                    fields=(german, english))\n",
        "\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=3)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=3)\n",
        "\n",
        "print(f\"Unique tokens in source (german) vocabulary: {len(german.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(english.vocab)}\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'love', 'machine', 'learning']\n",
            "Unique tokens in source (german) vocabulary: 4587\n",
            "Unique tokens in target (en) vocabulary: 4556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WunTmSIJzBaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f037d4b3-4ccc-4254-b94e-addf3337bccd"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
        "\n",
        "print(train_data[5].__dict__.keys())\n",
        "pprint(train_data[5].__dict__.values())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n",
            "dict_keys(['src', 'trg'])\n",
            "dict_values([['a', 'man', 'in', 'green', 'holds', 'a', 'guitar', 'while', 'the', 'other', 'man', 'observes', 'his', 'shirt', '.'], ['a', 'man', 'in', 'green', 'holds', 'a', 'guitar', 'while', 'the', 'other', 'man', 'observes', 'his', 'shirt', '.']])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRGP9EsizRRN"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
        "                                                                      batch_size = BATCH_SIZE, \n",
        "                                                                      sort_within_batch=True,\n",
        "                                                                      sort_key=lambda x: len(x.src),\n",
        "                                                                      device = device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3nozOT8zdeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a03d8aa-30fa-4dfd-9e37-d82b8d28165d"
      },
      "source": [
        "max_len_eng = []\n",
        "max_len_ger = []\n",
        "for counter, data in enumerate(train_data):\n",
        "  max_len_ger.append(len(data.src))\n",
        "  max_len_eng.append(len(data.trg))\n",
        "  if counter < 10 :\n",
        "    print(\"German - \",*data.src, \" Length - \", len(data.src))\n",
        "    print(\"English - \",*data.trg, \" Length - \", len(data.trg))\n",
        "    print()\n",
        "\n",
        "\n",
        "print(\"Maximum Length of English sentence {} and German sentence {} in the dataset\".format(max(max_len_eng),max(max_len_ger)))\n",
        "print(\"Minimum Length of English sentence {} and German sentence {} in the dataset\".format(min(max_len_eng),min(max_len_ger)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German -  two young , white males are outside near many bushes .  Length -  11\n",
            "English -  two young , white males are outside near many bushes .  Length -  11\n",
            "\n",
            "German -  several men in hard hats are operating a giant pulley system .  Length -  12\n",
            "English -  several men in hard hats are operating a giant pulley system .  Length -  12\n",
            "\n",
            "German -  a little girl climbing into a wooden playhouse .  Length -  9\n",
            "English -  a little girl climbing into a wooden playhouse .  Length -  9\n",
            "\n",
            "German -  a man in a blue shirt is standing on a ladder cleaning a window .  Length -  15\n",
            "English -  a man in a blue shirt is standing on a ladder cleaning a window .  Length -  15\n",
            "\n",
            "German -  two men are at the stove preparing food .  Length -  9\n",
            "English -  two men are at the stove preparing food .  Length -  9\n",
            "\n",
            "German -  a man in green holds a guitar while the other man observes his shirt .  Length -  15\n",
            "English -  a man in green holds a guitar while the other man observes his shirt .  Length -  15\n",
            "\n",
            "German -  a man is smiling at a stuffed lion  Length -  8\n",
            "English -  a man is smiling at a stuffed lion  Length -  8\n",
            "\n",
            "German -  a trendy girl talking on her cellphone while gliding slowly down the street .  Length -  14\n",
            "English -  a trendy girl talking on her cellphone while gliding slowly down the street .  Length -  14\n",
            "\n",
            "German -  a woman with a large purse is walking by a gate .  Length -  12\n",
            "English -  a woman with a large purse is walking by a gate .  Length -  12\n",
            "\n",
            "German -  boys dancing on poles in the middle of the night .  Length -  11\n",
            "English -  boys dancing on poles in the middle of the night .  Length -  11\n",
            "\n",
            "Maximum Length of English sentence 41 and German sentence 40 in the dataset\n",
            "Minimum Length of English sentence 4 and German sentence 4 in the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE_S5yMdwRsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea75e76e-342a-4654-ee76-ee514b0a124c"
      },
      "source": [
        "count = 0\n",
        "for data in train_iterator:\n",
        "  if count < 1 :\n",
        "    print(\"Shapes\", data.src.shape, data.trg.shape)\n",
        "    print()\n",
        "    print(\"German - \",*data.src, \" Length - \", len(data.src))\n",
        "    print()\n",
        "    print(\"English - \",*data.trg, \" Length - \", len(data.trg))\n",
        "    temp_ger = data.src\n",
        "    temp_eng = data.trg\n",
        "    count += 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shapes torch.Size([10, 32]) torch.Size([12, 32])\n",
            "\n",
            "German -  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([   4,    4,    4, 1187,    4,   16,   46,  472,    4,    4,    4,    4,\n",
            "           4,    4,    4,   23, 1161,   21,    4,    4,  460,    4,   21,   19,\n",
            "         290,    4, 2565,   16,    4,  190,    4,  110], device='cuda:0') tensor([  51,   55,   32,  153,  167,  109,  100,    6,   23,   23,   15, 1554,\n",
            "         173,    9,   26,  100,    9,  109,    9,   38,  107,   15, 1626,   31,\n",
            "         357,    9,   11,   49,   34,  125,   32,   62], device='cuda:0') tensor([  32,  134,   90,  160,   97,   30,  983,  169,  150,   34,   31,   63,\n",
            "          32,  401,   35,  180,  456,   15,   10,   12,   17,   13,    9,    6,\n",
            "         695,    6,    7,    8,  174,    6,   10, 3493], device='cuda:0') tensor([  10,   13,    8,    6,    0,  116, 2783,   11,  961, 2721,  275,   36,\n",
            "          11,  126,  598,   73,    6,   10,   54,   19,  611,   24,   10,   42,\n",
            "         236,   21,  100,    4,  330,    4, 3977, 1520], device='cuda:0') tensor([  77,   43,    4,    4,   52,    8,    8,  225,    4, 3668,    4,    8,\n",
            "         714,    4,   75,  948,    4, 1012,   68,   37,    6,  211,  212,   12,\n",
            "           6,  499,  161, 1797,    4, 1526,    6,  192], device='cuda:0') tensor([   4, 4059,  306,  978,    7,    4,    4,  180, 2846,    4,  233,    4,\n",
            "          32,  247,    4,   12,   24,   43,    4,  860,    4,   10,    4,   71,\n",
            "           4, 1189,    0,  363, 1050,   52,    4,    4], device='cuda:0') tensor([ 153,  512,  276,  658, 1587,   83,  576,  266,  923,  216,  135,  162,\n",
            "          41,  240,  949,  323,  181,  194,  852, 1779,  160,  414, 1162,   19,\n",
            "         394, 1616,    0, 4381,   47,  428,  573,  578], device='cuda:0') tensor([   5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
            "           5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
            "           5,    5,    5, 3043,  412,    5,    5,    5], device='cuda:0') tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')  Length -  10\n",
            "\n",
            "English -  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([   4,    4,    4, 1192,    4,   16,   48,  480,    4,    4,    4,    4,\n",
            "           4,    4,    4,   24,  163,   21,    4,    4,  457,    4,   21,   19,\n",
            "         295,    4, 2560,   16,    4,  196,    4,  110], device='cuda:0') tensor([  53,   55,   33,  157,  170,  115,  104,    6,   24,   24,   14, 1506,\n",
            "         122,    9,   26,  104,   42,  115,    9,   38,  112,   14, 1634,   32,\n",
            "         362,    9,   11,   50,   34,  128,   33,   63], device='cuda:0') tensor([  33,  137,   91,  164,   99,   30,  992,  175,  154,   34,   32,   64,\n",
            "          33,  407,   35,  186,  210,   14,   10,   12,   17,   13,    9,    6,\n",
            "         705,    6,    7,    8,  179,    6,   10, 3482], device='cuda:0') tensor([  10,   13,    8,    6,    0,  119, 2787,   11,  972, 2723,  280,   36,\n",
            "          11,  129,  601,   74,    9,   10,   56,   19,  617,   25,   10,   43,\n",
            "         241,   21,  104,    4,  336,    4, 3960, 1531], device='cuda:0') tensor([  78,   44,    4,    4,   51,    8,    8,  208,    4, 3657,    4,    8,\n",
            "         719,    4,   76,  958,  465, 1021,   69,   37,    6,  217,  218,   12,\n",
            "           6,  502,  165, 1800,    4, 1356,    6,  199], device='cuda:0') tensor([   4,  665,  308,  990,    7,    4,    4,   42, 2854,    4,  238,    4,\n",
            "          33,  248,    4,   12,    6,   44,    4,  866,    4,   10,    4,   72,\n",
            "           4, 1195,    0,  363, 1054,   51,    4,    4], device='cuda:0') tensor([ 157,  102,  281,  664, 1598,   84,  578,   23,  929,  224,  138,  166,\n",
            "          41,  243,  959,  330,    4,  201,  859, 1782,  164,  419, 1165,   19,\n",
            "         398, 1625,    0, 4351,   47,  435,  575,  580], device='cuda:0') tensor([   5,  515,    5,    5,    5,    5,    5,  186,    5,    5,    5,    5,\n",
            "           5,    5,    5,    5,   25,    5,    5,    5,    5,    5,    5,    5,\n",
            "           5,    5,    5, 3044,  416,    5,    5,    5], device='cuda:0') tensor([  3,   5,   3,   3,   3,   3,   3, 270,   3,   3,   3,   3,   3,   3,\n",
            "          3,   3, 187,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
            "          3,   3,   3,   3], device='cuda:0') tensor([1, 3, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')  Length -  12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSP5RchXyuaz"
      },
      "source": [
        "temp_eng_idx = (temp_eng).cpu().detach().numpy()\n",
        "temp_ger_idx = (temp_ger).cpu().detach().numpy()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgAmQS4I6k9v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "a0cc3657-5c14-4ddb-e4dd-110b656e903d"
      },
      "source": [
        "df_eng_idx = pd.DataFrame(data = temp_eng_idx, columns = [str(\"S_\")+str(x + 1) for x in range(BATCH_SIZE)])\n",
        "df_eng_idx.index.name = 'Time Steps'\n",
        "df_eng_idx.index = df_eng_idx.index + 1 \n",
        "# df_eng_idx.to_csv('/content/idx.csv')\n",
        "df_eng_idx\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S_1</th>\n",
              "      <th>S_2</th>\n",
              "      <th>S_3</th>\n",
              "      <th>S_4</th>\n",
              "      <th>S_5</th>\n",
              "      <th>S_6</th>\n",
              "      <th>S_7</th>\n",
              "      <th>S_8</th>\n",
              "      <th>S_9</th>\n",
              "      <th>S_10</th>\n",
              "      <th>S_11</th>\n",
              "      <th>S_12</th>\n",
              "      <th>S_13</th>\n",
              "      <th>S_14</th>\n",
              "      <th>S_15</th>\n",
              "      <th>S_16</th>\n",
              "      <th>S_17</th>\n",
              "      <th>S_18</th>\n",
              "      <th>S_19</th>\n",
              "      <th>S_20</th>\n",
              "      <th>S_21</th>\n",
              "      <th>S_22</th>\n",
              "      <th>S_23</th>\n",
              "      <th>S_24</th>\n",
              "      <th>S_25</th>\n",
              "      <th>S_26</th>\n",
              "      <th>S_27</th>\n",
              "      <th>S_28</th>\n",
              "      <th>S_29</th>\n",
              "      <th>S_30</th>\n",
              "      <th>S_31</th>\n",
              "      <th>S_32</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time Steps</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1192</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>48</td>\n",
              "      <td>480</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>163</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>457</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>19</td>\n",
              "      <td>295</td>\n",
              "      <td>4</td>\n",
              "      <td>2560</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>196</td>\n",
              "      <td>4</td>\n",
              "      <td>110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>55</td>\n",
              "      <td>33</td>\n",
              "      <td>157</td>\n",
              "      <td>170</td>\n",
              "      <td>115</td>\n",
              "      <td>104</td>\n",
              "      <td>6</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>14</td>\n",
              "      <td>1506</td>\n",
              "      <td>122</td>\n",
              "      <td>9</td>\n",
              "      <td>26</td>\n",
              "      <td>104</td>\n",
              "      <td>42</td>\n",
              "      <td>115</td>\n",
              "      <td>9</td>\n",
              "      <td>38</td>\n",
              "      <td>112</td>\n",
              "      <td>14</td>\n",
              "      <td>1634</td>\n",
              "      <td>32</td>\n",
              "      <td>362</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>50</td>\n",
              "      <td>34</td>\n",
              "      <td>128</td>\n",
              "      <td>33</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>137</td>\n",
              "      <td>91</td>\n",
              "      <td>164</td>\n",
              "      <td>99</td>\n",
              "      <td>30</td>\n",
              "      <td>992</td>\n",
              "      <td>175</td>\n",
              "      <td>154</td>\n",
              "      <td>34</td>\n",
              "      <td>32</td>\n",
              "      <td>64</td>\n",
              "      <td>33</td>\n",
              "      <td>407</td>\n",
              "      <td>35</td>\n",
              "      <td>186</td>\n",
              "      <td>210</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>13</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>705</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>179</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>3482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>119</td>\n",
              "      <td>2787</td>\n",
              "      <td>11</td>\n",
              "      <td>972</td>\n",
              "      <td>2723</td>\n",
              "      <td>280</td>\n",
              "      <td>36</td>\n",
              "      <td>11</td>\n",
              "      <td>129</td>\n",
              "      <td>601</td>\n",
              "      <td>74</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>56</td>\n",
              "      <td>19</td>\n",
              "      <td>617</td>\n",
              "      <td>25</td>\n",
              "      <td>10</td>\n",
              "      <td>43</td>\n",
              "      <td>241</td>\n",
              "      <td>21</td>\n",
              "      <td>104</td>\n",
              "      <td>4</td>\n",
              "      <td>336</td>\n",
              "      <td>4</td>\n",
              "      <td>3960</td>\n",
              "      <td>1531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>78</td>\n",
              "      <td>44</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>51</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>208</td>\n",
              "      <td>4</td>\n",
              "      <td>3657</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>719</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>958</td>\n",
              "      <td>465</td>\n",
              "      <td>1021</td>\n",
              "      <td>69</td>\n",
              "      <td>37</td>\n",
              "      <td>6</td>\n",
              "      <td>217</td>\n",
              "      <td>218</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>502</td>\n",
              "      <td>165</td>\n",
              "      <td>1800</td>\n",
              "      <td>4</td>\n",
              "      <td>1356</td>\n",
              "      <td>6</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>665</td>\n",
              "      <td>308</td>\n",
              "      <td>990</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>2854</td>\n",
              "      <td>4</td>\n",
              "      <td>238</td>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>248</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>44</td>\n",
              "      <td>4</td>\n",
              "      <td>866</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>72</td>\n",
              "      <td>4</td>\n",
              "      <td>1195</td>\n",
              "      <td>0</td>\n",
              "      <td>363</td>\n",
              "      <td>1054</td>\n",
              "      <td>51</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>157</td>\n",
              "      <td>102</td>\n",
              "      <td>281</td>\n",
              "      <td>664</td>\n",
              "      <td>1598</td>\n",
              "      <td>84</td>\n",
              "      <td>578</td>\n",
              "      <td>23</td>\n",
              "      <td>929</td>\n",
              "      <td>224</td>\n",
              "      <td>138</td>\n",
              "      <td>166</td>\n",
              "      <td>41</td>\n",
              "      <td>243</td>\n",
              "      <td>959</td>\n",
              "      <td>330</td>\n",
              "      <td>4</td>\n",
              "      <td>201</td>\n",
              "      <td>859</td>\n",
              "      <td>1782</td>\n",
              "      <td>164</td>\n",
              "      <td>419</td>\n",
              "      <td>1165</td>\n",
              "      <td>19</td>\n",
              "      <td>398</td>\n",
              "      <td>1625</td>\n",
              "      <td>0</td>\n",
              "      <td>4351</td>\n",
              "      <td>47</td>\n",
              "      <td>435</td>\n",
              "      <td>575</td>\n",
              "      <td>580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5</td>\n",
              "      <td>515</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>186</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3044</td>\n",
              "      <td>416</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>270</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>187</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            S_1  S_2  S_3   S_4   S_5  S_6  ...  S_27  S_28  S_29  S_30  S_31  S_32\n",
              "Time Steps                                  ...                                    \n",
              "1             2    2    2     2     2    2  ...     2     2     2     2     2     2\n",
              "2             4    4    4  1192     4   16  ...  2560    16     4   196     4   110\n",
              "3            53   55   33   157   170  115  ...    11    50    34   128    33    63\n",
              "4            33  137   91   164    99   30  ...     7     8   179     6    10  3482\n",
              "5            10   13    8     6     0  119  ...   104     4   336     4  3960  1531\n",
              "6            78   44    4     4    51    8  ...   165  1800     4  1356     6   199\n",
              "7             4  665  308   990     7    4  ...     0   363  1054    51     4     4\n",
              "8           157  102  281   664  1598   84  ...     0  4351    47   435   575   580\n",
              "9             5  515    5     5     5    5  ...     5  3044   416     5     5     5\n",
              "10            3    5    3     3     3    3  ...     3     3     3     3     3     3\n",
              "11            1    3    1     1     1    1  ...     1     1     1     1     1     1\n",
              "12            1    1    1     1     1    1  ...     1     1     1     1     1     1\n",
              "\n",
              "[12 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXy1431M6o02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "42fd5366-d43d-43fa-db76-5005620381d6"
      },
      "source": [
        "idx2word = {idx: word for idx, word in enumerate(english.vocab.itos)}\n",
        "df_eng_word = pd.DataFrame(columns = [str(\"S_\")+str(x+1) for x in range(BATCH_SIZE)])\n",
        "df_eng_word = df_eng_idx.replace(idx2word)\n",
        "# df_eng_word.to_csv('/content/Words.csv')\n",
        "df_eng_word"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S_1</th>\n",
              "      <th>S_2</th>\n",
              "      <th>S_3</th>\n",
              "      <th>S_4</th>\n",
              "      <th>S_5</th>\n",
              "      <th>S_6</th>\n",
              "      <th>S_7</th>\n",
              "      <th>S_8</th>\n",
              "      <th>S_9</th>\n",
              "      <th>S_10</th>\n",
              "      <th>S_11</th>\n",
              "      <th>S_12</th>\n",
              "      <th>S_13</th>\n",
              "      <th>S_14</th>\n",
              "      <th>S_15</th>\n",
              "      <th>S_16</th>\n",
              "      <th>S_17</th>\n",
              "      <th>S_18</th>\n",
              "      <th>S_19</th>\n",
              "      <th>S_20</th>\n",
              "      <th>S_21</th>\n",
              "      <th>S_22</th>\n",
              "      <th>S_23</th>\n",
              "      <th>S_24</th>\n",
              "      <th>S_25</th>\n",
              "      <th>S_26</th>\n",
              "      <th>S_27</th>\n",
              "      <th>S_28</th>\n",
              "      <th>S_29</th>\n",
              "      <th>S_30</th>\n",
              "      <th>S_31</th>\n",
              "      <th>S_32</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time Steps</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>fast</td>\n",
              "      <td>a</td>\n",
              "      <td>two</td>\n",
              "      <td>three</td>\n",
              "      <td>skateboarder</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>young</td>\n",
              "      <td>long</td>\n",
              "      <td>an</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>six</td>\n",
              "      <td>a</td>\n",
              "      <td>an</td>\n",
              "      <td>people</td>\n",
              "      <td>motorcycle</td>\n",
              "      <td>a</td>\n",
              "      <td>pops</td>\n",
              "      <td>two</td>\n",
              "      <td>a</td>\n",
              "      <td>kids</td>\n",
              "      <td>a</td>\n",
              "      <td>four</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>little</td>\n",
              "      <td>child</td>\n",
              "      <td>girl</td>\n",
              "      <td>bicycle</td>\n",
              "      <td>dirt</td>\n",
              "      <td>older</td>\n",
              "      <td>girls</td>\n",
              "      <td>in</td>\n",
              "      <td>young</td>\n",
              "      <td>young</td>\n",
              "      <td>woman</td>\n",
              "      <td>naked</td>\n",
              "      <td>blond</td>\n",
              "      <td>man</td>\n",
              "      <td>black</td>\n",
              "      <td>girls</td>\n",
              "      <td>-</td>\n",
              "      <td>older</td>\n",
              "      <td>man</td>\n",
              "      <td>group</td>\n",
              "      <td>dogs</td>\n",
              "      <td>woman</td>\n",
              "      <td>homeless</td>\n",
              "      <td>sitting</td>\n",
              "      <td>police</td>\n",
              "      <td>man</td>\n",
              "      <td>and</td>\n",
              "      <td>women</td>\n",
              "      <td>boy</td>\n",
              "      <td>play</td>\n",
              "      <td>girl</td>\n",
              "      <td>children</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>girl</td>\n",
              "      <td>plays</td>\n",
              "      <td>sits</td>\n",
              "      <td>race</td>\n",
              "      <td>bike</td>\n",
              "      <td>men</td>\n",
              "      <td>hammer</td>\n",
              "      <td>jeans</td>\n",
              "      <td>couple</td>\n",
              "      <td>boy</td>\n",
              "      <td>sitting</td>\n",
              "      <td>person</td>\n",
              "      <td>girl</td>\n",
              "      <td>putting</td>\n",
              "      <td>dog</td>\n",
              "      <td>performing</td>\n",
              "      <td>haired</td>\n",
              "      <td>woman</td>\n",
              "      <td>is</td>\n",
              "      <td>of</td>\n",
              "      <td>are</td>\n",
              "      <td>with</td>\n",
              "      <td>man</td>\n",
              "      <td>in</td>\n",
              "      <td>officer</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "      <td>on</td>\n",
              "      <td>jumps</td>\n",
              "      <td>in</td>\n",
              "      <td>is</td>\n",
              "      <td>keeping</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>is</td>\n",
              "      <td>with</td>\n",
              "      <td>on</td>\n",
              "      <td>in</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>talking</td>\n",
              "      <td>nails</td>\n",
              "      <td>and</td>\n",
              "      <td>enjoys</td>\n",
              "      <td>excitedly</td>\n",
              "      <td>inside</td>\n",
              "      <td>standing</td>\n",
              "      <td>and</td>\n",
              "      <td>together</td>\n",
              "      <td>leaps</td>\n",
              "      <td>some</td>\n",
              "      <td>man</td>\n",
              "      <td>is</td>\n",
              "      <td>looking</td>\n",
              "      <td>people</td>\n",
              "      <td>competing</td>\n",
              "      <td>white</td>\n",
              "      <td>is</td>\n",
              "      <td>front</td>\n",
              "      <td>rides</td>\n",
              "      <td>an</td>\n",
              "      <td>girls</td>\n",
              "      <td>a</td>\n",
              "      <td>onto</td>\n",
              "      <td>a</td>\n",
              "      <td>curling</td>\n",
              "      <td>warm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>riding</td>\n",
              "      <td>her</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>up</td>\n",
              "      <td>on</td>\n",
              "      <td>on</td>\n",
              "      <td>t</td>\n",
              "      <td>a</td>\n",
              "      <td>shoots</td>\n",
              "      <td>a</td>\n",
              "      <td>on</td>\n",
              "      <td>brunette</td>\n",
              "      <td>a</td>\n",
              "      <td>over</td>\n",
              "      <td>kind</td>\n",
              "      <td>works</td>\n",
              "      <td>checking</td>\n",
              "      <td>into</td>\n",
              "      <td>playing</td>\n",
              "      <td>in</td>\n",
              "      <td>sunglasses</td>\n",
              "      <td>reading</td>\n",
              "      <td>of</td>\n",
              "      <td>in</td>\n",
              "      <td>apron</td>\n",
              "      <td>taking</td>\n",
              "      <td>movie</td>\n",
              "      <td>a</td>\n",
              "      <td>blow</td>\n",
              "      <td>in</td>\n",
              "      <td>under</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>a</td>\n",
              "      <td>father</td>\n",
              "      <td>swing</td>\n",
              "      <td>foreign</td>\n",
              "      <td>the</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>-</td>\n",
              "      <td>snowball</td>\n",
              "      <td>a</td>\n",
              "      <td>train</td>\n",
              "      <td>a</td>\n",
              "      <td>girl</td>\n",
              "      <td>wooden</td>\n",
              "      <td>a</td>\n",
              "      <td>of</td>\n",
              "      <td>in</td>\n",
              "      <td>her</td>\n",
              "      <td>a</td>\n",
              "      <td>carnival</td>\n",
              "      <td>a</td>\n",
              "      <td>is</td>\n",
              "      <td>a</td>\n",
              "      <td>other</td>\n",
              "      <td>a</td>\n",
              "      <td>grilling</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>set</td>\n",
              "      <td>inflatable</td>\n",
              "      <td>up</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bicycle</td>\n",
              "      <td>'s</td>\n",
              "      <td>ride</td>\n",
              "      <td>country</td>\n",
              "      <td>terrain</td>\n",
              "      <td>sidewalk</td>\n",
              "      <td>roof</td>\n",
              "      <td>shirt</td>\n",
              "      <td>fight</td>\n",
              "      <td>basketball</td>\n",
              "      <td>car</td>\n",
              "      <td>rock</td>\n",
              "      <td>walking</td>\n",
              "      <td>chair</td>\n",
              "      <td>log</td>\n",
              "      <td>dance</td>\n",
              "      <td>a</td>\n",
              "      <td>watch</td>\n",
              "      <td>telescope</td>\n",
              "      <td>games</td>\n",
              "      <td>race</td>\n",
              "      <td>laughing</td>\n",
              "      <td>magazine</td>\n",
              "      <td>people</td>\n",
              "      <td>parade</td>\n",
              "      <td>corn</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>reviewing</td>\n",
              "      <td>water</td>\n",
              "      <td>playground</td>\n",
              "      <td>competition</td>\n",
              "      <td>blanket</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>.</td>\n",
              "      <td>boots</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>performing</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>white</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>film</td>\n",
              "      <td>slide</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>.</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>jump</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>room</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>.</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>.</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "      <td>&lt;pad&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                S_1     S_2    S_3  ...        S_30         S_31      S_32\n",
              "Time Steps                          ...                                   \n",
              "1             <sos>   <sos>  <sos>  ...       <sos>        <sos>     <sos>\n",
              "2                 a       a      a  ...        kids            a      four\n",
              "3            little   child   girl  ...        play         girl  children\n",
              "4              girl   plays   sits  ...          in           is   keeping\n",
              "5                is    with     on  ...           a      curling      warm\n",
              "6            riding     her      a  ...        blow           in     under\n",
              "7                 a  father  swing  ...          up            a         a\n",
              "8           bicycle      's   ride  ...  playground  competition   blanket\n",
              "9                 .   boots      .  ...           .            .         .\n",
              "10            <eos>       .  <eos>  ...       <eos>        <eos>     <eos>\n",
              "11            <pad>   <eos>  <pad>  ...       <pad>        <pad>     <pad>\n",
              "12            <pad>   <pad>  <pad>  ...       <pad>        <pad>     <pad>\n",
              "\n",
              "[12 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLV-t-MzVQdg"
      },
      "source": [
        "## 用 LSTM 搭建的 Encoder 類別: EncoderLSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dZT3Zs17yMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca97a8ef-4cfc-4c35-92f4-781e105d067a"
      },
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_rate):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "    self.tag = True\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    \n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = drop_rate)\n",
        "\n",
        "  # Shape of x (26, 32) [Sequence_length, batch_size]\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    \n",
        "    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "\n",
        "    return hidden_state, cell_state\n",
        "\n",
        "input_size_encoder = len(german.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "\n",
        "encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
        "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
        "print(encoder_lstm)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(4587, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTew1tbHVer5"
      },
      "source": [
        "## 用 LSTM 搭建的 decoder 類別: DecoderLSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPGbQiBP72iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f34b861-f6e9-41d2-831a-4a2c6d46c3c3"
      },
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_rate, output_size):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = drop_rate)\n",
        "\n",
        "    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  # Shape of x (32) [batch_size]\n",
        "  def forward(self, x, hidden_state, cell_state):\n",
        "\n",
        "    # Shape of x (1, 32) [1, batch_size]\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "\n",
        "    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
        "    predictions = self.fc(outputs)\n",
        "\n",
        "    # Shape --> predictions (32, 4556) [batch_size , output_size]\n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden_state, cell_state\n",
        "\n",
        "input_size_decoder = len(english.vocab)\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "decoder_dropout = 0.5\n",
        "output_size = len(english.vocab)\n",
        "\n",
        "decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
        "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
        "print(decoder_lstm)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(4556, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=1024, out_features=4556, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xof3dPly753w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1dcabb-a14d-4b7b-bf40-2023328b72e2"
      },
      "source": [
        "for batch in train_iterator:\n",
        "  print(batch.src.shape)\n",
        "  print(batch.trg.shape)\n",
        "  break\n",
        "\n",
        "x = batch.trg[1]\n",
        "print(x)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([12, 32])\n",
            "torch.Size([16, 32])\n",
            "tensor([ 16, 765, 202,   4,   4,   4,   4,   4,   4,   4,   4,  16,   4,   4,\n",
            "          4,   4,  16,   7,   4,   9,   4, 110,   4,   4,   4,   4,   4,  53,\n",
            "          4,   4,  19,   4], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGnQbCnGVire"
      },
      "source": [
        "# Sequence to Sequence 類別"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vzOor_Q782h"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.Encoder_LSTM = Encoder_LSTM\n",
        "    self.Decoder_LSTM = Decoder_LSTM\n",
        "\n",
        "  def forward(self, source, target, tfr=0.5):\n",
        "    # Shape - Source : (10, 32) [(Sentence length german + some padding), Number of Sentences]\n",
        "    batch_size = source.shape[1]\n",
        "\n",
        "    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(english.vocab)\n",
        "    \n",
        "    # Shape --> outputs (14, 32, 5766) \n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n",
        "    hidden_state, cell_state = self.Encoder_LSTM(source)\n",
        "\n",
        "    # Shape of x (32 elements)\n",
        "    x = target[0] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, target_len):\n",
        "      # Shape --> output (32, 5766) \n",
        "      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
        "      outputs[i] = output\n",
        "      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
        "      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "\n",
        "    # Shape --> outputs (14, 32, 5766) \n",
        "    return outputs\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywW6f9fM8AMa"
      },
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD0pRilG8CHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd85290e-baa3-429a-8762-05591bcc1974"
      },
      "source": [
        "model"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (Encoder_LSTM): EncoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(4587, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (Decoder_LSTM): DecoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(4556, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=1024, out_features=4556, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQyZ_vfq8G6C"
      },
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "\n",
        "    if type(sentence) == str:\n",
        "        tokens = tokenize_de(sentence)\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "# 用來評估模型的函式: bleu\n",
        "def bleu(data, model, german, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, german, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n",
        "\n",
        "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n",
        "    print('saving')\n",
        "    print()\n",
        "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
        "    torch.save(state, './checkpoint-NMT')\n",
        "    torch.save(model.state_dict(),'./checkpoint-NMT-SD')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmsUWf-61cPc",
        "outputId": "3e62d664-2a6f-493c-f495-2c8953286f6e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb  3 12:20:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    32W /  70W |   2013MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysc4A5HX8Qyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516c61da-38b3-4bb3-b9a7-abb7a4e03602"
      },
      "source": [
        "epoch_loss = 0.0\n",
        "num_epochs = 30\n",
        "best_loss = 999999\n",
        "best_epoch = -1\n",
        "sentence1 = \"ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster\"\n",
        "ts1  = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
        "  model.eval()\n",
        "  translated_sentence1 = translate_sentence(model, sentence1, german, english, device, max_length=50)\n",
        "  print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n",
        "  ts1.append(translated_sentence1)\n",
        "\n",
        "  model.train(True)\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    input = batch.src.to(device)\n",
        "    target = batch.trg.to(device)\n",
        "\n",
        "    # Pass the input and target for model's forward method\n",
        "    output = model(input, target)\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    # Clear the accumulating gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss value for every epoch\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Calculate the gradients for weights & biases using back-propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the gradient value is it exceeds > 1\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    # Update the weights values using the gradients we calculated using bp \n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "    epoch_loss += loss.item()\n",
        "    writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "\n",
        "  if epoch_loss < best_loss:\n",
        "    best_loss = epoch_loss\n",
        "    best_epoch = epoch\n",
        "    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
        "    if ((epoch - best_epoch) >= 10):\n",
        "      print(\"no improvement in 10 epochs, break\")\n",
        "      break\n",
        "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
        "  print()\n",
        "  \n",
        "print(epoch_loss / len(train_iterator))\n",
        "\n",
        "# score = bleu(test_data[1:100], model, german, english, device)\n",
        "# print(f\"Bleu score {score*100:.2f}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch - 1 / 30\n",
            "Translated example sentence 1: \n",
            " ['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<eos>']\n",
            "saving\n",
            "\n",
            "Epoch_Loss - 3.3219828605651855\n",
            "\n",
            "Epoch - 2 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'with', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 3.0540013313293457\n",
            "\n",
            "Epoch - 3 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 1.9232869148254395\n",
            "\n",
            "Epoch - 4 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<eos>']\n",
            "Epoch_Loss - 1.7466254234313965\n",
            "\n",
            "Epoch - 5 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<eos>']\n",
            "Epoch_Loss - 1.6457366943359375\n",
            "\n",
            "Epoch - 6 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<eos>']\n",
            "Epoch_Loss - 2.1476047039031982\n",
            "\n",
            "Epoch - 7 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<eos>']\n",
            "Epoch_Loss - 3.060360908508301\n",
            "\n",
            "Epoch - 8 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<eos>']\n",
            "Epoch_Loss - 1.0298733711242676\n",
            "\n",
            "Epoch - 9 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', \"'s\", '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 0.5836925506591797\n",
            "\n",
            "Epoch - 10 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '<unk>', 'that', 'that', '<unk>', '\"', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 0.9373911619186401\n",
            "\n",
            "Epoch - 11 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', \"'s\", '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 0.608814001083374\n",
            "\n",
            "Epoch - 12 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '\"', '<unk>', '\"', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<eos>']\n",
            "Epoch_Loss - 1.6224186420440674\n",
            "\n",
            "Epoch - 13 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', 'number', \"'s\", '<unk>', 'that', '<unk>', '\"', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 1.210343360900879\n",
            "\n",
            "Epoch - 14 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '\"', '<unk>', '\"', '<unk>', 'live', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 2.5873613357543945\n",
            "\n",
            "Epoch - 15 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', \"'s\", '<unk>', 'that', \"'s\", 'head', '<unk>', '<unk>', 'threw', '<unk>', '<unk>', '<eos>']\n",
            "Epoch_Loss - 0.32342690229415894\n",
            "\n",
            "Epoch - 16 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 0.8505768775939941\n",
            "\n",
            "Epoch - 17 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', 'that', '<unk>', '\"', '<unk>', '<unk>', '<unk>', '<unk>', '2', '.', '<eos>']\n",
            "Epoch_Loss - 1.5913435220718384\n",
            "\n",
            "Epoch - 18 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', 'that', 'says', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 2.2894585132598877\n",
            "\n",
            "Epoch - 19 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', \"'s\", '<unk>', '\"', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 1.1298390626907349\n",
            "\n",
            "Epoch - 20 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', 'that', '<unk>', '\"', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 0.6385598182678223\n",
            "\n",
            "Epoch - 21 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '\"', '<unk>', '\"', '<unk>', 'still', 'still', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 1.3813360929489136\n",
            "\n",
            "Epoch - 22 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', 'that', '<unk>', '\"', '<unk>', 'still', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 0.43134593963623047\n",
            "\n",
            "Epoch - 23 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '\"', '<unk>', 'on', '<unk>', \"'\", '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 0.17106953263282776\n",
            "\n",
            "Epoch - 24 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', \"'s\", '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '2', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 0.15982642769813538\n",
            "\n",
            "Epoch - 25 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', \"'s\", '<unk>', '\"', '<unk>', 'that', 'you', '<unk>', '<unk>', '<unk>', '2', '.', '<eos>']\n",
            "Epoch_Loss - 0.1319192498922348\n",
            "\n",
            "Epoch - 26 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', 'balances', '\"', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '2', '<eos>']\n",
            "Epoch_Loss - 0.37466225028038025\n",
            "\n",
            "Epoch - 27 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '\"', '<unk>', '\"', '<unk>', '<unk>', '<unk>', '<unk>', '2', '<eos>']\n",
            "Epoch_Loss - 0.514785885810852\n",
            "\n",
            "Epoch - 28 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', \"'s\", 'i', 'i', '<unk>', 'you', 'you', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 0.11024939268827438\n",
            "\n",
            "Epoch - 29 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', \"'s\", '\"', '<unk>', 'still', 'still', 'still', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 0.10366814583539963\n",
            "\n",
            "Epoch - 30 / 30\n",
            "Translated example sentence 1: \n",
            " ['<unk>', '<unk>', 'in', '<unk>', '<unk>', '<unk>', '<unk>', 'you', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 1.572758436203003\n",
            "\n",
            "27.349170117564046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAsJuPpLzz6G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}