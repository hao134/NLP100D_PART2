{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "範例_transformer_decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu1c8eWEQG76"
      },
      "source": [
        "# 範例 : Transformer decoder\n",
        "***\n",
        "- 實做 Transformer decoder 以更了解　Transformer \n",
        "- 應用 Transformer decoder 建立一個簡單的 ptt 貼文回應器 驗證 Transformer decoder 可以運行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZljCWHhScnI"
      },
      "source": [
        "# [教學目標]\n",
        "- 了解如何實作 transformer decoder 和其結構\n",
        "- 了解如何應用 transformer decoder 並證明 decoder 可以作用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C50wfDUxTFvZ"
      },
      "source": [
        "# [範例重點]\n",
        "- 觀察 TransformerDecoder 的建立\n",
        "- 觀察 TransformerDecoderLayer 的建立\n",
        "-- 使用 encoder 相同的 MultiHeadAttentionSubLayer\n",
        "-- 使用 encoder 相同的 PosFeedForwardSubLayer\n",
        "- 觀察如何使用 建立的 TransformerDecoder \n",
        "-- 使用 TransformerDecoder 做序列生成 SequenceGenerate\n",
        "-- 如何使用 SequenceGenerate 模型 訓練一個 ptt 回應機"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNsXiF46TvpP"
      },
      "source": [
        "# [範例結構]\n",
        "- TransformerDecoder 模型和 SequenceGenerate 實作\n",
        "- ptt 資料準備\n",
        "- 應用 SequenceGenerate 訓練 ptt answer machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVOgcAidH989"
      },
      "source": [
        "# import 需要的 packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import spacy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT2vUzUAXkik"
      },
      "source": [
        "from torchtext.data import Field, BucketIterator, TabularDataset"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOKbhccjXtg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2e1a4b-80a0-4251-b397-f3a623debc31"
      },
      "source": [
        "# 連接個人資料 讀取 ＰＴＴ 訓練資料和儲存模型\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGOuTtZbWhIj"
      },
      "source": [
        "# 實做 TransformerDecoder\n",
        "- 如果只用 Transfomer decoder 而已 不和 encoder　一起使用 \n",
        "-- skip_encoder_attn 不需要和 encoder attention\n",
        "-- enc_hidden　和 enc_mask　不用輸入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiWfhvskI2fH"
      },
      "source": [
        "## 啟動參數\n",
        "## hidden_dim 內部 embedding 大小\n",
        "## feedforward_dim  feedforward 中間層大小\n",
        "## n_dec_layers 幾層 Transformer Layers\n",
        "## n_attn_heads 幾個 attention heads \n",
        "## dropout dropout 比例\n",
        "## dec_voca_length  字彙集合大小\n",
        "## max_pos_length  最大 decode 序列長度\n",
        "## device \n",
        "## skip_encoder_attn 不需要和 encoder attention\n",
        "\n",
        "## 輸入值\n",
        "## dec_seq 解碼序列　（句子）\n",
        "## enc_hidden　編碼的　hidden embedding　(Optional)\n",
        "## dec_mask 解碼遮罩\n",
        "## enc_mask　編碼遮罩　(Optional)\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, hidden_dim, feedforward_dim, n_dec_layers, n_attn_heads, dropout, dec_voca_length, max_pos_length , device , skip_encoder_attn = False):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    # 建立 decoder token embedding \n",
        "    self.dec_tok_embedding = nn.Embedding(dec_voca_length, hidden_dim )\n",
        "    # 建立 decoder position embedding \n",
        "    self.dec_pos_embedding = nn.Embedding(max_pos_length, hidden_dim)\n",
        "\n",
        "    # 建立 n_dec_layers 個 TransformerDecoderLayer 層\n",
        "    self.transformer_decoder_layers = nn.ModuleList([TransformerDecoderLayer(hidden_dim,\n",
        "                                          feedforward_dim, \n",
        "                                          n_dec_layers,\n",
        "                                          n_attn_heads,\n",
        "                                          dropout, \n",
        "                                          device, skip_encoder_attn) for _ in range(n_dec_layers)])\n",
        "\n",
        "    # 輸出層 輸出 vocabulary 個長度\n",
        "    self.full_conn_out = nn.Linear(hidden_dim, dec_voca_length)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "  def forward(self, dec_seq, enc_hidden , dec_mask, enc_mask):\n",
        "    #dec_seq 輸入 tensor 形狀 [batch size, decode sequence len]\n",
        "    #enc_hidden 輸入 tensor 形狀 [batch size, encode sequence len, hid dim] # optional 不需要時輸入空值\n",
        "    #dec_mask 輸入 tensor 形狀 [batch size, decode sequence len]\n",
        "    #enc_mask 輸入 tensor 形狀 [batch size, encode sequence len] # optional 不需要時輸入空值\n",
        "                \n",
        "    batch_size = dec_seq.shape[0]\n",
        "    dec_len = dec_seq.shape[1]\n",
        "        \n",
        "    pos = torch.arange(0, dec_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "    #pos 的 tensor 形狀 [batch size, decode sequence len]\n",
        "            \n",
        "    # 將 decoder token embedding 加上 decoder postion embedding\n",
        "    dec_seq = self.dropout(self.dec_tok_embedding(dec_seq)  + self.dec_pos_embedding(pos))\n",
        "                \n",
        "    #dec_seq 輸出 tensor 形狀 [batch size, decode sequence len, hid dim]\n",
        "        \n",
        "    for layer in self.transformer_decoder_layers:\n",
        "      dec_seq, encoder_decoder_attention , decoder_self_attention = layer(dec_seq, enc_hidden, dec_mask, enc_mask)\n",
        "        \n",
        "    #dec_seq 輸出 tensor 形狀 [batch size, decode sequence  len, hid dim]\n",
        "    #attention 輸出 tensor 形狀 [batch size, n heads, trg len, src len]\n",
        "        \n",
        "    output = self.full_conn_out(dec_seq)\n",
        "        \n",
        "    #output tensor 形狀 [batch size, trg len, output dim]\n",
        "            \n",
        "    return output, encoder_decoder_attention , decoder_self_attention\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNvw02zeMGIT"
      },
      "source": [
        "# 實做 TransformerDecoderLayer\n",
        "- 實作在transformerDecoder 使用多層 的TransformerDecoderLayer\n",
        "- 如果只使用 decoder 則不用 encoder attention, --> skip_encoder_attn = True "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAV9O7SnJsLU"
      },
      "source": [
        "## 啟動參數\n",
        "## hidden_dim 內部 embedding 大小\n",
        "## feedforward_dim  feedforward 中間層大小\n",
        "## n_dec_layers 幾層 Transformer Layers\n",
        "## n_attn_heads 幾個 attention heads \n",
        "## dropout dropout 比例\n",
        "## device \n",
        "## skip_encoder_attn 不需要和 encoder attention\n",
        "\n",
        "## 輸入值\n",
        "## dec_seq 解碼序列　（句子）\n",
        "## enc_hidden　編碼的　hidden embedding　(Optional)\n",
        "## dec_mask 解碼遮罩\n",
        "## enc_mask　編碼遮罩　(Optional)\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim , feedforward_dim, n_dec_layers, n_attn_heads, dropout , device , skip_encoder_attn = False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.skip_encoder_attn = skip_encoder_attn \n",
        "\n",
        "    self.self_attention_sublayer = MultiHeadAttentionSubLayer(hidden_dim, n_attn_heads, dropout, device)\n",
        "    self.self_attn_layernorm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    if not skip_encoder_attn:\n",
        "      self.encoder_attention_sublayer = MultiHeadAttentionSubLayer(hidden_dim, n_attn_heads, dropout, device)\n",
        "      self.encoder_attn_layernorm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    self.positionwise_feedforward = PosFeedForwardSubLayer(hidden_dim,feedforward_dim ,dropout)\n",
        "    self.feedforward_layernorm = nn.LayerNorm(hidden_dim)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)     \n",
        "\n",
        "  def forward(self, dec_seq, enc_hidden , dec_mask, enc_mask):\n",
        "    #dec_seq 輸入 tensor 形狀 [batch size, decode sequence len, hid dim]\n",
        "    #enc_hidden 輸入 tensor 形狀 [batch size, encode sequence len, hid dim] # optional 不需要時輸入空值\n",
        "    #dec_mask 輸入 tensor 形狀 [batch size, decode sequence len]\n",
        "    #enc_mask 輸入 tensor 形狀 [batch size, encode sequence len] # optional 不需要時輸入空值\n",
        "        \n",
        "    #self attention 子層\n",
        "    _dec_seq, decoder_self_attention = self.self_attention_sublayer(dec_seq, dec_seq, dec_seq, dec_mask)\n",
        "        \n",
        "    #dropout, residual connection and layer norm　(Add and Norm)\n",
        "    dec_seq = self.self_attn_layernorm(dec_seq + self.dropout(_dec_seq))\n",
        "            \n",
        "    #dec_seq  輸出 tensor 形狀 [batch size, decode sequence len, hid dim]\n",
        "            \n",
        "    # 需不需要建立　encoder attention 層        \n",
        "    if not self.skip_encoder_attn:\n",
        "      #encoder attention\n",
        "      _dec_seq, encoder_decoder_attention = self.encoder_attention_sublayer(dec_seq, enc_hidden, enc_hidden, enc_mask)\n",
        "          \n",
        "      #dropout, residual connection and layer norm\n",
        "      dec_seq = self.encoder_attn_layernorm(dec_seq + self.dropout(_dec_seq))\n",
        "    else:\n",
        "      encoder_decoder_attention = None\n",
        "                    \n",
        "    #dec_seq 輸出 tensor 形狀 [batch size, decode sequence len, hid dim]\n",
        "    #positionwise feedforward\n",
        "    _dec_seq = self.positionwise_feedforward(dec_seq)\n",
        "        \n",
        "    #dropout, residual and layer norm (Add and Norm)\n",
        "    dec_seq = self.feedforward_layernorm(dec_seq + self.dropout(_dec_seq))\n",
        "        \n",
        "    #dec_seq 輸出 tensor 形狀 [batch size, decode sequence len, hid dim]\n",
        "    #attention 輸出 tensor 形狀 [batch size, n heads, decode sequence len, encode sequence len]\n",
        "        \n",
        "    return dec_seq, encoder_decoder_attention , decoder_self_attention\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MZZrOaLOEQn"
      },
      "source": [
        "# 實做 MultiHeadAttentionSubLayer\n",
        "- 實作 encoder and decoder 同時共用的 MultiHeadAttention SubLayer \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_9DTM9jJyDR"
      },
      "source": [
        "## 啟動參數\n",
        "## hidden_dim 內部 embedding 大小\n",
        "## n_attn_heads 幾個 attention heads \n",
        "## dropout dropout 比例\n",
        "## device \n",
        "\n",
        "## 輸入值\n",
        "## query_input, --> K \n",
        "## key_input, --> Q\n",
        "## value_input, --> V\n",
        "## mask 遮罩\n",
        "\n",
        "class MultiHeadAttentionSubLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim , n_attn_heads, dropout, device):\n",
        "    super().__init__()\n",
        "\n",
        "    # 確定 設定的 hidden layer 維度可以被 attention head 整除\n",
        "    assert hidden_dim % n_attn_heads ==0\n",
        "\n",
        "    # hidden layer 維度\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    # multi-heads 的個數\n",
        "    self.n_attn_heads = n_attn_heads\n",
        "\n",
        "    # 平均分到每個 multi-head 的 維度\n",
        "    self.head_dim = hidden_dim // n_attn_heads\n",
        "\n",
        "    # 就是在課程中提到的 Wq Wk Wv\n",
        "    self.full_conn_q = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.full_conn_k = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.full_conn_v = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    # 最後結果再過一層 線性轉換\n",
        "    self.full_conn_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    # 根據維度大小調整 attention 值 以免維度太大 Q dot K 結果過大影響學習效率    \n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  \n",
        "\n",
        "  def forward(self, query_input, key_input, value_input, mask = None):\n",
        "    batch_size = query_input.shape[0]\n",
        "\n",
        "    #query_input shape [batch size, query len, hid dim]\n",
        "    #key_input shape [batch size, key len, hid dim]\n",
        "    #value_input shape [batch size, value len, hid dim]\n",
        "\n",
        "    Q = self.full_conn_q(query_input)\n",
        "    K = self.full_conn_k(key_input)\n",
        "    V = self.full_conn_v(value_input)\n",
        "\n",
        "    #Q shape [batch size, query len, hid dim]\n",
        "    #K shape [batch size, key len, hid dim]\n",
        "    #V shape [batch size, value len, hid dim]\n",
        "\n",
        "    # 將 attention 切成多塊小的 attention\n",
        "    def split_attention(Q, K, V):\n",
        "      Q = Q.view(batch_size, -1, self.n_attn_heads, self.head_dim)\n",
        "      K = K.view(batch_size, -1, self.n_attn_heads, self.head_dim)\n",
        "      V = V.view(batch_size, -1, self.n_attn_heads, self.head_dim)\n",
        "      return Q , K , V\n",
        "\n",
        "    # 將 attention 的 2 和 3 維度轉置 以達到將 attention head 提到前面 而分開每個 attention head\n",
        "    def seperate_heads(Q, K, V):\n",
        "      Q = Q.permute(0, 2, 1, 3) # (batch_size, self.n_heads , query len , self.head_dim)\n",
        "      K = K.permute(0, 2, 1, 3) # (batch_size, self.n_heads , key len , self.head_dim)\n",
        "      V = V.permute(0, 2, 1, 3) # (batch_size, self.n_heads , value len , self.head_dim)\n",
        "      return Q , K , V\n",
        "\n",
        "    Q, K, V = split_attention(Q, K, V)\n",
        "\n",
        "    Q, K, V = seperate_heads (Q, K, V)\n",
        "\n",
        "    \n",
        "    # 調整過的 dot product attention, 由於之前分開了每個 attention head \n",
        "    # 所以現在只要把 Ｋ的最後兩個維度轉置 就可以 by attention head 求得 Q dot K\n",
        "    scaled_dot_product_similarity = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "    #scaled_dot_product_similarity 輸出 [batch size, n heads, query len, key len]\n",
        "\n",
        "    if mask is not None:\n",
        "      scaled_dot_product_similarity = scaled_dot_product_similarity.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "    attention = torch.softmax(scaled_dot_product_similarity, dim = -1)\n",
        "    #attention = [batch size, n heads, query len, key len]\n",
        "\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "    #x 輸出 [batch size, n heads, query len, head dim]\n",
        "        \n",
        "    x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "    #x 輸出 [batch size, query len, n heads, head dim]\n",
        "        \n",
        "    x = x.view(batch_size, -1, self.hidden_dim)\n",
        "        \n",
        "    #x 輸出 [batch size, query len, hid dim]\n",
        "        \n",
        "    x = self.full_conn_o(x)\n",
        "        \n",
        "    #x 輸出 [batch size, query len, hid dim]\n",
        "        \n",
        "    return x, attention"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4kg56sgOzpj"
      },
      "source": [
        "# 實做 PosFeedForwardSubLayer\n",
        "- 實作 encoder and decoder 同時共用的 PosFeedForward SubLayer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv_5iQiFJ37I"
      },
      "source": [
        "class PosFeedForwardSubLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, ff_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.full_conn_1 = nn.Linear(hidden_dim, ff_dim)\n",
        "    self.full_conn_2 = nn.Linear(ff_dim,  hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "    x = self.dropout(torch.relu(self.full_conn_1(x)))\n",
        "        \n",
        "    #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "    x = self.full_conn_2(x)\n",
        "        \n",
        "    #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "    return x\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Awfn3ELO8sg"
      },
      "source": [
        "# 實做 SequenceGenerate \n",
        "- 處理 序列生成工作\n",
        "- 叫用 TransformerDecoderLayer\n",
        "-- 不使用 encoder decoder attention 子層\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URWQcxLJJ63p"
      },
      "source": [
        "## 啟動參數\n",
        "## decoder　Transformer decoder\n",
        "## dec_pad_idx decoder padding index  \n",
        "## device \n",
        "\n",
        "## 輸入值\n",
        "## dec_seq 解碼訓練\n",
        "class SequenceGenerate(nn.Module):\n",
        "  def __init__(self, decoder, dec_pad_idx, device):\n",
        "    super().__init__()\n",
        "    self.decoder = decoder\n",
        "    self.dec_pad_idx = dec_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "  def make_dec_mask(self, dec_seq):\n",
        "        \n",
        "    #dec_seq 輸入 [batch size, decoder sequence len]\n",
        "        \n",
        "    dec_pad_mask = (dec_seq != self.dec_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "    #dec_pad_mask 輸出 [batch size, 1, 1, decoder sequence len]\n",
        "        \n",
        "    dec_len = dec_seq.shape[1]\n",
        "        \n",
        "    dec_sub_mask = torch.tril(torch.ones((dec_len, dec_len), device = self.device)).bool()\n",
        "        \n",
        "    #dec_sub_mask 輸出 [decoder sequence len, decoder sequence len]\n",
        "            \n",
        "    dec_mask = dec_pad_mask & dec_sub_mask\n",
        "        \n",
        "    #dec_mask 輸出 [batch size, 1, decoder sequence len, decoder sequence len]\n",
        "        \n",
        "    return dec_mask\n",
        "\n",
        "  def forward(self, dec_seq):\n",
        "        \n",
        "    #dec_seq 輸入　tensor [batch size, trg len]\n",
        "                \n",
        "    dec_mask = self.make_dec_mask(dec_seq)\n",
        "        \n",
        "    #dec_mask 輸出 [batch size, 1, trg len, trg len]\n",
        "        \n",
        "    # 呼叫　transformer decoder 不需要輸入　encoder 相關資訊\n",
        "    # 也不用接收　encoder decoder attnetion            \n",
        "    output, _ , decoder_self_attention = self.decoder(dec_seq, None, dec_mask, None)\n",
        "        \n",
        "    #output 輸出 [batch size, trg len, output dim]\n",
        "    #attention 輸出 [batch size, n heads, trg len, src len]\n",
        "        \n",
        "    return output, decoder_self_attention"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM76XyjFSw-m"
      },
      "source": [
        "# PTT 資料準備\n",
        "\n",
        "- 我們的資料來源是 https://github.com/zake7749/Gossiping-Chinese-Corpus\n",
        "- 詳情請看 github\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bcPOST4uZbs",
        "outputId": "15917819-4409-45b1-cd76-a92036dc52c1"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/zake7749/Gossiping-Chinese-Corpus/master/data/Gossiping-QA-Dataset-2_0.csv"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-07 09:01:46--  https://raw.githubusercontent.com/zake7749/Gossiping-Chinese-Corpus/master/data/Gossiping-QA-Dataset-2_0.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 61356419 (59M) [text/plain]\n",
            "Saving to: ‘Gossiping-QA-Dataset-2_0.csv’\n",
            "\n",
            "Gossiping-QA-Datase 100%[===================>]  58.51M   319MB/s    in 0.2s    \n",
            "\n",
            "2021-02-07 09:01:48 (319 MB/s) - ‘Gossiping-QA-Dataset-2_0.csv’ saved [61356419/61356419]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfjPS42RudMH"
      },
      "source": [
        "mv *.csv /content/drive/MyDrive/cupoy/data/"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRBsuTjcSxhm",
        "outputId": "21cd8729-4e11-4274-a723-a18b6233b809"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/cupoy/data/'\n",
        "with open(data_dir + 'Gossiping-QA-Dataset-2_0.csv' , encoding='utf-8') as fin:\n",
        "  csvreader = csv.reader(fin)\n",
        "  ptt_qa_pairs = [ row for row in csvreader]\n",
        "\n",
        "print (\"Sample: \" , ptt_qa_pairs[1000][0:2] )\n",
        "print (\"Total records:\" , len(ptt_qa_pairs))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample:  ['油價又要噴出了??', '政府：中油臺電內部控管不佳；財團：民營化砍肥貓']\n",
            "Total records: 774115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POdVnkF5XZRg"
      },
      "source": [
        "# do training test split 如果已經分過了 可以跳過這段"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDH9OthiSzqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0737b040-f162-4cd1-9e9d-aca3bb0c64a3"
      },
      "source": [
        "\n",
        "print (\"Total records after filtering :\" , len(ptt_qa_pairs))\n",
        "train, val = train_test_split(ptt_qa_pairs, test_size=10000)\n",
        "\n",
        "print (\"training data:{} , develop data: {} \".format(len(train),len(val)))\n",
        "    \n",
        "def write_csv(trn_data, file_path ):\n",
        "    with open(file_path ,'w', newline='', encoding='utf-8') as fout:\n",
        "        writer = csv.writer (fout)\n",
        "        for itm in trn_data: \n",
        "            writer.writerow ([itm[0] + \"|\" + itm[1] , itm[0] + \"|\" + itm[1]] )\n",
        "            \n",
        "file_path = data_dir + 'train.csv'\n",
        "write_csv(train, file_path )\n",
        "\n",
        "file_path = data_dir + 'val.csv'\n",
        "write_csv(val, file_path )\n",
        "    \n",
        "#file_path = data_dir + 'test.csv'\n",
        "# write_csv(test, file_path )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total records after filtering : 774115\n",
            "training data:764115 , develop data: 10000 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lVcYLXjTDf4"
      },
      "source": [
        "# 資料處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAO08pxJTEMh"
      },
      "source": [
        "def tokenize_cmn(text):\n",
        "  #去掉非中文字元\n",
        "  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
        "  text = text.replace(\"\\\\\",\"\").split(\"|\")\n",
        "\n",
        "  return [word for word in regex.sub(text[0],' ') if word.strip()] + [\"<sep>\"] + [word for word in regex.sub(text[1],' ') if word.strip()]\n",
        "\n",
        "def tokenize_trg(text):\n",
        "  #去掉非中文字元\n",
        "  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
        "  text = text.replace(\"\\\\\",\"\").split(\"|\")\n",
        "\n",
        "  return ['<pad>' for word in regex.sub(text[0],' ') if word.strip()] + [\"<pad>\"] + [word for word in regex.sub(text[1],' ') if word.strip()]   \n",
        "\n",
        "CMN_FIELD = Field(tokenize = tokenize_cmn, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "TRG_FIELD = Field(tokenize = tokenize_trg, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "train_dataset, dev_dataset = TabularDataset.splits(\n",
        "    path = data_dir , format = 'csv', skip_header = False,\n",
        "    train='train.csv', validation='val.csv',\n",
        "    fields=[\n",
        "        ('qa', CMN_FIELD),\n",
        "        ('trg', TRG_FIELD)\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rMbQIsMV0pF"
      },
      "source": [
        "# 我們要使用的資料格式\n",
        "- 建立 vocabulary\n",
        "- qa: ptt 上蒐集的問題和回答 中間用 “sep”隔開\n",
        "- trg: 我們的訓練目標只有回答的部分，其他的字元（包括“sep”）我們都以 “pad” 取代 , 計算 loss 的時候系統會忽略 ”pad“ token 註記的目標"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7NXC4GhTRDB",
        "outputId": "c95d8dbb-19e9-4c1f-bbd8-71827d5fb863"
      },
      "source": [
        "CMN_FIELD.build_vocab(train_dataset, min_freq = 2)\n",
        "TRG_FIELD.vocab = CMN_FIELD.vocab\n",
        "print (\"中文語料的字元表長度: \" , len(CMN_FIELD.vocab) )\n",
        "print (\"Sample Q and A:\", dev_dataset[0].qa)\n",
        "print (\"Sample Target:\",  dev_dataset[0].trg  )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "中文語料的字元表長度:  6519\n",
            "Sample Q and A: ['為', '何', '血', '壓', '計', '不', '能', '在', '網', '路', '上', '賣', '?', '<sep>', '哪', '天', '在', '網', '路', '上', '買', '到', '有', '問', '題', '的', '黑', '心', '貨', '就', '不', '要', '出', '來', '開', '記', '者', '會']\n",
            "Sample Target: ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '哪', '天', '在', '網', '路', '上', '買', '到', '有', '問', '題', '的', '黑', '心', '貨', '就', '不', '要', '出', '來', '開', '記', '者', '會']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WydwZItmV2KF"
      },
      "source": [
        "# 準備 train_iterator and valid_iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldRVLGBJTXiB"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_dataset, dev_dataset), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.qa),\n",
        "     device = device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFTfx3TuR-5Z"
      },
      "source": [
        "# model training and evaluate function\n",
        "- 注意 我們要輸入的字和目標要shift 一位 \n",
        "- 也就是輸入 為', '什', '麼', '淘', '寶', '一', '堆', '賣', '家', '能', '國', '內', '免', '運', '?', '<sep>' --> 希望輸出 '有'\n",
        "- 輸入 為', '什', '麼', '淘', '寶', '一', '堆', '賣', '家', '能', '國', '內', '免', '運', '?', '<sep>', '有' --> 希望輸出 '的'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIcei9Ooblb9"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        qa = batch.qa\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _  = model(qa[:,:-1])\n",
        "                \n",
        "        # print (output.shape, trg.shape)\n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if i % 1000 == 0: print (\"Train Batch:\" , i , \"Loss:\" , loss.item())\n",
        "        \n",
        "    \n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AobBpcaSboe8"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            qa = batch.qa\n",
        "            trg = batch.trg\n",
        "\n",
        "            \n",
        "            output, _  = model(qa[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asQT783VaaUU"
      },
      "source": [
        "# 實際建立模型\n",
        "- 設定重要參數\n",
        "-- 建立一個 hidden embedding 256，三層decoder layer，八個attention heads\n",
        "-- position wise feedforward 中間層 512 dropout 0.1 learning rate: 0.0005\n",
        "-- 最長句長 70\n",
        "- 如果要保留訓練出來的模型，建議和 vocabulary 一起儲存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtWL65_Ha1dX"
      },
      "source": [
        "model_dir =  '/content/drive/My Drive/cupoy/transformer/model/'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "VOC_SIZE = len(CMN_FIELD.vocab)\n",
        "MAX_SENT_LENGTH = 70\n",
        "HID_DIM = 256\n",
        "DEC_LAYERS = 3\n",
        "DEC_HEADS = 8\n",
        "DEC_FF_DIM = 512\n",
        "DEC_DROPOUT = 0.1\n",
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "dec = TransformerDecoder(HID_DIM, DEC_FF_DIM,\n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS,  \n",
        "              DEC_DROPOUT, \n",
        "              VOC_SIZE, MAX_SENT_LENGTH,\n",
        "              device , skip_encoder_attn = True)\n",
        "\n",
        "CMN_PAD_IDX = CMN_FIELD.vocab.stoi[CMN_FIELD.pad_token]\n",
        "\n",
        "#TransformerSequenceGenerate\n",
        "model = SequenceGenerate(dec, CMN_PAD_IDX, device).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = CMN_PAD_IDX)\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDXajaXalpKX"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "model.apply(initialize_weights);"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOnClWtxc8VI"
      },
      "source": [
        "# 實際訓練\n",
        "- Ｔ4 大約 四分半一個 epoch\n",
        "- 訓練十個 epoch 就有一定的成績了\n",
        "- 如果沒時間訓練 也可以下載我們訓練好的權重"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIzda5JyvEoP",
        "outputId": "5992a537-9462-4cf3-c0df-355588971999"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb  7 09:04:42 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    27W /  70W |    985MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzmG1zJ4a6_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "285ae545-a4a1-4988-982c-22e7904a7a05"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = 9999999\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    #torch.save(model.state_dict(), model_dir + 'model-ptt-{}.pt'.format(epoch))\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        #torch.save(model.state_dict(), model_dir + 'model-ptt-best.pt')\n",
        "\n",
        "    \n",
        "    print (\"Epoch {} training time: {:.2f} sec Training Loss: {:.3f} , Valiation Loss: {:.3f}\".format( epoch , end_time - start_time , train_loss , valid_loss))\n",
        " \n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Batch: 0 Loss: 8.81271743774414\n",
            "Train Batch: 1000 Loss: 4.903470039367676\n",
            "Train Batch: 2000 Loss: 4.646539211273193\n",
            "Train Batch: 3000 Loss: 4.509260177612305\n",
            "Train Batch: 4000 Loss: 4.4734907150268555\n",
            "Train Batch: 5000 Loss: 4.539374828338623\n",
            "Epoch 0 training time: 267.01 sec Training Loss: 4.690 , Valiation Loss: 4.285\n",
            "Train Batch: 0 Loss: 4.341724872589111\n",
            "Train Batch: 1000 Loss: 4.124035835266113\n",
            "Train Batch: 2000 Loss: 4.3822221755981445\n",
            "Train Batch: 3000 Loss: 4.215755939483643\n",
            "Train Batch: 4000 Loss: 4.091890811920166\n",
            "Train Batch: 5000 Loss: 4.0102620124816895\n",
            "Epoch 1 training time: 266.83 sec Training Loss: 4.253 , Valiation Loss: 4.143\n",
            "Train Batch: 0 Loss: 4.1995954513549805\n",
            "Train Batch: 1000 Loss: 4.152331352233887\n",
            "Train Batch: 2000 Loss: 4.133296012878418\n",
            "Train Batch: 3000 Loss: 4.128142356872559\n",
            "Train Batch: 4000 Loss: 4.153467178344727\n",
            "Train Batch: 5000 Loss: 4.1189680099487305\n",
            "Epoch 2 training time: 265.88 sec Training Loss: 4.145 , Valiation Loss: 4.072\n",
            "Train Batch: 0 Loss: 3.928279399871826\n",
            "Train Batch: 1000 Loss: 4.221997261047363\n",
            "Train Batch: 2000 Loss: 3.9883224964141846\n",
            "Train Batch: 3000 Loss: 4.0035295486450195\n",
            "Train Batch: 4000 Loss: 4.011176109313965\n",
            "Train Batch: 5000 Loss: 4.097039222717285\n",
            "Epoch 3 training time: 265.70 sec Training Loss: 4.083 , Valiation Loss: 4.034\n",
            "Train Batch: 0 Loss: 3.943840980529785\n",
            "Train Batch: 1000 Loss: 4.1211676597595215\n",
            "Train Batch: 2000 Loss: 4.214779853820801\n",
            "Train Batch: 3000 Loss: 4.028078079223633\n",
            "Train Batch: 4000 Loss: 4.143233299255371\n",
            "Train Batch: 5000 Loss: 4.112513065338135\n",
            "Epoch 4 training time: 264.69 sec Training Loss: 4.043 , Valiation Loss: 4.011\n",
            "Train Batch: 0 Loss: 3.9079136848449707\n",
            "Train Batch: 1000 Loss: 3.9806249141693115\n",
            "Train Batch: 2000 Loss: 3.9957096576690674\n",
            "Train Batch: 3000 Loss: 3.8769023418426514\n",
            "Train Batch: 4000 Loss: 3.861497640609741\n",
            "Train Batch: 5000 Loss: 4.066368103027344\n",
            "Epoch 5 training time: 263.66 sec Training Loss: 4.015 , Valiation Loss: 3.992\n",
            "Train Batch: 0 Loss: 3.9321930408477783\n",
            "Train Batch: 1000 Loss: 3.6495213508605957\n",
            "Train Batch: 2000 Loss: 3.841654062271118\n",
            "Train Batch: 3000 Loss: 3.9568417072296143\n",
            "Train Batch: 4000 Loss: 4.110739707946777\n",
            "Train Batch: 5000 Loss: 3.977020263671875\n",
            "Epoch 6 training time: 264.46 sec Training Loss: 3.993 , Valiation Loss: 3.972\n",
            "Train Batch: 0 Loss: 4.049239158630371\n",
            "Train Batch: 1000 Loss: 4.039275646209717\n",
            "Train Batch: 2000 Loss: 4.007325172424316\n",
            "Train Batch: 3000 Loss: 4.152517795562744\n",
            "Train Batch: 4000 Loss: 3.9591498374938965\n",
            "Train Batch: 5000 Loss: 3.8862016201019287\n",
            "Epoch 7 training time: 264.99 sec Training Loss: 3.975 , Valiation Loss: 3.962\n",
            "Train Batch: 0 Loss: 3.873152017593384\n",
            "Train Batch: 1000 Loss: 3.8629772663116455\n",
            "Train Batch: 2000 Loss: 3.997136116027832\n",
            "Train Batch: 3000 Loss: 4.0190911293029785\n",
            "Train Batch: 4000 Loss: 4.0326433181762695\n",
            "Train Batch: 5000 Loss: 3.997786283493042\n",
            "Epoch 8 training time: 264.26 sec Training Loss: 3.959 , Valiation Loss: 3.953\n",
            "Train Batch: 0 Loss: 3.967686891555786\n",
            "Train Batch: 1000 Loss: 3.8502707481384277\n",
            "Train Batch: 2000 Loss: 4.021941184997559\n",
            "Train Batch: 3000 Loss: 4.077414035797119\n",
            "Train Batch: 4000 Loss: 3.8242950439453125\n",
            "Train Batch: 5000 Loss: 3.8524608612060547\n",
            "Epoch 9 training time: 263.72 sec Training Loss: 3.945 , Valiation Loss: 3.944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePu_d6fSbLkZ"
      },
      "source": [
        "# 如果要保留訓練出來的模型，建議和 vocabulary 一起儲存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGtaydsuHQ2X"
      },
      "source": [
        "model_dir =  '/content/drive/MyDrive/cupoy/data/'\n",
        "torch.save(CMN_FIELD.vocab, model_dir + 'vocab.pt')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC5fZzIUeByb"
      },
      "source": [
        "# 讀取訓練最佳結果\n",
        "-- 如果下載我們的訓練結果 別忘了讀取 vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLq4YhL1bFa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "c70893c4-6ed4-4679-b530-b4c890182cf0"
      },
      "source": [
        "# 保留讀取之前儲存的 vocabulary\n",
        "CMN_FIELD.vocab = torch.load( model_dir + 'vocab.pt')\n",
        "TRG_FIELD.vocab = CMN_FIELD.vocab\n",
        "\n",
        "model_dir =  '/content/drive/MyDrive/cupoy/data/'\n",
        "model.load_state_dict(torch.load( model_dir + 'model-ptt-best.pt'))\n",
        "#model.load_state_dict(torch.load(model_dir + 'model-8.pt'))\n",
        "test_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-698156a31783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m'/content/drive/MyDrive/cupoy/data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model-ptt-best.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#model.load_state_dict(torch.load(model_dir + 'model-8.pt'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/cupoy/data/model-ptt-best.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsGJujTfe3b5"
      },
      "source": [
        "# 使用訓練結果產生回答\n",
        "- 用模型每一步最佳猜測產生回答"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sguk0pbPPT20"
      },
      "source": [
        "\n",
        "def simple_answer_ptt_question(sentence, qa_field, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [qa_field.init_token] + tokens + [\"<sep>\"]\n",
        "        \n",
        "    qa_indexes = [qa_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    qa_tensor = torch.LongTensor(qa_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "\n",
        "    for i in range(max_len):\n",
        "        qa_tensor = torch.LongTensor(qa_indexes).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            dec_qa, decoder_self_attention  = model(qa_tensor)\n",
        "        \n",
        "        pred_token = dec_qa.argmax(2)[:,-1].item()\n",
        "        qa_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == qa_field.vocab.stoi[qa_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    qa_tokens = [qa_field.vocab.itos[i] for i in qa_indexes]\n",
        "    answer = \"\".join(qa_tokens[qa_tokens.index(\"<sep>\")+1:-1])\n",
        "            \n",
        "    return answer,  decoder_self_attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBrYey4fetvJ"
      },
      "source": [
        "# Fun Time\n",
        "-- 自己上 ptt 找新的標題來玩吧"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUFx32UDKE9j"
      },
      "source": [
        "question = \"日月光 是找老婆的好地方嗎\"\n",
        "question = '長這麼大，做過最壞的事是什麼？'\n",
        "question = '看到前女友生小孩是什麼感覺'\n",
        "question = '把中國人惹翻了 會怎麼樣嗎？'\n",
        "question = '泰國人民為何不推翻王室?'\n",
        "qa_result, _ = answer_ptt_question(question, CMN_FIELD, model, device, max_len = 50)\n",
        "\n",
        "print (qa_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjnOca_aKgHB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}