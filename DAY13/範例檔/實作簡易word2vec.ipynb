{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 透過實作深入了解word2vec模型\n",
    "\n",
    "在前兩次的課程中，我們了解的word2vec的兩種模型(CBOW/Skip-gram)，接下來來看看如何實際搭建word2vec的模型。 本次課程講解會使用CBOW模型作為例子來進行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建基礎層\n",
    "\n",
    "由前面的課程可以學到，訓練word2vec模型時採用的神經網路為全連接層(FC)。在輸出層的部分則是採用softmax，損失函數使用cross entropy。\n",
    "\n",
    "FC層的核心計算就是矩陣向量乘法。因此在開始搭建CBOW模型前，需要先搭建基本的FC層(矩陣乘法)與欲使用的softmax與cross entropy函數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. FC/Dense層: 損失函數對輸入的偏微分如下 (此部分計算有興趣的同學可以參考Appendix，本章節重點在於實作word2vec模型)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{L}}{\\partial{x}} &= \\sum_j\\frac{\\partial{L}}{\\partial{y}}W^T \\\\\n",
    "\\frac{\\partial{L}}{\\partial{W}} &= X^T \\sum_j\\frac{\\partial{L}}{\\partial{y}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "2. Softmax & Loss層: 在訓練word2vec主要使用的輸出層函數為softmax，而使用的損失函數為cross entropy，其中偏微分如下\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{SoftmaxIn}} = SoftmaxOut - \\hat{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from utils.utility import clip_grads, remove_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax function\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = np.exp(x)\n",
    "        x = np.divide(x, x.sum(axis=1, keepdims=True) + 1e-7)\n",
    "    elif x.ndim == 1:\n",
    "        x = np.exp(x)\n",
    "        x = np.divide(x, np.sum(x) + 1e-7)\n",
    "        \n",
    "    return x\n",
    "\n",
    "# define cross entropy\n",
    "def cross_entropy(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return -(1/batch_size) * np.sum(np.log(y[np.arange(batch_size), t] + 1e-7))\n",
    "\n",
    "# Define FC layer\n",
    "class Dense():\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        w = self.params[0]\n",
    "        out = np.dot(x, w)\n",
    "        self.x = x\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        w = self.params[0]\n",
    "        dx = np.dot(dout, w.T) #dx = dout * W^T\n",
    "        dw = np.dot(self.x.T, dout) #dw = x^T * dout\n",
    "        self.grads[0][...] = dw\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "# define softmax with cross entropy layer\n",
    "class SoftmaxWithCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        \n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        \n",
    "        loss = cross_entropy(self.y, self.t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "        dx = self.y.copy() #softmax output\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx /= batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料前處理\n",
    "\n",
    "除了建構搭建模型的基礎元素外，還需要將輸入的文本資料進行前置處理:\n",
    "\n",
    "1. Tokenize: 將文本的字詞轉換為index\n",
    "2. Data label pair: 將文本資料轉換為訓練資料與目標字詞的配對\n",
    "3. One-hot encoding: 將訓練資料與目標字詞轉換為one-hot的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define preprocess function\n",
    "def preprocess(corpus: List[str], only_word: bool = False):\n",
    "    '''Function to do preprocess of input corpus\n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: str\n",
    "        input corpus to be processed\n",
    "    only_word: bool\n",
    "        whether to filter out non-word\n",
    "    '''\n",
    "    word_dic = set()\n",
    "    processed_sentence = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        #將所有字詞轉為小寫\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        #移除標點符號(可以依據使用狀況決定是否要移除標點符號)\n",
    "        if only_word:\n",
    "            pattern = r'[^\\W_]+'\n",
    "            sentence = re.findall(pattern, sentence)\n",
    "        else:\n",
    "            punctuation_list = ['.', ',', '!', '?']\n",
    "            for pun in punctuation_list:\n",
    "                sentence = sentence.replace(pun, ' '+pun)\n",
    "            sentence = sentence.split(' ')\n",
    "        \n",
    "        #添加字詞到字典中\n",
    "        word_dic |= set(sentence)\n",
    "        processed_sentence.append(sentence)\n",
    "    \n",
    "    \n",
    "    #建立字詞ID清單\n",
    "    word2idx = dict()\n",
    "    idx2word = dict()\n",
    "    for word in word_dic:\n",
    "        if word not in word2idx:\n",
    "            idx = len(word2idx)\n",
    "            word2idx[word] = idx\n",
    "            idx2word[idx] = word\n",
    "\n",
    "    #將文本轉為ID型式\n",
    "    id_mapping = lambda x: word2idx[x]\n",
    "    \n",
    "    corpus = np.array([list(map(id_mapping, sentence)) for sentence in processed_sentence])\n",
    "\n",
    "    return corpus, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 2, 6, 5, 4, 3, 7, 1]]),\n",
       " {'i': 0,\n",
       "  '.': 1,\n",
       "  'am': 2,\n",
       "  'processing': 3,\n",
       "  'language': 4,\n",
       "  'natural': 5,\n",
       "  'studying': 6,\n",
       "  'now': 7},\n",
       " {0: 'i',\n",
       "  1: '.',\n",
       "  2: 'am',\n",
       "  3: 'processing',\n",
       "  4: 'language',\n",
       "  5: 'natural',\n",
       "  6: 'studying',\n",
       "  7: 'now'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test preprocessing\n",
    "text = \"I am studying Natural Language Processing now.\"\n",
    "corpus, word2idx, idx2word = preprocess([text])\n",
    "corpus, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to create contexts - target\n",
    "def create_contexts_target(corpus: List, window_size: int=1):\n",
    "\n",
    "    contexts = []\n",
    "    targets = corpus[window_size:-window_size]\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                # skip target word itself\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 2, 5, 4],\n",
       "        [2, 6, 4, 3],\n",
       "        [6, 5, 3, 7],\n",
       "        [5, 4, 7, 1]]),\n",
       " array([6, 5, 4, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test contexts target function\n",
    "contexts, targets= create_contexts_target(corpus[0], window_size=2)\n",
    "contexts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1]],\n",
       " \n",
       "        [[0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0]]], dtype=int32),\n",
       " array([[0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0]], dtype=int32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = convert_one_hot(contexts, len(word2idx))\n",
    "targets = convert_one_hot(targets, len(word2idx))\n",
    "contexts, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建CBOW模型\n",
    "\n",
    "在完成所以的前置作業(建構基本元素、資料前處理)後，可以開始搭建word2vec CBOW模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # initialize weights\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # create layers\n",
    "        self.in_layers = [Dense(W_in) for i in range(window_size*2)]\n",
    "        self.out_layer = Dense(W_out)\n",
    "        self.loss_layer = SoftmaxWithCrossEntropy()\n",
    "\n",
    "\n",
    "        layers = self.in_layers + [self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        hs = sum([self.in_layers[i].forward(contexts[:, i]) for i in range(self.window_size*2)])\n",
    "        hs /= self.window_size*2\n",
    "        \n",
    "        score = self.out_layer.forward(hs)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da /= self.window_size*2\n",
    "        \n",
    "        for i in range(self.window_size*2):\n",
    "            self.in_layers[i].backward(da)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer與Optimizer\n",
    "\n",
    "搭建完模型後，最後一個步驟就是需要一個optimizer來針對由loss取得的gradient來更新模型的參數，optimizer像是模型參數的更新策略，這邊因為重點不在optimizer，因此會使用最單純好懂的SGD(Stochastic Gradient Descent)來當作optimizer\n",
    "\n",
    "$$\n",
    "W_t = W_{t-1} - \\alpha\\frac{\\partial{L}}{\\partial{W_{t-1}}}\n",
    "$$\n",
    "\n",
    "當所有都準備就緒後，就是串起資料、模型、optimizer來開始進行訓練了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "class SGD:\n",
    "    '''\n",
    "    Stochastic Gradient Descent\n",
    "    '''\n",
    "    def __init__(self, lr=0.1):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "            \n",
    "# define trainer for training purpose\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(max_epoch)):\n",
    "            # shuffling\n",
    "            idx = np.random.permutation(np.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # calculate loss and update weights\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                # remove duplicate weights (for weights sharing purpose)\n",
    "                params, grads = remove_duplicate(model.params, model.grads) \n",
    "                # for gradient clipping purpose\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                    \n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # evaluation\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    print(f\"Epoch: {self.current_epoch+1}, Iteration: {iters+1}/{max_iters}, Loss: {avg_loss}\")\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = list(range(len(self.loss_list)))\n",
    "        \n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel(f\"iterations (x{self.eval_interval})\")\n",
    "        plt.ylabel(f\"loss\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練\n",
    "\n",
    "一切準備就緒，可以開始訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 275/1000 [00:00<00:00, 1225.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1/2, Loss: 1.945870612779989\n",
      "Epoch: 2, Iteration: 1/2, Loss: 1.9456582499549633\n",
      "Epoch: 3, Iteration: 1/2, Loss: 1.9458323664117319\n",
      "Epoch: 4, Iteration: 1/2, Loss: 1.94571469856855\n",
      "Epoch: 5, Iteration: 1/2, Loss: 1.9456842973942783\n",
      "Epoch: 6, Iteration: 1/2, Loss: 1.945660996697967\n",
      "Epoch: 7, Iteration: 1/2, Loss: 1.945710731817462\n",
      "Epoch: 8, Iteration: 1/2, Loss: 1.9457377070785467\n",
      "Epoch: 9, Iteration: 1/2, Loss: 1.9456168393699778\n",
      "Epoch: 10, Iteration: 1/2, Loss: 1.945554628595425\n",
      "Epoch: 11, Iteration: 1/2, Loss: 1.9456455456011217\n",
      "Epoch: 12, Iteration: 1/2, Loss: 1.9454995615161328\n",
      "Epoch: 13, Iteration: 1/2, Loss: 1.9455166585386874\n",
      "Epoch: 14, Iteration: 1/2, Loss: 1.9454838517358568\n",
      "Epoch: 15, Iteration: 1/2, Loss: 1.9455680895179488\n",
      "Epoch: 16, Iteration: 1/2, Loss: 1.945392517408041\n",
      "Epoch: 17, Iteration: 1/2, Loss: 1.945488623736027\n",
      "Epoch: 18, Iteration: 1/2, Loss: 1.9453674349216723\n",
      "Epoch: 19, Iteration: 1/2, Loss: 1.9453712254698472\n",
      "Epoch: 20, Iteration: 1/2, Loss: 1.945287149131357\n",
      "Epoch: 21, Iteration: 1/2, Loss: 1.9453096706098\n",
      "Epoch: 22, Iteration: 1/2, Loss: 1.9450679428358826\n",
      "Epoch: 23, Iteration: 1/2, Loss: 1.9453623940141762\n",
      "Epoch: 24, Iteration: 1/2, Loss: 1.945037981914413\n",
      "Epoch: 25, Iteration: 1/2, Loss: 1.9450917282822686\n",
      "Epoch: 26, Iteration: 1/2, Loss: 1.9449523852503274\n",
      "Epoch: 27, Iteration: 1/2, Loss: 1.9448706939949298\n",
      "Epoch: 28, Iteration: 1/2, Loss: 1.9449363384830578\n",
      "Epoch: 29, Iteration: 1/2, Loss: 1.9448208259890156\n",
      "Epoch: 30, Iteration: 1/2, Loss: 1.9446673768452314\n",
      "Epoch: 31, Iteration: 1/2, Loss: 1.9445724204011838\n",
      "Epoch: 32, Iteration: 1/2, Loss: 1.9444823009027596\n",
      "Epoch: 33, Iteration: 1/2, Loss: 1.9445467815764148\n",
      "Epoch: 34, Iteration: 1/2, Loss: 1.9442984839163104\n",
      "Epoch: 35, Iteration: 1/2, Loss: 1.9446216602802247\n",
      "Epoch: 36, Iteration: 1/2, Loss: 1.9438982207723474\n",
      "Epoch: 37, Iteration: 1/2, Loss: 1.9439092833295797\n",
      "Epoch: 38, Iteration: 1/2, Loss: 1.9442599521286592\n",
      "Epoch: 39, Iteration: 1/2, Loss: 1.94319142631459\n",
      "Epoch: 40, Iteration: 1/2, Loss: 1.9440425609088547\n",
      "Epoch: 41, Iteration: 1/2, Loss: 1.9432287421436167\n",
      "Epoch: 42, Iteration: 1/2, Loss: 1.9430494782326848\n",
      "Epoch: 43, Iteration: 1/2, Loss: 1.9430738453470895\n",
      "Epoch: 44, Iteration: 1/2, Loss: 1.9430031245414048\n",
      "Epoch: 45, Iteration: 1/2, Loss: 1.942464427131648\n",
      "Epoch: 46, Iteration: 1/2, Loss: 1.9418404423235192\n",
      "Epoch: 47, Iteration: 1/2, Loss: 1.9426054686417427\n",
      "Epoch: 48, Iteration: 1/2, Loss: 1.9424283958651662\n",
      "Epoch: 49, Iteration: 1/2, Loss: 1.9405847345342777\n",
      "Epoch: 50, Iteration: 1/2, Loss: 1.9425972377836345\n",
      "Epoch: 51, Iteration: 1/2, Loss: 1.940702565908713\n",
      "Epoch: 52, Iteration: 1/2, Loss: 1.9398545586418994\n",
      "Epoch: 53, Iteration: 1/2, Loss: 1.9393900536153446\n",
      "Epoch: 54, Iteration: 1/2, Loss: 1.9414681371799367\n",
      "Epoch: 55, Iteration: 1/2, Loss: 1.9384307488023071\n",
      "Epoch: 56, Iteration: 1/2, Loss: 1.938735020567651\n",
      "Epoch: 57, Iteration: 1/2, Loss: 1.9383166234842484\n",
      "Epoch: 58, Iteration: 1/2, Loss: 1.9393853569299875\n",
      "Epoch: 59, Iteration: 1/2, Loss: 1.9349096505258416\n",
      "Epoch: 60, Iteration: 1/2, Loss: 1.939065076288879\n",
      "Epoch: 61, Iteration: 1/2, Loss: 1.934249143102646\n",
      "Epoch: 62, Iteration: 1/2, Loss: 1.936651589367142\n",
      "Epoch: 63, Iteration: 1/2, Loss: 1.934092776258558\n",
      "Epoch: 64, Iteration: 1/2, Loss: 1.9306268419189012\n",
      "Epoch: 65, Iteration: 1/2, Loss: 1.9324806075085192\n",
      "Epoch: 66, Iteration: 1/2, Loss: 1.9353022082884237\n",
      "Epoch: 67, Iteration: 1/2, Loss: 1.9287652524749477\n",
      "Epoch: 68, Iteration: 1/2, Loss: 1.9300238730667223\n",
      "Epoch: 69, Iteration: 1/2, Loss: 1.92757982573595\n",
      "Epoch: 70, Iteration: 1/2, Loss: 1.9294174232535224\n",
      "Epoch: 71, Iteration: 1/2, Loss: 1.923077048078004\n",
      "Epoch: 72, Iteration: 1/2, Loss: 1.9247457758252449\n",
      "Epoch: 73, Iteration: 1/2, Loss: 1.9224860900835463\n",
      "Epoch: 74, Iteration: 1/2, Loss: 1.9157136537594148\n",
      "Epoch: 75, Iteration: 1/2, Loss: 1.9232752495749659\n",
      "Epoch: 76, Iteration: 1/2, Loss: 1.9206896812921959\n",
      "Epoch: 77, Iteration: 1/2, Loss: 1.909964476803684\n",
      "Epoch: 78, Iteration: 1/2, Loss: 1.9122569118683865\n",
      "Epoch: 79, Iteration: 1/2, Loss: 1.911524574676299\n",
      "Epoch: 80, Iteration: 1/2, Loss: 1.9058496155735165\n",
      "Epoch: 81, Iteration: 1/2, Loss: 1.9047434945686617\n",
      "Epoch: 82, Iteration: 1/2, Loss: 1.9029270601146893\n",
      "Epoch: 83, Iteration: 1/2, Loss: 1.8971033661063734\n",
      "Epoch: 84, Iteration: 1/2, Loss: 1.8965294282366632\n",
      "Epoch: 85, Iteration: 1/2, Loss: 1.8937898042173573\n",
      "Epoch: 86, Iteration: 1/2, Loss: 1.8832791785627925\n",
      "Epoch: 87, Iteration: 1/2, Loss: 1.883269131311069\n",
      "Epoch: 88, Iteration: 1/2, Loss: 1.8876412373877438\n",
      "Epoch: 89, Iteration: 1/2, Loss: 1.8805930686988739\n",
      "Epoch: 90, Iteration: 1/2, Loss: 1.8579026632445483\n",
      "Epoch: 91, Iteration: 1/2, Loss: 1.8523509600483576\n",
      "Epoch: 92, Iteration: 1/2, Loss: 1.8730025292822496\n",
      "Epoch: 93, Iteration: 1/2, Loss: 1.8450011261387114\n",
      "Epoch: 94, Iteration: 1/2, Loss: 1.85372268608857\n",
      "Epoch: 95, Iteration: 1/2, Loss: 1.8356234846758368\n",
      "Epoch: 96, Iteration: 1/2, Loss: 1.83573840222166\n",
      "Epoch: 97, Iteration: 1/2, Loss: 1.8202820900352148\n",
      "Epoch: 98, Iteration: 1/2, Loss: 1.8100133566226766\n",
      "Epoch: 99, Iteration: 1/2, Loss: 1.8135857507303699\n",
      "Epoch: 100, Iteration: 1/2, Loss: 1.7840643623010137\n",
      "Epoch: 101, Iteration: 1/2, Loss: 1.8042817730310432\n",
      "Epoch: 102, Iteration: 1/2, Loss: 1.7791712860670534\n",
      "Epoch: 103, Iteration: 1/2, Loss: 1.7655565335806176\n",
      "Epoch: 104, Iteration: 1/2, Loss: 1.7917017819780212\n",
      "Epoch: 105, Iteration: 1/2, Loss: 1.6794255772149747\n",
      "Epoch: 106, Iteration: 1/2, Loss: 1.8058248606047123\n",
      "Epoch: 107, Iteration: 1/2, Loss: 1.694224806454867\n",
      "Epoch: 108, Iteration: 1/2, Loss: 1.6973012708904693\n",
      "Epoch: 109, Iteration: 1/2, Loss: 1.6874034719330189\n",
      "Epoch: 110, Iteration: 1/2, Loss: 1.6882031599291947\n",
      "Epoch: 111, Iteration: 1/2, Loss: 1.7098393867285917\n",
      "Epoch: 112, Iteration: 1/2, Loss: 1.560813393880418\n",
      "Epoch: 113, Iteration: 1/2, Loss: 1.700261120207593\n",
      "Epoch: 114, Iteration: 1/2, Loss: 1.5606012147118962\n",
      "Epoch: 115, Iteration: 1/2, Loss: 1.6671676437811491\n",
      "Epoch: 116, Iteration: 1/2, Loss: 1.5402528447057637\n",
      "Epoch: 117, Iteration: 1/2, Loss: 1.5430759633991102\n",
      "Epoch: 118, Iteration: 1/2, Loss: 1.6061665311007292\n",
      "Epoch: 119, Iteration: 1/2, Loss: 1.5373949383027266\n",
      "Epoch: 120, Iteration: 1/2, Loss: 1.5202808207186949\n",
      "Epoch: 121, Iteration: 1/2, Loss: 1.5347591436144952\n",
      "Epoch: 122, Iteration: 1/2, Loss: 1.4775134408908048\n",
      "Epoch: 123, Iteration: 1/2, Loss: 1.5450899677445324\n",
      "Epoch: 124, Iteration: 1/2, Loss: 1.351647645195063\n",
      "Epoch: 125, Iteration: 1/2, Loss: 1.4695519323820823\n",
      "Epoch: 126, Iteration: 1/2, Loss: 1.4190229541206452\n",
      "Epoch: 127, Iteration: 1/2, Loss: 1.3659449697470165\n",
      "Epoch: 128, Iteration: 1/2, Loss: 1.4214579193858259\n",
      "Epoch: 129, Iteration: 1/2, Loss: 1.4502470132123244\n",
      "Epoch: 130, Iteration: 1/2, Loss: 1.1909947830696457\n",
      "Epoch: 131, Iteration: 1/2, Loss: 1.5045256820894766\n",
      "Epoch: 132, Iteration: 1/2, Loss: 1.129004477359122\n",
      "Epoch: 133, Iteration: 1/2, Loss: 1.3964839156888564\n",
      "Epoch: 134, Iteration: 1/2, Loss: 1.3327080050924938\n",
      "Epoch: 135, Iteration: 1/2, Loss: 1.2825564171259511\n",
      "Epoch: 136, Iteration: 1/2, Loss: 1.2328726645045929\n",
      "Epoch: 137, Iteration: 1/2, Loss: 1.256918585342668\n",
      "Epoch: 138, Iteration: 1/2, Loss: 1.244754586652705\n",
      "Epoch: 139, Iteration: 1/2, Loss: 1.365001648191929\n",
      "Epoch: 140, Iteration: 1/2, Loss: 1.127084307793732\n",
      "Epoch: 141, Iteration: 1/2, Loss: 1.2680870912056244\n",
      "Epoch: 142, Iteration: 1/2, Loss: 1.1482984866728876\n",
      "Epoch: 143, Iteration: 1/2, Loss: 1.2472861418715042\n",
      "Epoch: 144, Iteration: 1/2, Loss: 1.1191821113542504\n",
      "Epoch: 145, Iteration: 1/2, Loss: 1.1806903999916563\n",
      "Epoch: 146, Iteration: 1/2, Loss: 1.161902479407519\n",
      "Epoch: 147, Iteration: 1/2, Loss: 1.1069669035884178\n",
      "Epoch: 148, Iteration: 1/2, Loss: 1.2939289608051783\n",
      "Epoch: 149, Iteration: 1/2, Loss: 1.1431111796217928\n",
      "Epoch: 150, Iteration: 1/2, Loss: 0.983927793988788\n",
      "Epoch: 151, Iteration: 1/2, Loss: 1.2579302741351255\n",
      "Epoch: 152, Iteration: 1/2, Loss: 1.0062301438225116\n",
      "Epoch: 153, Iteration: 1/2, Loss: 1.1071633954494646\n",
      "Epoch: 154, Iteration: 1/2, Loss: 0.9466291007081364\n",
      "Epoch: 155, Iteration: 1/2, Loss: 1.1159296510591223\n",
      "Epoch: 156, Iteration: 1/2, Loss: 1.1857613692651208\n",
      "Epoch: 157, Iteration: 1/2, Loss: 1.0632541156453135\n",
      "Epoch: 158, Iteration: 1/2, Loss: 1.0241519417471305\n",
      "Epoch: 159, Iteration: 1/2, Loss: 1.1959637477559895\n",
      "Epoch: 160, Iteration: 1/2, Loss: 0.8899294964038877\n",
      "Epoch: 161, Iteration: 1/2, Loss: 1.220488709798973\n",
      "Epoch: 162, Iteration: 1/2, Loss: 1.1564115678377238\n",
      "Epoch: 163, Iteration: 1/2, Loss: 0.9406862532871331\n",
      "Epoch: 164, Iteration: 1/2, Loss: 0.8888547968712915\n",
      "Epoch: 165, Iteration: 1/2, Loss: 1.3207709444843139\n",
      "Epoch: 166, Iteration: 1/2, Loss: 0.9222186533623226\n",
      "Epoch: 167, Iteration: 1/2, Loss: 1.0496427031719682\n",
      "Epoch: 168, Iteration: 1/2, Loss: 1.1256030491456817\n",
      "Epoch: 169, Iteration: 1/2, Loss: 0.9083924077209213\n",
      "Epoch: 170, Iteration: 1/2, Loss: 0.9809536155140739\n",
      "Epoch: 171, Iteration: 1/2, Loss: 1.014381367458181\n",
      "Epoch: 172, Iteration: 1/2, Loss: 1.0096857755232724\n",
      "Epoch: 173, Iteration: 1/2, Loss: 1.006405135147208\n",
      "Epoch: 174, Iteration: 1/2, Loss: 0.9830152917006095\n",
      "Epoch: 175, Iteration: 1/2, Loss: 1.0533149536269322\n",
      "Epoch: 176, Iteration: 1/2, Loss: 1.0137840081383402\n",
      "Epoch: 177, Iteration: 1/2, Loss: 0.9901029182921641\n",
      "Epoch: 178, Iteration: 1/2, Loss: 0.9296459197991555\n",
      "Epoch: 179, Iteration: 1/2, Loss: 0.9849705727226604\n",
      "Epoch: 180, Iteration: 1/2, Loss: 1.0373638381356736\n",
      "Epoch: 181, Iteration: 1/2, Loss: 0.9015500682810283\n",
      "Epoch: 182, Iteration: 1/2, Loss: 1.0290905286965035\n",
      "Epoch: 183, Iteration: 1/2, Loss: 0.8185840167113025\n",
      "Epoch: 184, Iteration: 1/2, Loss: 1.1291983855121108\n",
      "Epoch: 185, Iteration: 1/2, Loss: 0.9822703352200686\n",
      "Epoch: 186, Iteration: 1/2, Loss: 0.9604971331647485\n",
      "Epoch: 187, Iteration: 1/2, Loss: 0.9024225851371868\n",
      "Epoch: 188, Iteration: 1/2, Loss: 0.9398758649177271\n",
      "Epoch: 189, Iteration: 1/2, Loss: 1.0056204516579954\n",
      "Epoch: 190, Iteration: 1/2, Loss: 0.9506969341742861\n",
      "Epoch: 191, Iteration: 1/2, Loss: 0.8942009359570426\n",
      "Epoch: 192, Iteration: 1/2, Loss: 1.1046444206413693\n",
      "Epoch: 193, Iteration: 1/2, Loss: 0.8543218431790316\n",
      "Epoch: 194, Iteration: 1/2, Loss: 0.7737443878154626\n",
      "Epoch: 195, Iteration: 1/2, Loss: 1.0912548112543223\n",
      "Epoch: 196, Iteration: 1/2, Loss: 0.8820877427978987\n",
      "Epoch: 197, Iteration: 1/2, Loss: 0.9277309089874157\n",
      "Epoch: 198, Iteration: 1/2, Loss: 0.9254607528397808\n",
      "Epoch: 199, Iteration: 1/2, Loss: 0.921835242379889\n",
      "Epoch: 200, Iteration: 1/2, Loss: 0.82393083756316\n",
      "Epoch: 201, Iteration: 1/2, Loss: 1.0333381853415005\n",
      "Epoch: 202, Iteration: 1/2, Loss: 0.8102378696773533\n",
      "Epoch: 203, Iteration: 1/2, Loss: 1.0608454777675862\n",
      "Epoch: 204, Iteration: 1/2, Loss: 0.8756424342580654\n",
      "Epoch: 205, Iteration: 1/2, Loss: 0.9409022392088675\n",
      "Epoch: 206, Iteration: 1/2, Loss: 1.0081334882951445\n",
      "Epoch: 207, Iteration: 1/2, Loss: 0.7612662887618257\n",
      "Epoch: 208, Iteration: 1/2, Loss: 0.8001988855895388\n",
      "Epoch: 209, Iteration: 1/2, Loss: 0.9130742566303999\n",
      "Epoch: 210, Iteration: 1/2, Loss: 0.9765355710389345\n",
      "Epoch: 211, Iteration: 1/2, Loss: 0.8892974307759107\n",
      "Epoch: 212, Iteration: 1/2, Loss: 0.8857109921820926\n",
      "Epoch: 213, Iteration: 1/2, Loss: 0.7868876217791085\n",
      "Epoch: 214, Iteration: 1/2, Loss: 0.8616124630471977\n",
      "Epoch: 215, Iteration: 1/2, Loss: 0.9847986780388491\n",
      "Epoch: 216, Iteration: 1/2, Loss: 0.9692134752423724\n",
      "Epoch: 217, Iteration: 1/2, Loss: 0.8983936576640028\n",
      "Epoch: 218, Iteration: 1/2, Loss: 0.7365107325307877\n",
      "Epoch: 219, Iteration: 1/2, Loss: 0.78854388234246\n",
      "Epoch: 220, Iteration: 1/2, Loss: 0.9597538330596153\n",
      "Epoch: 221, Iteration: 1/2, Loss: 0.8306117634750467\n",
      "Epoch: 222, Iteration: 1/2, Loss: 0.8756100662713993\n",
      "Epoch: 223, Iteration: 1/2, Loss: 0.8505688212570611\n",
      "Epoch: 224, Iteration: 1/2, Loss: 0.8406432067752091\n",
      "Epoch: 225, Iteration: 1/2, Loss: 0.7161570883508432\n",
      "Epoch: 226, Iteration: 1/2, Loss: 0.8478631581654359\n",
      "Epoch: 227, Iteration: 1/2, Loss: 0.9351027528373297\n",
      "Epoch: 228, Iteration: 1/2, Loss: 0.7168324149426144\n",
      "Epoch: 229, Iteration: 1/2, Loss: 0.849743036406046\n",
      "Epoch: 230, Iteration: 1/2, Loss: 0.9181903374237941\n",
      "Epoch: 231, Iteration: 1/2, Loss: 0.9080911290560996\n",
      "Epoch: 232, Iteration: 1/2, Loss: 0.6166222898210703\n",
      "Epoch: 233, Iteration: 1/2, Loss: 0.9137084394986498\n",
      "Epoch: 234, Iteration: 1/2, Loss: 0.8142509718866244\n",
      "Epoch: 235, Iteration: 1/2, Loss: 0.70678314986317\n",
      "Epoch: 236, Iteration: 1/2, Loss: 0.8896025181897929\n",
      "Epoch: 237, Iteration: 1/2, Loss: 0.7854016441055222\n",
      "Epoch: 238, Iteration: 1/2, Loss: 0.6903626379760037\n",
      "Epoch: 239, Iteration: 1/2, Loss: 0.8846821172130712\n",
      "Epoch: 240, Iteration: 1/2, Loss: 0.7720380182690197\n",
      "Epoch: 241, Iteration: 1/2, Loss: 0.7676395098968505\n",
      "Epoch: 242, Iteration: 1/2, Loss: 0.8558009631429713\n",
      "Epoch: 243, Iteration: 1/2, Loss: 0.6902248645334129\n",
      "Epoch: 244, Iteration: 1/2, Loss: 0.7460335092080279\n",
      "Epoch: 245, Iteration: 1/2, Loss: 0.840662781632989\n",
      "Epoch: 246, Iteration: 1/2, Loss: 0.7562675632879124\n",
      "Epoch: 247, Iteration: 1/2, Loss: 0.6555916333800921\n",
      "Epoch: 248, Iteration: 1/2, Loss: 0.7574934819747644\n",
      "Epoch: 249, Iteration: 1/2, Loss: 0.7974909369015816\n",
      "Epoch: 250, Iteration: 1/2, Loss: 0.6508772670390727\n",
      "Epoch: 251, Iteration: 1/2, Loss: 0.6367217212670875\n",
      "Epoch: 252, Iteration: 1/2, Loss: 0.77947408040631\n",
      "Epoch: 253, Iteration: 1/2, Loss: 0.722384650545883\n",
      "Epoch: 254, Iteration: 1/2, Loss: 0.7062466172758282\n",
      "Epoch: 255, Iteration: 1/2, Loss: 0.7042568258690239\n",
      "Epoch: 256, Iteration: 1/2, Loss: 0.6134657185834634\n",
      "Epoch: 257, Iteration: 1/2, Loss: 0.7638629670573385\n",
      "Epoch: 258, Iteration: 1/2, Loss: 0.6969573179139978\n",
      "Epoch: 259, Iteration: 1/2, Loss: 0.610347425826374\n",
      "Epoch: 260, Iteration: 1/2, Loss: 0.7421482339035783\n",
      "Epoch: 261, Iteration: 1/2, Loss: 0.7642386752276485\n",
      "Epoch: 262, Iteration: 1/2, Loss: 0.5589660282225689\n",
      "Epoch: 263, Iteration: 1/2, Loss: 0.6756911213307759\n",
      "Epoch: 264, Iteration: 1/2, Loss: 0.6117535024824046\n",
      "Epoch: 265, Iteration: 1/2, Loss: 0.7133674786482107\n",
      "Epoch: 266, Iteration: 1/2, Loss: 0.544707698872083\n",
      "Epoch: 267, Iteration: 1/2, Loss: 0.7111278026471545\n",
      "Epoch: 268, Iteration: 1/2, Loss: 0.6313297804485853\n",
      "Epoch: 269, Iteration: 1/2, Loss: 0.6298830993502156\n",
      "Epoch: 270, Iteration: 1/2, Loss: 0.505079894640613\n",
      "Epoch: 271, Iteration: 1/2, Loss: 0.7331824795197601\n",
      "Epoch: 272, Iteration: 1/2, Loss: 0.5538007647114478\n",
      "Epoch: 273, Iteration: 1/2, Loss: 0.594969132858179\n",
      "Epoch: 274, Iteration: 1/2, Loss: 0.7039425455872907\n",
      "Epoch: 275, Iteration: 1/2, Loss: 0.5273028802017182\n",
      "Epoch: 276, Iteration: 1/2, Loss: 0.6846797804740897\n",
      "Epoch: 277, Iteration: 1/2, Loss: 0.5031096441001337\n",
      "Epoch: 278, Iteration: 1/2, Loss: 0.5146083138372707\n",
      "Epoch: 279, Iteration: 1/2, Loss: 0.7279347349395473\n",
      "Epoch: 280, Iteration: 1/2, Loss: 0.4041610297820931\n",
      "Epoch: 281, Iteration: 1/2, Loss: 0.555767673625547\n",
      "Epoch: 282, Iteration: 1/2, Loss: 0.5945031739600508\n",
      "Epoch: 283, Iteration: 1/2, Loss: 0.6256385100418316\n",
      "Epoch: 284, Iteration: 1/2, Loss: 0.4983393340619531\n",
      "Epoch: 285, Iteration: 1/2, Loss: 0.4703060417988505\n",
      "Epoch: 286, Iteration: 1/2, Loss: 0.6051550801229213\n",
      "Epoch: 287, Iteration: 1/2, Loss: 0.556197879060093\n",
      "Epoch: 288, Iteration: 1/2, Loss: 0.4886042645076807\n",
      "Epoch: 289, Iteration: 1/2, Loss: 0.6502099981513596\n",
      "Epoch: 290, Iteration: 1/2, Loss: 0.39368935406582833\n",
      "Epoch: 291, Iteration: 1/2, Loss: 0.5158856926546715\n",
      "Epoch: 292, Iteration: 1/2, Loss: 0.5953953930212899\n",
      "Epoch: 293, Iteration: 1/2, Loss: 0.3643659434513028\n",
      "Epoch: 294, Iteration: 1/2, Loss: 0.5765907867845239\n",
      "Epoch: 295, Iteration: 1/2, Loss: 0.5971392552057823\n",
      "Epoch: 296, Iteration: 1/2, Loss: 0.3092364587046769\n",
      "Epoch: 297, Iteration: 1/2, Loss: 0.4420550222360523\n",
      "Epoch: 298, Iteration: 1/2, Loss: 0.6064429731416326\n",
      "Epoch: 299, Iteration: 1/2, Loss: 0.48455352677490415\n",
      "Epoch: 300, Iteration: 1/2, Loss: 0.3808819050353325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 661/1000 [00:00<00:00, 1506.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301, Iteration: 1/2, Loss: 0.43720092198979466\n",
      "Epoch: 302, Iteration: 1/2, Loss: 0.555589279230305\n",
      "Epoch: 303, Iteration: 1/2, Loss: 0.5694642058660747\n",
      "Epoch: 304, Iteration: 1/2, Loss: 0.4559958262044751\n",
      "Epoch: 305, Iteration: 1/2, Loss: 0.36030653145183955\n",
      "Epoch: 306, Iteration: 1/2, Loss: 0.483617570298711\n",
      "Epoch: 307, Iteration: 1/2, Loss: 0.45447971220063343\n",
      "Epoch: 308, Iteration: 1/2, Loss: 0.35686423716750443\n",
      "Epoch: 309, Iteration: 1/2, Loss: 0.530993688708382\n",
      "Epoch: 310, Iteration: 1/2, Loss: 0.5417726380355752\n",
      "Epoch: 311, Iteration: 1/2, Loss: 0.34148683018623865\n",
      "Epoch: 312, Iteration: 1/2, Loss: 0.33801191051614454\n",
      "Epoch: 313, Iteration: 1/2, Loss: 0.4179455344317061\n",
      "Epoch: 314, Iteration: 1/2, Loss: 0.4923589418808176\n",
      "Epoch: 315, Iteration: 1/2, Loss: 0.45977896848001726\n",
      "Epoch: 316, Iteration: 1/2, Loss: 0.4028134323001852\n",
      "Epoch: 317, Iteration: 1/2, Loss: 0.44495011179791866\n",
      "Epoch: 318, Iteration: 1/2, Loss: 0.3154762900162595\n",
      "Epoch: 319, Iteration: 1/2, Loss: 0.5030947554660575\n",
      "Epoch: 320, Iteration: 1/2, Loss: 0.496874956925782\n",
      "Epoch: 321, Iteration: 1/2, Loss: 0.3959374309368295\n",
      "Epoch: 322, Iteration: 1/2, Loss: 0.3220954424258903\n",
      "Epoch: 323, Iteration: 1/2, Loss: 0.4027164245243579\n",
      "Epoch: 324, Iteration: 1/2, Loss: 0.4077933333504171\n",
      "Epoch: 325, Iteration: 1/2, Loss: 0.38084819366625683\n",
      "Epoch: 326, Iteration: 1/2, Loss: 0.39486242642808156\n",
      "Epoch: 327, Iteration: 1/2, Loss: 0.3187069930949448\n",
      "Epoch: 328, Iteration: 1/2, Loss: 0.5875924479077824\n",
      "Epoch: 329, Iteration: 1/2, Loss: 0.28214848020909233\n",
      "Epoch: 330, Iteration: 1/2, Loss: 0.38769629330584743\n",
      "Epoch: 331, Iteration: 1/2, Loss: 0.3696525737761467\n",
      "Epoch: 332, Iteration: 1/2, Loss: 0.3859190258531008\n",
      "Epoch: 333, Iteration: 1/2, Loss: 0.4839653329230011\n",
      "Epoch: 334, Iteration: 1/2, Loss: 0.2867894111097731\n",
      "Epoch: 335, Iteration: 1/2, Loss: 0.4627040025242155\n",
      "Epoch: 336, Iteration: 1/2, Loss: 0.265288821628073\n",
      "Epoch: 337, Iteration: 1/2, Loss: 0.3737638353917388\n",
      "Epoch: 338, Iteration: 1/2, Loss: 0.3771711258606799\n",
      "Epoch: 339, Iteration: 1/2, Loss: 0.3717574685875604\n",
      "Epoch: 340, Iteration: 1/2, Loss: 0.3854479770334448\n",
      "Epoch: 341, Iteration: 1/2, Loss: 0.3435462932675164\n",
      "Epoch: 342, Iteration: 1/2, Loss: 0.2674216845183454\n",
      "Epoch: 343, Iteration: 1/2, Loss: 0.3540793933442304\n",
      "Epoch: 344, Iteration: 1/2, Loss: 0.47553574011765215\n",
      "Epoch: 345, Iteration: 1/2, Loss: 0.2529823788852643\n",
      "Epoch: 346, Iteration: 1/2, Loss: 0.43888646346565113\n",
      "Epoch: 347, Iteration: 1/2, Loss: 0.35946203734721405\n",
      "Epoch: 348, Iteration: 1/2, Loss: 0.3591934831823603\n",
      "Epoch: 349, Iteration: 1/2, Loss: 0.26017741027066676\n",
      "Epoch: 350, Iteration: 1/2, Loss: 0.4299934302426549\n",
      "Epoch: 351, Iteration: 1/2, Loss: 0.35272200068592985\n",
      "Epoch: 352, Iteration: 1/2, Loss: 0.3640271965969817\n",
      "Epoch: 353, Iteration: 1/2, Loss: 0.3367914157352515\n",
      "Epoch: 354, Iteration: 1/2, Loss: 0.4541707005657605\n",
      "Epoch: 355, Iteration: 1/2, Loss: 0.13354232113172781\n",
      "Epoch: 356, Iteration: 1/2, Loss: 0.42699634203019865\n",
      "Epoch: 357, Iteration: 1/2, Loss: 0.34287396304323337\n",
      "Epoch: 358, Iteration: 1/2, Loss: 0.34496948249008397\n",
      "Epoch: 359, Iteration: 1/2, Loss: 0.3414718274835577\n",
      "Epoch: 360, Iteration: 1/2, Loss: 0.44491404820321656\n",
      "Epoch: 361, Iteration: 1/2, Loss: 0.2285549081985791\n",
      "Epoch: 362, Iteration: 1/2, Loss: 0.3267150119672423\n",
      "Epoch: 363, Iteration: 1/2, Loss: 0.3491179304798656\n",
      "Epoch: 364, Iteration: 1/2, Loss: 0.4316267475895366\n",
      "Epoch: 365, Iteration: 1/2, Loss: 0.21452183670126312\n",
      "Epoch: 366, Iteration: 1/2, Loss: 0.3335343002076996\n",
      "Epoch: 367, Iteration: 1/2, Loss: 0.33829813557876437\n",
      "Epoch: 368, Iteration: 1/2, Loss: 0.3341000222281712\n",
      "Epoch: 369, Iteration: 1/2, Loss: 0.33028189568076705\n",
      "Epoch: 370, Iteration: 1/2, Loss: 0.31904102093834025\n",
      "Epoch: 371, Iteration: 1/2, Loss: 0.33250563415821277\n",
      "Epoch: 372, Iteration: 1/2, Loss: 0.31681864629152057\n",
      "Epoch: 373, Iteration: 1/2, Loss: 0.3300005720709735\n",
      "Epoch: 374, Iteration: 1/2, Loss: 0.2258537434168229\n",
      "Epoch: 375, Iteration: 1/2, Loss: 0.5194598315333886\n",
      "Epoch: 376, Iteration: 1/2, Loss: 0.2012408582819674\n",
      "Epoch: 377, Iteration: 1/2, Loss: 0.22395977914432386\n",
      "Epoch: 378, Iteration: 1/2, Loss: 0.4168954776030864\n",
      "Epoch: 379, Iteration: 1/2, Loss: 0.31276412781571583\n",
      "Epoch: 380, Iteration: 1/2, Loss: 0.32678318804237017\n",
      "Epoch: 381, Iteration: 1/2, Loss: 0.32131407342635887\n",
      "Epoch: 382, Iteration: 1/2, Loss: 0.30938354366621557\n",
      "Epoch: 383, Iteration: 1/2, Loss: 0.21605560528315013\n",
      "Epoch: 384, Iteration: 1/2, Loss: 0.4028140543804666\n",
      "Epoch: 385, Iteration: 1/2, Loss: 0.3175930608982472\n",
      "Epoch: 386, Iteration: 1/2, Loss: 0.3157321695465877\n",
      "Epoch: 387, Iteration: 1/2, Loss: 0.3191472736290808\n",
      "Epoch: 388, Iteration: 1/2, Loss: 0.4163042828145944\n",
      "Epoch: 389, Iteration: 1/2, Loss: 0.1998909711191971\n",
      "Epoch: 390, Iteration: 1/2, Loss: 0.3049541285111922\n",
      "Epoch: 391, Iteration: 1/2, Loss: 0.2083742136482724\n",
      "Epoch: 392, Iteration: 1/2, Loss: 0.28932749831889104\n",
      "Epoch: 393, Iteration: 1/2, Loss: 0.41756183906045685\n",
      "Epoch: 394, Iteration: 1/2, Loss: 0.30253670552036643\n",
      "Epoch: 395, Iteration: 1/2, Loss: 0.30373943451119667\n",
      "Epoch: 396, Iteration: 1/2, Loss: 0.3196490909880672\n",
      "Epoch: 397, Iteration: 1/2, Loss: 0.40951075764217215\n",
      "Epoch: 398, Iteration: 1/2, Loss: 0.1851807496464817\n",
      "Epoch: 399, Iteration: 1/2, Loss: 0.41412235174772005\n",
      "Epoch: 400, Iteration: 1/2, Loss: 0.18935942056972904\n",
      "Epoch: 401, Iteration: 1/2, Loss: 0.2985212778318668\n",
      "Epoch: 402, Iteration: 1/2, Loss: 0.3042672647860202\n",
      "Epoch: 403, Iteration: 1/2, Loss: 0.19442202400272995\n",
      "Epoch: 404, Iteration: 1/2, Loss: 0.5199671812436821\n",
      "Epoch: 405, Iteration: 1/2, Loss: 0.18113440323052804\n",
      "Epoch: 406, Iteration: 1/2, Loss: 0.30052916415101283\n",
      "Epoch: 407, Iteration: 1/2, Loss: 0.30313073085276754\n",
      "Epoch: 408, Iteration: 1/2, Loss: 0.2924843778575595\n",
      "Epoch: 409, Iteration: 1/2, Loss: 0.4205991643539141\n",
      "Epoch: 410, Iteration: 1/2, Loss: 0.16920879180987822\n",
      "Epoch: 411, Iteration: 1/2, Loss: 0.2997598409715233\n",
      "Epoch: 412, Iteration: 1/2, Loss: 0.4108025615145734\n",
      "Epoch: 413, Iteration: 1/2, Loss: 0.1757914299309033\n",
      "Epoch: 414, Iteration: 1/2, Loss: 0.2979378760995669\n",
      "Epoch: 415, Iteration: 1/2, Loss: 0.30718119654773685\n",
      "Epoch: 416, Iteration: 1/2, Loss: 0.291290269067486\n",
      "Epoch: 417, Iteration: 1/2, Loss: 0.2984936512674363\n",
      "Epoch: 418, Iteration: 1/2, Loss: 0.2968110378586375\n",
      "Epoch: 419, Iteration: 1/2, Loss: 0.29712485748002915\n",
      "Epoch: 420, Iteration: 1/2, Loss: 0.29002906128124545\n",
      "Epoch: 421, Iteration: 1/2, Loss: 0.41313656823672046\n",
      "Epoch: 422, Iteration: 1/2, Loss: 0.16509893016032431\n",
      "Epoch: 423, Iteration: 1/2, Loss: 0.2952046867911269\n",
      "Epoch: 424, Iteration: 1/2, Loss: 0.4047945984381328\n",
      "Epoch: 425, Iteration: 1/2, Loss: 0.2792972456442151\n",
      "Epoch: 426, Iteration: 1/2, Loss: 0.17428830537255846\n",
      "Epoch: 427, Iteration: 1/2, Loss: 0.29477043896592164\n",
      "Epoch: 428, Iteration: 1/2, Loss: 0.29219752331795656\n",
      "Epoch: 429, Iteration: 1/2, Loss: 0.2836846913420285\n",
      "Epoch: 430, Iteration: 1/2, Loss: 0.40595252019331646\n",
      "Epoch: 431, Iteration: 1/2, Loss: 0.2781655484633886\n",
      "Epoch: 432, Iteration: 1/2, Loss: 0.17211444201402054\n",
      "Epoch: 433, Iteration: 1/2, Loss: 0.3952198570315607\n",
      "Epoch: 434, Iteration: 1/2, Loss: 0.16957494761591796\n",
      "Epoch: 435, Iteration: 1/2, Loss: 0.28632046233643044\n",
      "Epoch: 436, Iteration: 1/2, Loss: 0.28713126443526404\n",
      "Epoch: 437, Iteration: 1/2, Loss: 0.2963924912152512\n",
      "Epoch: 438, Iteration: 1/2, Loss: 0.4013047127546762\n",
      "Epoch: 439, Iteration: 1/2, Loss: 0.15624269971859112\n",
      "Epoch: 440, Iteration: 1/2, Loss: 0.18361663690177246\n",
      "Epoch: 441, Iteration: 1/2, Loss: 0.38628430578331996\n",
      "Epoch: 442, Iteration: 1/2, Loss: 0.2839674100308125\n",
      "Epoch: 443, Iteration: 1/2, Loss: 0.29243844405904396\n",
      "Epoch: 444, Iteration: 1/2, Loss: 0.39217674912647954\n",
      "Epoch: 445, Iteration: 1/2, Loss: 0.16974751847891578\n",
      "Epoch: 446, Iteration: 1/2, Loss: 0.17018372635645737\n",
      "Epoch: 447, Iteration: 1/2, Loss: 0.4984588831935829\n",
      "Epoch: 448, Iteration: 1/2, Loss: 0.15352117645841284\n",
      "Epoch: 449, Iteration: 1/2, Loss: 0.17458000327383655\n",
      "Epoch: 450, Iteration: 1/2, Loss: 0.2774609037874788\n",
      "Epoch: 451, Iteration: 1/2, Loss: 0.49213422399349754\n",
      "Epoch: 452, Iteration: 1/2, Loss: 0.15652531993676852\n",
      "Epoch: 453, Iteration: 1/2, Loss: 0.2909709855607578\n",
      "Epoch: 454, Iteration: 1/2, Loss: 0.28296709455248115\n",
      "Epoch: 455, Iteration: 1/2, Loss: 0.2784789560613424\n",
      "Epoch: 456, Iteration: 1/2, Loss: 0.2847424321194326\n",
      "Epoch: 457, Iteration: 1/2, Loss: 0.2866426095899591\n",
      "Epoch: 458, Iteration: 1/2, Loss: 0.392408352962359\n",
      "Epoch: 459, Iteration: 1/2, Loss: 0.15775756168739166\n",
      "Epoch: 460, Iteration: 1/2, Loss: 0.39423450114939673\n",
      "Epoch: 461, Iteration: 1/2, Loss: 0.0487901598521845\n",
      "Epoch: 462, Iteration: 1/2, Loss: 0.4900657562041577\n",
      "Epoch: 463, Iteration: 1/2, Loss: 0.15579029581573034\n",
      "Epoch: 464, Iteration: 1/2, Loss: 0.3928602879042866\n",
      "Epoch: 465, Iteration: 1/2, Loss: 0.15455900557311225\n",
      "Epoch: 466, Iteration: 1/2, Loss: 0.17393589666810175\n",
      "Epoch: 467, Iteration: 1/2, Loss: 0.37929089229373214\n",
      "Epoch: 468, Iteration: 1/2, Loss: 0.2763106617840318\n",
      "Epoch: 469, Iteration: 1/2, Loss: 0.28149864890702025\n",
      "Epoch: 470, Iteration: 1/2, Loss: 0.28218836756330523\n",
      "Epoch: 471, Iteration: 1/2, Loss: 0.2811376582180153\n",
      "Epoch: 472, Iteration: 1/2, Loss: 0.3884440135658205\n",
      "Epoch: 473, Iteration: 1/2, Loss: 0.2672977342137746\n",
      "Epoch: 474, Iteration: 1/2, Loss: 0.1503346870322632\n",
      "Epoch: 475, Iteration: 1/2, Loss: 0.28033141191635313\n",
      "Epoch: 476, Iteration: 1/2, Loss: 0.28119633006415334\n",
      "Epoch: 477, Iteration: 1/2, Loss: 0.27851250240712133\n",
      "Epoch: 478, Iteration: 1/2, Loss: 0.38963526358184664\n",
      "Epoch: 479, Iteration: 1/2, Loss: 0.26477436375818897\n",
      "Epoch: 480, Iteration: 1/2, Loss: 0.15600319013258268\n",
      "Epoch: 481, Iteration: 1/2, Loss: 0.3864684673002051\n",
      "Epoch: 482, Iteration: 1/2, Loss: 0.15306442906483528\n",
      "Epoch: 483, Iteration: 1/2, Loss: 0.27414396701602056\n",
      "Epoch: 484, Iteration: 1/2, Loss: 0.2817366655481499\n",
      "Epoch: 485, Iteration: 1/2, Loss: 0.2751565023168956\n",
      "Epoch: 486, Iteration: 1/2, Loss: 0.16718241547456275\n",
      "Epoch: 487, Iteration: 1/2, Loss: 0.4892707157109536\n",
      "Epoch: 488, Iteration: 1/2, Loss: 0.03688573473213889\n",
      "Epoch: 489, Iteration: 1/2, Loss: 0.3760245656823735\n",
      "Epoch: 490, Iteration: 1/2, Loss: 0.2749209848770727\n",
      "Epoch: 491, Iteration: 1/2, Loss: 0.2732313616641495\n",
      "Epoch: 492, Iteration: 1/2, Loss: 0.3888473423370178\n",
      "Epoch: 493, Iteration: 1/2, Loss: 0.2661678149805117\n",
      "Epoch: 494, Iteration: 1/2, Loss: 0.1476845802415478\n",
      "Epoch: 495, Iteration: 1/2, Loss: 0.27231158337799677\n",
      "Epoch: 496, Iteration: 1/2, Loss: 0.16201953859953694\n",
      "Epoch: 497, Iteration: 1/2, Loss: 0.3748200461376505\n",
      "Epoch: 498, Iteration: 1/2, Loss: 0.2799639657980366\n",
      "Epoch: 499, Iteration: 1/2, Loss: 0.2745202442215859\n",
      "Epoch: 500, Iteration: 1/2, Loss: 0.2705450299315273\n",
      "Epoch: 501, Iteration: 1/2, Loss: 0.2750499653791638\n",
      "Epoch: 502, Iteration: 1/2, Loss: 0.38595498087995717\n",
      "Epoch: 503, Iteration: 1/2, Loss: 0.1460533027753127\n",
      "Epoch: 504, Iteration: 1/2, Loss: 0.2740062966545581\n",
      "Epoch: 505, Iteration: 1/2, Loss: 0.3893200157058837\n",
      "Epoch: 506, Iteration: 1/2, Loss: 0.14522395986002487\n",
      "Epoch: 507, Iteration: 1/2, Loss: 0.1639789222326317\n",
      "Epoch: 508, Iteration: 1/2, Loss: 0.36892148003356384\n",
      "Epoch: 509, Iteration: 1/2, Loss: 0.2752468391022102\n",
      "Epoch: 510, Iteration: 1/2, Loss: 0.27171366813133757\n",
      "Epoch: 511, Iteration: 1/2, Loss: 0.38587350835830614\n",
      "Epoch: 512, Iteration: 1/2, Loss: 0.25896356965024436\n",
      "Epoch: 513, Iteration: 1/2, Loss: 0.1452999447158567\n",
      "Epoch: 514, Iteration: 1/2, Loss: 0.16159392676832843\n",
      "Epoch: 515, Iteration: 1/2, Loss: 0.37407223425676883\n",
      "Epoch: 516, Iteration: 1/2, Loss: 0.27046044767672417\n",
      "Epoch: 517, Iteration: 1/2, Loss: 0.26859784208606974\n",
      "Epoch: 518, Iteration: 1/2, Loss: 0.27483047215475986\n",
      "Epoch: 519, Iteration: 1/2, Loss: 0.2724719985087132\n",
      "Epoch: 520, Iteration: 1/2, Loss: 0.15897938213762972\n",
      "Epoch: 521, Iteration: 1/2, Loss: 0.37107126871149765\n",
      "Epoch: 522, Iteration: 1/2, Loss: 0.1568763829317386\n",
      "Epoch: 523, Iteration: 1/2, Loss: 0.3710917589571233\n",
      "Epoch: 524, Iteration: 1/2, Loss: 0.2722792883165195\n",
      "Epoch: 525, Iteration: 1/2, Loss: 0.26862192889558834\n",
      "Epoch: 526, Iteration: 1/2, Loss: 0.15841460733532692\n",
      "Epoch: 527, Iteration: 1/2, Loss: 0.4817320229774884\n",
      "Epoch: 528, Iteration: 1/2, Loss: 0.14626286462456883\n",
      "Epoch: 529, Iteration: 1/2, Loss: 0.15608246860873717\n",
      "Epoch: 530, Iteration: 1/2, Loss: 0.3728956536821617\n",
      "Epoch: 531, Iteration: 1/2, Loss: 0.3824014980593577\n",
      "Epoch: 532, Iteration: 1/2, Loss: 0.14373690772175654\n",
      "Epoch: 533, Iteration: 1/2, Loss: 0.2687167374750311\n",
      "Epoch: 534, Iteration: 1/2, Loss: 0.384041052194718\n",
      "Epoch: 535, Iteration: 1/2, Loss: 0.1443582566651545\n",
      "Epoch: 536, Iteration: 1/2, Loss: 0.38144188720134425\n",
      "Epoch: 537, Iteration: 1/2, Loss: 0.13861215874832514\n",
      "Epoch: 538, Iteration: 1/2, Loss: 0.1591854341767818\n",
      "Epoch: 539, Iteration: 1/2, Loss: 0.3675071135751604\n",
      "Epoch: 540, Iteration: 1/2, Loss: 0.26950484963679777\n",
      "Epoch: 541, Iteration: 1/2, Loss: 0.2686205465746774\n",
      "Epoch: 542, Iteration: 1/2, Loss: 0.2689947224077938\n",
      "Epoch: 543, Iteration: 1/2, Loss: 0.27198524456657625\n",
      "Epoch: 544, Iteration: 1/2, Loss: 0.26301443487936554\n",
      "Epoch: 545, Iteration: 1/2, Loss: 0.27332208845794914\n",
      "Epoch: 546, Iteration: 1/2, Loss: 0.26658169584049085\n",
      "Epoch: 547, Iteration: 1/2, Loss: 0.2727441861417841\n",
      "Epoch: 548, Iteration: 1/2, Loss: 0.15322875518348186\n",
      "Epoch: 549, Iteration: 1/2, Loss: 0.25433525761108017\n",
      "Epoch: 550, Iteration: 1/2, Loss: 0.3663361477758086\n",
      "Epoch: 551, Iteration: 1/2, Loss: 0.26469364140349727\n",
      "Epoch: 552, Iteration: 1/2, Loss: 0.2709515176582748\n",
      "Epoch: 553, Iteration: 1/2, Loss: 0.38126549649367814\n",
      "Epoch: 554, Iteration: 1/2, Loss: 0.2538134106691316\n",
      "Epoch: 555, Iteration: 1/2, Loss: 0.14187529069018517\n",
      "Epoch: 556, Iteration: 1/2, Loss: 0.38261157345619845\n",
      "Epoch: 557, Iteration: 1/2, Loss: 0.13914712748034067\n",
      "Epoch: 558, Iteration: 1/2, Loss: 0.1540985313314502\n",
      "Epoch: 559, Iteration: 1/2, Loss: 0.36901646930173226\n",
      "Epoch: 560, Iteration: 1/2, Loss: 0.2619732149961651\n",
      "Epoch: 561, Iteration: 1/2, Loss: 0.3818215868271212\n",
      "Epoch: 562, Iteration: 1/2, Loss: 0.13835724973496388\n",
      "Epoch: 563, Iteration: 1/2, Loss: 0.38170825349648324\n",
      "Epoch: 564, Iteration: 1/2, Loss: 0.14120989072038595\n",
      "Epoch: 565, Iteration: 1/2, Loss: 0.2658325732298955\n",
      "Epoch: 566, Iteration: 1/2, Loss: 0.26622985217523953\n",
      "Epoch: 567, Iteration: 1/2, Loss: 0.26524920925719364\n",
      "Epoch: 568, Iteration: 1/2, Loss: 0.2679163815370021\n",
      "Epoch: 569, Iteration: 1/2, Loss: 0.3810448658658742\n",
      "Epoch: 570, Iteration: 1/2, Loss: 0.14150263760687687\n",
      "Epoch: 571, Iteration: 1/2, Loss: 0.3761033684401996\n",
      "Epoch: 572, Iteration: 1/2, Loss: 0.02688898198240847\n",
      "Epoch: 573, Iteration: 1/2, Loss: 0.36505845693443273\n",
      "Epoch: 574, Iteration: 1/2, Loss: 0.15023000010490106\n",
      "Epoch: 575, Iteration: 1/2, Loss: 0.47995998169281673\n",
      "Epoch: 576, Iteration: 1/2, Loss: 0.14259223525693315\n",
      "Epoch: 577, Iteration: 1/2, Loss: 0.26403890315168266\n",
      "Epoch: 578, Iteration: 1/2, Loss: 0.2591801671523638\n",
      "Epoch: 579, Iteration: 1/2, Loss: 0.2684574483870809\n",
      "Epoch: 580, Iteration: 1/2, Loss: 0.1553509747405902\n",
      "Epoch: 581, Iteration: 1/2, Loss: 0.36332944074750206\n",
      "Epoch: 582, Iteration: 1/2, Loss: 0.3816118694735373\n",
      "Epoch: 583, Iteration: 1/2, Loss: 0.020270944941525808\n",
      "Epoch: 584, Iteration: 1/2, Loss: 0.36647782150572306\n",
      "Epoch: 585, Iteration: 1/2, Loss: 0.2657680983772673\n",
      "Epoch: 586, Iteration: 1/2, Loss: 0.1519727587085739\n",
      "Epoch: 587, Iteration: 1/2, Loss: 0.3647664295494036\n",
      "Epoch: 588, Iteration: 1/2, Loss: 0.26630256396762975\n",
      "Epoch: 589, Iteration: 1/2, Loss: 0.2633946641728717\n",
      "Epoch: 590, Iteration: 1/2, Loss: 0.2658081037210696\n",
      "Epoch: 591, Iteration: 1/2, Loss: 0.26458618765315733\n",
      "Epoch: 592, Iteration: 1/2, Loss: 0.37639666814265704\n",
      "Epoch: 593, Iteration: 1/2, Loss: 0.2504220465791476\n",
      "Epoch: 594, Iteration: 1/2, Loss: 0.13830634020821428\n",
      "Epoch: 595, Iteration: 1/2, Loss: 0.26220801906449265\n",
      "Epoch: 596, Iteration: 1/2, Loss: 0.26514877233261447\n",
      "Epoch: 597, Iteration: 1/2, Loss: 0.2664536042357889\n",
      "Epoch: 598, Iteration: 1/2, Loss: 0.2638777309181868\n",
      "Epoch: 599, Iteration: 1/2, Loss: 0.37761422005608136\n",
      "Epoch: 600, Iteration: 1/2, Loss: 0.020397985380978895\n",
      "Epoch: 601, Iteration: 1/2, Loss: 0.3686954916353311\n",
      "Epoch: 602, Iteration: 1/2, Loss: 0.14777269884367772\n",
      "Epoch: 603, Iteration: 1/2, Loss: 0.47558935809021985\n",
      "Epoch: 604, Iteration: 1/2, Loss: 0.13759843254457585\n",
      "Epoch: 605, Iteration: 1/2, Loss: 0.26104905497057973\n",
      "Epoch: 606, Iteration: 1/2, Loss: 0.15268563148897285\n",
      "Epoch: 607, Iteration: 1/2, Loss: 0.47533745568617397\n",
      "Epoch: 608, Iteration: 1/2, Loss: 0.25028873481503355\n",
      "Epoch: 609, Iteration: 1/2, Loss: 0.13381838440144614\n",
      "Epoch: 610, Iteration: 1/2, Loss: 0.264801580539837\n",
      "Epoch: 611, Iteration: 1/2, Loss: 0.15057024077483466\n",
      "Epoch: 612, Iteration: 1/2, Loss: 0.4775431681725837\n",
      "Epoch: 613, Iteration: 1/2, Loss: 0.1339273510859898\n",
      "Epoch: 614, Iteration: 1/2, Loss: 0.26430800791198295\n",
      "Epoch: 615, Iteration: 1/2, Loss: 0.2655358524352205\n",
      "Epoch: 616, Iteration: 1/2, Loss: 0.25671917054406257\n",
      "Epoch: 617, Iteration: 1/2, Loss: 0.15197209703935122\n",
      "Epoch: 618, Iteration: 1/2, Loss: 0.36042172892527535\n",
      "Epoch: 619, Iteration: 1/2, Loss: 0.2667913986329431\n",
      "Epoch: 620, Iteration: 1/2, Loss: 0.14738322055335595\n",
      "Epoch: 621, Iteration: 1/2, Loss: 0.4756539215175687\n",
      "Epoch: 622, Iteration: 1/2, Loss: 0.13620677070115286\n",
      "Epoch: 623, Iteration: 1/2, Loss: 0.3749875236741552\n",
      "Epoch: 624, Iteration: 1/2, Loss: 0.24824283751751247\n",
      "Epoch: 625, Iteration: 1/2, Loss: 0.24817860398581743\n",
      "Epoch: 626, Iteration: 1/2, Loss: 0.1339380312545291\n",
      "Epoch: 627, Iteration: 1/2, Loss: 0.15039126614733112\n",
      "Epoch: 628, Iteration: 1/2, Loss: 0.3635423730614004\n",
      "Epoch: 629, Iteration: 1/2, Loss: 0.14692875494122168\n",
      "Epoch: 630, Iteration: 1/2, Loss: 0.3632294576910895\n",
      "Epoch: 631, Iteration: 1/2, Loss: 0.26427667518797837\n",
      "Epoch: 632, Iteration: 1/2, Loss: 0.3716120015740471\n",
      "Epoch: 633, Iteration: 1/2, Loss: 0.13875926430567417\n",
      "Epoch: 634, Iteration: 1/2, Loss: 0.2539789284850625\n",
      "Epoch: 635, Iteration: 1/2, Loss: 0.15293433747095464\n",
      "Epoch: 636, Iteration: 1/2, Loss: 0.3606055994865427\n",
      "Epoch: 637, Iteration: 1/2, Loss: 0.26152961736477937\n",
      "Epoch: 638, Iteration: 1/2, Loss: 0.3760408494753401\n",
      "Epoch: 639, Iteration: 1/2, Loss: 0.13317174344763089\n",
      "Epoch: 640, Iteration: 1/2, Loss: 0.2652000111252029\n",
      "Epoch: 641, Iteration: 1/2, Loss: 0.14396893863903398\n",
      "Epoch: 642, Iteration: 1/2, Loss: 0.361213511051899\n",
      "Epoch: 643, Iteration: 1/2, Loss: 0.26105007164944216\n",
      "Epoch: 644, Iteration: 1/2, Loss: 0.26395933721291687\n",
      "Epoch: 645, Iteration: 1/2, Loss: 0.2625052660869919\n",
      "Epoch: 646, Iteration: 1/2, Loss: 0.14641307556617558\n",
      "Epoch: 647, Iteration: 1/2, Loss: 0.2478483018585832\n",
      "Epoch: 648, Iteration: 1/2, Loss: 0.3600406514184385\n",
      "Epoch: 649, Iteration: 1/2, Loss: 0.25855777083019515\n",
      "Epoch: 650, Iteration: 1/2, Loss: 0.26375794895670474\n",
      "Epoch: 651, Iteration: 1/2, Loss: 0.3775743268307223\n",
      "Epoch: 652, Iteration: 1/2, Loss: 0.12874559732373136\n",
      "Epoch: 653, Iteration: 1/2, Loss: 0.26258360915988677\n",
      "Epoch: 654, Iteration: 1/2, Loss: 0.2592841865269996\n",
      "Epoch: 655, Iteration: 1/2, Loss: 0.26280463373893237\n",
      "Epoch: 656, Iteration: 1/2, Loss: 0.26530915292736623\n",
      "Epoch: 657, Iteration: 1/2, Loss: 0.2554994123747133\n",
      "Epoch: 658, Iteration: 1/2, Loss: 0.3772620478671611\n",
      "Epoch: 659, Iteration: 1/2, Loss: 0.24633411428196145\n",
      "Epoch: 660, Iteration: 1/2, Loss: 0.01937577106246289\n",
      "Epoch: 661, Iteration: 1/2, Loss: 0.4752032147494546\n",
      "Epoch: 662, Iteration: 1/2, Loss: 0.2450414769484797\n",
      "Epoch: 663, Iteration: 1/2, Loss: 0.016205380404272482\n",
      "Epoch: 664, Iteration: 1/2, Loss: 0.3598984216819088\n",
      "Epoch: 665, Iteration: 1/2, Loss: 0.2608924531852985\n",
      "Epoch: 666, Iteration: 1/2, Loss: 0.26199411668160466\n",
      "Epoch: 667, Iteration: 1/2, Loss: 0.26084366886983257\n",
      "Epoch: 668, Iteration: 1/2, Loss: 0.2624757716226832\n",
      "Epoch: 669, Iteration: 1/2, Loss: 0.14517586307587915\n",
      "Epoch: 670, Iteration: 1/2, Loss: 0.3581702571869165\n",
      "Epoch: 671, Iteration: 1/2, Loss: 0.2591702156071847\n",
      "Epoch: 672, Iteration: 1/2, Loss: 0.2611950949893087\n",
      "Epoch: 673, Iteration: 1/2, Loss: 0.2597275187594509\n",
      "Epoch: 674, Iteration: 1/2, Loss: 0.26454408511137645\n",
      "Epoch: 675, Iteration: 1/2, Loss: 0.25640732816533074\n",
      "Epoch: 676, Iteration: 1/2, Loss: 0.14870874660512454\n",
      "Epoch: 677, Iteration: 1/2, Loss: 0.3605549751987734\n",
      "Epoch: 678, Iteration: 1/2, Loss: 0.14419879496047877\n",
      "Epoch: 679, Iteration: 1/2, Loss: 0.4743347142748989\n",
      "Epoch: 680, Iteration: 1/2, Loss: 0.13120226073974173\n",
      "Epoch: 681, Iteration: 1/2, Loss: 0.2610942742889724\n",
      "Epoch: 682, Iteration: 1/2, Loss: 0.25639352065638366\n",
      "Epoch: 683, Iteration: 1/2, Loss: 0.26052849239748177\n",
      "Epoch: 684, Iteration: 1/2, Loss: 0.375194249292848\n",
      "Epoch: 685, Iteration: 1/2, Loss: 0.1299449290334719\n",
      "Epoch: 686, Iteration: 1/2, Loss: 0.3750630678219574\n",
      "Epoch: 687, Iteration: 1/2, Loss: 0.1291787687520194\n",
      "Epoch: 688, Iteration: 1/2, Loss: 0.3765326740795928\n",
      "Epoch: 689, Iteration: 1/2, Loss: 0.129545784731051\n",
      "Epoch: 690, Iteration: 1/2, Loss: 0.14692364370992808\n",
      "Epoch: 691, Iteration: 1/2, Loss: 0.47221052457854207\n",
      "Epoch: 692, Iteration: 1/2, Loss: 0.016663652103347533\n",
      "Epoch: 693, Iteration: 1/2, Loss: 0.35931180206793706\n",
      "Epoch: 694, Iteration: 1/2, Loss: 0.37409816659302314\n",
      "Epoch: 695, Iteration: 1/2, Loss: 0.24472549938603078\n",
      "Epoch: 696, Iteration: 1/2, Loss: 0.01644749642557528\n",
      "Epoch: 697, Iteration: 1/2, Loss: 0.3587585473692764\n",
      "Epoch: 698, Iteration: 1/2, Loss: 0.3726938243373531\n",
      "Epoch: 699, Iteration: 1/2, Loss: 0.016312906667291217\n",
      "Epoch: 700, Iteration: 1/2, Loss: 0.24537197024702584\n",
      "Epoch: 701, Iteration: 1/2, Loss: 0.24448304305154855\n",
      "Epoch: 702, Iteration: 1/2, Loss: 0.3586060585322817\n",
      "Epoch: 703, Iteration: 1/2, Loss: 0.2563134132542638\n",
      "Epoch: 704, Iteration: 1/2, Loss: 0.14612094783526264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1724.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 705, Iteration: 1/2, Loss: 0.4736139243513207\n",
      "Epoch: 706, Iteration: 1/2, Loss: 0.24428668414757235\n",
      "Epoch: 707, Iteration: 1/2, Loss: 0.13029436599717742\n",
      "Epoch: 708, Iteration: 1/2, Loss: 0.26023224658764554\n",
      "Epoch: 709, Iteration: 1/2, Loss: 0.2561712136039237\n",
      "Epoch: 710, Iteration: 1/2, Loss: 0.25963373306102866\n",
      "Epoch: 711, Iteration: 1/2, Loss: 0.26206986498745666\n",
      "Epoch: 712, Iteration: 1/2, Loss: 0.25828389365469057\n",
      "Epoch: 713, Iteration: 1/2, Loss: 0.2538408535760537\n",
      "Epoch: 714, Iteration: 1/2, Loss: 0.26325288812135333\n",
      "Epoch: 715, Iteration: 1/2, Loss: 0.3739009673950882\n",
      "Epoch: 716, Iteration: 1/2, Loss: 0.12960032239510128\n",
      "Epoch: 717, Iteration: 1/2, Loss: 0.2606491005927094\n",
      "Epoch: 718, Iteration: 1/2, Loss: 0.25184712561661415\n",
      "Epoch: 719, Iteration: 1/2, Loss: 0.26546721047229266\n",
      "Epoch: 720, Iteration: 1/2, Loss: 0.2596349282072592\n",
      "Epoch: 721, Iteration: 1/2, Loss: 0.2519196032569499\n",
      "Epoch: 722, Iteration: 1/2, Loss: 0.26115956571079185\n",
      "Epoch: 723, Iteration: 1/2, Loss: 0.37272355796803613\n",
      "Epoch: 724, Iteration: 1/2, Loss: 0.13039065676460324\n",
      "Epoch: 725, Iteration: 1/2, Loss: 0.37257767938752523\n",
      "Epoch: 726, Iteration: 1/2, Loss: 0.1265647561467946\n",
      "Epoch: 727, Iteration: 1/2, Loss: 0.26063230438532203\n",
      "Epoch: 728, Iteration: 1/2, Loss: 0.2558203968443614\n",
      "Epoch: 729, Iteration: 1/2, Loss: 0.14640238525778795\n",
      "Epoch: 730, Iteration: 1/2, Loss: 0.24344112861584363\n",
      "Epoch: 731, Iteration: 1/2, Loss: 0.4719617835640792\n",
      "Epoch: 732, Iteration: 1/2, Loss: 0.12995475110253013\n",
      "Epoch: 733, Iteration: 1/2, Loss: 0.2577715714135018\n",
      "Epoch: 734, Iteration: 1/2, Loss: 0.14356094960370183\n",
      "Epoch: 735, Iteration: 1/2, Loss: 0.4735839742617015\n",
      "Epoch: 736, Iteration: 1/2, Loss: 0.12881034215941295\n",
      "Epoch: 737, Iteration: 1/2, Loss: 0.1414623483561429\n",
      "Epoch: 738, Iteration: 1/2, Loss: 0.3621212408813116\n",
      "Epoch: 739, Iteration: 1/2, Loss: 0.3704123704959372\n",
      "Epoch: 740, Iteration: 1/2, Loss: 0.12881513319420848\n",
      "Epoch: 741, Iteration: 1/2, Loss: 0.14289893663252878\n",
      "Epoch: 742, Iteration: 1/2, Loss: 0.3555645427427403\n",
      "Epoch: 743, Iteration: 1/2, Loss: 0.2600939147799295\n",
      "Epoch: 744, Iteration: 1/2, Loss: 0.1417267733554468\n",
      "Epoch: 745, Iteration: 1/2, Loss: 0.3572573394957885\n",
      "Epoch: 746, Iteration: 1/2, Loss: 0.1436101531632344\n",
      "Epoch: 747, Iteration: 1/2, Loss: 0.3578684032647026\n",
      "Epoch: 748, Iteration: 1/2, Loss: 0.259063046164067\n",
      "Epoch: 749, Iteration: 1/2, Loss: 0.3719125734318771\n",
      "Epoch: 750, Iteration: 1/2, Loss: 0.12752017702686874\n",
      "Epoch: 751, Iteration: 1/2, Loss: 0.3722489778123605\n",
      "Epoch: 752, Iteration: 1/2, Loss: 0.12779381395244147\n",
      "Epoch: 753, Iteration: 1/2, Loss: 0.3726610050415212\n",
      "Epoch: 754, Iteration: 1/2, Loss: 0.013962045220432925\n",
      "Epoch: 755, Iteration: 1/2, Loss: 0.24343126810132412\n",
      "Epoch: 756, Iteration: 1/2, Loss: 0.35700494216175144\n",
      "Epoch: 757, Iteration: 1/2, Loss: 0.1416159948423301\n",
      "Epoch: 758, Iteration: 1/2, Loss: 0.2426015986491662\n",
      "Epoch: 759, Iteration: 1/2, Loss: 0.35566661686831613\n",
      "Epoch: 760, Iteration: 1/2, Loss: 0.3739734967840904\n",
      "Epoch: 761, Iteration: 1/2, Loss: 0.12899519238437207\n",
      "Epoch: 762, Iteration: 1/2, Loss: 0.1421485437760464\n",
      "Epoch: 763, Iteration: 1/2, Loss: 0.3557037818853205\n",
      "Epoch: 764, Iteration: 1/2, Loss: 0.2589692824538282\n",
      "Epoch: 765, Iteration: 1/2, Loss: 0.2586440326073445\n",
      "Epoch: 766, Iteration: 1/2, Loss: 0.37002864924832257\n",
      "Epoch: 767, Iteration: 1/2, Loss: 0.1269888135592612\n",
      "Epoch: 768, Iteration: 1/2, Loss: 0.372619181060932\n",
      "Epoch: 769, Iteration: 1/2, Loss: 0.12876287892807203\n",
      "Epoch: 770, Iteration: 1/2, Loss: 0.14111880445993322\n",
      "Epoch: 771, Iteration: 1/2, Loss: 0.3577884651071889\n",
      "Epoch: 772, Iteration: 1/2, Loss: 0.3728229962633386\n",
      "Epoch: 773, Iteration: 1/2, Loss: 0.12878004286368905\n",
      "Epoch: 774, Iteration: 1/2, Loss: 0.2581174185019911\n",
      "Epoch: 775, Iteration: 1/2, Loss: 0.3681601096092586\n",
      "Epoch: 776, Iteration: 1/2, Loss: 0.12548309486546774\n",
      "Epoch: 777, Iteration: 1/2, Loss: 0.3736870498477206\n",
      "Epoch: 778, Iteration: 1/2, Loss: 0.012364192966333913\n",
      "Epoch: 779, Iteration: 1/2, Loss: 0.35802960935090566\n",
      "Epoch: 780, Iteration: 1/2, Loss: 0.2581135926319058\n",
      "Epoch: 781, Iteration: 1/2, Loss: 0.37106070188532514\n",
      "Epoch: 782, Iteration: 1/2, Loss: 0.12982952825181057\n",
      "Epoch: 783, Iteration: 1/2, Loss: 0.2563576652988168\n",
      "Epoch: 784, Iteration: 1/2, Loss: 0.13852529129447083\n",
      "Epoch: 785, Iteration: 1/2, Loss: 0.3531876059848086\n",
      "Epoch: 786, Iteration: 1/2, Loss: 0.3768086861494637\n",
      "Epoch: 787, Iteration: 1/2, Loss: 0.1278017298670402\n",
      "Epoch: 788, Iteration: 1/2, Loss: 0.25713615582399535\n",
      "Epoch: 789, Iteration: 1/2, Loss: 0.2577031484822151\n",
      "Epoch: 790, Iteration: 1/2, Loss: 0.25059036770785187\n",
      "Epoch: 791, Iteration: 1/2, Loss: 0.1460955158847846\n",
      "Epoch: 792, Iteration: 1/2, Loss: 0.3585468978592916\n",
      "Epoch: 793, Iteration: 1/2, Loss: 0.2503039826840005\n",
      "Epoch: 794, Iteration: 1/2, Loss: 0.26094671670196656\n",
      "Epoch: 795, Iteration: 1/2, Loss: 0.14047340264819935\n",
      "Epoch: 796, Iteration: 1/2, Loss: 0.3592230075146581\n",
      "Epoch: 797, Iteration: 1/2, Loss: 0.25230686959332205\n",
      "Epoch: 798, Iteration: 1/2, Loss: 0.2589595214585951\n",
      "Epoch: 799, Iteration: 1/2, Loss: 0.25651250845687834\n",
      "Epoch: 800, Iteration: 1/2, Loss: 0.37204327308289964\n",
      "Epoch: 801, Iteration: 1/2, Loss: 0.12561439159847776\n",
      "Epoch: 802, Iteration: 1/2, Loss: 0.2569497845992612\n",
      "Epoch: 803, Iteration: 1/2, Loss: 0.37040941948555417\n",
      "Epoch: 804, Iteration: 1/2, Loss: 0.011609761107641767\n",
      "Epoch: 805, Iteration: 1/2, Loss: 0.3558312743290084\n",
      "Epoch: 806, Iteration: 1/2, Loss: 0.25704003339214215\n",
      "Epoch: 807, Iteration: 1/2, Loss: 0.25824456061029966\n",
      "Epoch: 808, Iteration: 1/2, Loss: 0.2519493430240543\n",
      "Epoch: 809, Iteration: 1/2, Loss: 0.25800023293814767\n",
      "Epoch: 810, Iteration: 1/2, Loss: 0.2583783802975985\n",
      "Epoch: 811, Iteration: 1/2, Loss: 0.1406201862199587\n",
      "Epoch: 812, Iteration: 1/2, Loss: 0.47171793539574886\n",
      "Epoch: 813, Iteration: 1/2, Loss: 0.23980009194437307\n",
      "Epoch: 814, Iteration: 1/2, Loss: 0.12749132300238186\n",
      "Epoch: 815, Iteration: 1/2, Loss: 0.37126262661171844\n",
      "Epoch: 816, Iteration: 1/2, Loss: 0.2404486734480278\n",
      "Epoch: 817, Iteration: 1/2, Loss: 0.12785433463588672\n",
      "Epoch: 818, Iteration: 1/2, Loss: 0.3700760386024818\n",
      "Epoch: 819, Iteration: 1/2, Loss: 0.24105972798738523\n",
      "Epoch: 820, Iteration: 1/2, Loss: 0.12412748704910266\n",
      "Epoch: 821, Iteration: 1/2, Loss: 0.25758910059964385\n",
      "Epoch: 822, Iteration: 1/2, Loss: 0.25801952841375786\n",
      "Epoch: 823, Iteration: 1/2, Loss: 0.36814813235617305\n",
      "Epoch: 824, Iteration: 1/2, Loss: 0.12588394989116455\n",
      "Epoch: 825, Iteration: 1/2, Loss: 0.14186501569576146\n",
      "Epoch: 826, Iteration: 1/2, Loss: 0.47005846079732283\n",
      "Epoch: 827, Iteration: 1/2, Loss: 0.12608786591611867\n",
      "Epoch: 828, Iteration: 1/2, Loss: 0.37140421029849774\n",
      "Epoch: 829, Iteration: 1/2, Loss: 0.12394906060270723\n",
      "Epoch: 830, Iteration: 1/2, Loss: 0.2593280460277362\n",
      "Epoch: 831, Iteration: 1/2, Loss: 0.13878965130145837\n",
      "Epoch: 832, Iteration: 1/2, Loss: 0.47064624699219804\n",
      "Epoch: 833, Iteration: 1/2, Loss: 0.012187178116811455\n",
      "Epoch: 834, Iteration: 1/2, Loss: 0.35371881245141623\n",
      "Epoch: 835, Iteration: 1/2, Loss: 0.2572156165109589\n",
      "Epoch: 836, Iteration: 1/2, Loss: 0.1413653171363568\n",
      "Epoch: 837, Iteration: 1/2, Loss: 0.24065550687825657\n",
      "Epoch: 838, Iteration: 1/2, Loss: 0.35279709935452147\n",
      "Epoch: 839, Iteration: 1/2, Loss: 0.1435700010384298\n",
      "Epoch: 840, Iteration: 1/2, Loss: 0.23927140319969925\n",
      "Epoch: 841, Iteration: 1/2, Loss: 0.35736147888142905\n",
      "Epoch: 842, Iteration: 1/2, Loss: 0.2562462166332361\n",
      "Epoch: 843, Iteration: 1/2, Loss: 0.13895821595770744\n",
      "Epoch: 844, Iteration: 1/2, Loss: 0.24052501161082407\n",
      "Epoch: 845, Iteration: 1/2, Loss: 0.4704540269590649\n",
      "Epoch: 846, Iteration: 1/2, Loss: 0.12717803948744022\n",
      "Epoch: 847, Iteration: 1/2, Loss: 0.2512050761860362\n",
      "Epoch: 848, Iteration: 1/2, Loss: 0.2578503919490607\n",
      "Epoch: 849, Iteration: 1/2, Loss: 0.2576038372497319\n",
      "Epoch: 850, Iteration: 1/2, Loss: 0.25671186772859506\n",
      "Epoch: 851, Iteration: 1/2, Loss: 0.2491054052485564\n",
      "Epoch: 852, Iteration: 1/2, Loss: 0.14488755303533915\n",
      "Epoch: 853, Iteration: 1/2, Loss: 0.35350109139127295\n",
      "Epoch: 854, Iteration: 1/2, Loss: 0.3709460801853799\n",
      "Epoch: 855, Iteration: 1/2, Loss: 0.12453799749608835\n",
      "Epoch: 856, Iteration: 1/2, Loss: 0.25742920672879716\n",
      "Epoch: 857, Iteration: 1/2, Loss: 0.2531511523495944\n",
      "Epoch: 858, Iteration: 1/2, Loss: 0.25718061947471627\n",
      "Epoch: 859, Iteration: 1/2, Loss: 0.25272052544997514\n",
      "Epoch: 860, Iteration: 1/2, Loss: 0.14229637362039887\n",
      "Epoch: 861, Iteration: 1/2, Loss: 0.470139153613489\n",
      "Epoch: 862, Iteration: 1/2, Loss: 0.1261483361250183\n",
      "Epoch: 863, Iteration: 1/2, Loss: 0.2572056135755354\n",
      "Epoch: 864, Iteration: 1/2, Loss: 0.3680979848655811\n",
      "Epoch: 865, Iteration: 1/2, Loss: 0.1267369318329113\n",
      "Epoch: 866, Iteration: 1/2, Loss: 0.13925731716786938\n",
      "Epoch: 867, Iteration: 1/2, Loss: 0.3524129530863147\n",
      "Epoch: 868, Iteration: 1/2, Loss: 0.25772750605516626\n",
      "Epoch: 869, Iteration: 1/2, Loss: 0.2569322711749557\n",
      "Epoch: 870, Iteration: 1/2, Loss: 0.2561693349002711\n",
      "Epoch: 871, Iteration: 1/2, Loss: 0.3677759176954898\n",
      "Epoch: 872, Iteration: 1/2, Loss: 0.12235409991579214\n",
      "Epoch: 873, Iteration: 1/2, Loss: 0.2593438879598754\n",
      "Epoch: 874, Iteration: 1/2, Loss: 0.25098328817426885\n",
      "Epoch: 875, Iteration: 1/2, Loss: 0.25865288764219446\n",
      "Epoch: 876, Iteration: 1/2, Loss: 0.250791882956666\n",
      "Epoch: 877, Iteration: 1/2, Loss: 0.1433709678470289\n",
      "Epoch: 878, Iteration: 1/2, Loss: 0.3543621288507353\n",
      "Epoch: 879, Iteration: 1/2, Loss: 0.13988606215585275\n",
      "Epoch: 880, Iteration: 1/2, Loss: 0.35607403901740564\n",
      "Epoch: 881, Iteration: 1/2, Loss: 0.2520889738857368\n",
      "Epoch: 882, Iteration: 1/2, Loss: 0.25655205519780067\n",
      "Epoch: 883, Iteration: 1/2, Loss: 0.2562732979183141\n",
      "Epoch: 884, Iteration: 1/2, Loss: 0.13813589300772428\n",
      "Epoch: 885, Iteration: 1/2, Loss: 0.3561654552691177\n",
      "Epoch: 886, Iteration: 1/2, Loss: 0.2559288515746989\n",
      "Epoch: 887, Iteration: 1/2, Loss: 0.25640993042063304\n",
      "Epoch: 888, Iteration: 1/2, Loss: 0.3674422808885556\n",
      "Epoch: 889, Iteration: 1/2, Loss: 0.2397683023713714\n",
      "Epoch: 890, Iteration: 1/2, Loss: 0.12097732328559424\n",
      "Epoch: 891, Iteration: 1/2, Loss: 0.25772513721992296\n",
      "Epoch: 892, Iteration: 1/2, Loss: 0.25557814647081006\n",
      "Epoch: 893, Iteration: 1/2, Loss: 0.3686693762028928\n",
      "Epoch: 894, Iteration: 1/2, Loss: 0.23961633058851223\n",
      "Epoch: 895, Iteration: 1/2, Loss: 0.12420463731367688\n",
      "Epoch: 896, Iteration: 1/2, Loss: 0.25470963605349994\n",
      "Epoch: 897, Iteration: 1/2, Loss: 0.1406148578625724\n",
      "Epoch: 898, Iteration: 1/2, Loss: 0.35606471151871866\n",
      "Epoch: 899, Iteration: 1/2, Loss: 0.254103055144868\n",
      "Epoch: 900, Iteration: 1/2, Loss: 0.13825053781687674\n",
      "Epoch: 901, Iteration: 1/2, Loss: 0.2406981672350961\n",
      "Epoch: 902, Iteration: 1/2, Loss: 0.35240367044530785\n",
      "Epoch: 903, Iteration: 1/2, Loss: 0.3712902435532503\n",
      "Epoch: 904, Iteration: 1/2, Loss: 0.009890264826585601\n",
      "Epoch: 905, Iteration: 1/2, Loss: 0.3539728055046627\n",
      "Epoch: 906, Iteration: 1/2, Loss: 0.1394329937492666\n",
      "Epoch: 907, Iteration: 1/2, Loss: 0.4689632540457983\n",
      "Epoch: 908, Iteration: 1/2, Loss: 0.23999442491720627\n",
      "Epoch: 909, Iteration: 1/2, Loss: 0.008657569853625188\n",
      "Epoch: 910, Iteration: 1/2, Loss: 0.353577981449377\n",
      "Epoch: 911, Iteration: 1/2, Loss: 0.3715408082773856\n",
      "Epoch: 912, Iteration: 1/2, Loss: 0.12413707496412323\n",
      "Epoch: 913, Iteration: 1/2, Loss: 0.13959609375652918\n",
      "Epoch: 914, Iteration: 1/2, Loss: 0.35557550383680103\n",
      "Epoch: 915, Iteration: 1/2, Loss: 0.2555288291130593\n",
      "Epoch: 916, Iteration: 1/2, Loss: 0.36626246926755524\n",
      "Epoch: 917, Iteration: 1/2, Loss: 0.24040167513686392\n",
      "Epoch: 918, Iteration: 1/2, Loss: 0.12567965410579585\n",
      "Epoch: 919, Iteration: 1/2, Loss: 0.13790333226454823\n",
      "Epoch: 920, Iteration: 1/2, Loss: 0.35615012468124735\n",
      "Epoch: 921, Iteration: 1/2, Loss: 0.24887591198722342\n",
      "Epoch: 922, Iteration: 1/2, Loss: 0.25572784473828014\n",
      "Epoch: 923, Iteration: 1/2, Loss: 0.37074273587270845\n",
      "Epoch: 924, Iteration: 1/2, Loss: 0.23913200938890813\n",
      "Epoch: 925, Iteration: 1/2, Loss: 0.1242936995812994\n",
      "Epoch: 926, Iteration: 1/2, Loss: 0.2542677439629951\n",
      "Epoch: 927, Iteration: 1/2, Loss: 0.3692012300570106\n",
      "Epoch: 928, Iteration: 1/2, Loss: 0.12451174931215275\n",
      "Epoch: 929, Iteration: 1/2, Loss: 0.2526414409924305\n",
      "Epoch: 930, Iteration: 1/2, Loss: 0.2557700161927593\n",
      "Epoch: 931, Iteration: 1/2, Loss: 0.2539564522586244\n",
      "Epoch: 932, Iteration: 1/2, Loss: 0.2551304317792229\n",
      "Epoch: 933, Iteration: 1/2, Loss: 0.13868467147147798\n",
      "Epoch: 934, Iteration: 1/2, Loss: 0.3521275480114061\n",
      "Epoch: 935, Iteration: 1/2, Loss: 0.255055112477121\n",
      "Epoch: 936, Iteration: 1/2, Loss: 0.2565814068546644\n",
      "Epoch: 937, Iteration: 1/2, Loss: 0.25105043374764524\n",
      "Epoch: 938, Iteration: 1/2, Loss: 0.37055643392429244\n",
      "Epoch: 939, Iteration: 1/2, Loss: 0.009246584890348755\n",
      "Epoch: 940, Iteration: 1/2, Loss: 0.3541310695313129\n",
      "Epoch: 941, Iteration: 1/2, Loss: 0.2558168159339172\n",
      "Epoch: 942, Iteration: 1/2, Loss: 0.13776441671336934\n",
      "Epoch: 943, Iteration: 1/2, Loss: 0.23890338020313154\n",
      "Epoch: 944, Iteration: 1/2, Loss: 0.355429472370819\n",
      "Epoch: 945, Iteration: 1/2, Loss: 0.366691921832194\n",
      "Epoch: 946, Iteration: 1/2, Loss: 0.12120888911908695\n",
      "Epoch: 947, Iteration: 1/2, Loss: 0.3716711734560124\n",
      "Epoch: 948, Iteration: 1/2, Loss: 0.008614058978064512\n",
      "Epoch: 949, Iteration: 1/2, Loss: 0.3542279887267036\n",
      "Epoch: 950, Iteration: 1/2, Loss: 0.1396065516974587\n",
      "Epoch: 951, Iteration: 1/2, Loss: 0.353373996454018\n",
      "Epoch: 952, Iteration: 1/2, Loss: 0.25375207638135355\n",
      "Epoch: 953, Iteration: 1/2, Loss: 0.3697877154806606\n",
      "Epoch: 954, Iteration: 1/2, Loss: 0.12482625590836977\n",
      "Epoch: 955, Iteration: 1/2, Loss: 0.2549941996463697\n",
      "Epoch: 956, Iteration: 1/2, Loss: 0.25541167854108693\n",
      "Epoch: 957, Iteration: 1/2, Loss: 0.2536823721384579\n",
      "Epoch: 958, Iteration: 1/2, Loss: 0.3658747574137853\n",
      "Epoch: 959, Iteration: 1/2, Loss: 0.1271614543777892\n",
      "Epoch: 960, Iteration: 1/2, Loss: 0.2546197612494714\n",
      "Epoch: 961, Iteration: 1/2, Loss: 0.3645283017141145\n",
      "Epoch: 962, Iteration: 1/2, Loss: 0.11856806160524636\n",
      "Epoch: 963, Iteration: 1/2, Loss: 0.14418246241333174\n",
      "Epoch: 964, Iteration: 1/2, Loss: 0.351650703400362\n",
      "Epoch: 965, Iteration: 1/2, Loss: 0.3709873920360056\n",
      "Epoch: 966, Iteration: 1/2, Loss: 0.2381038502739492\n",
      "Epoch: 967, Iteration: 1/2, Loss: 0.12345121531108223\n",
      "Epoch: 968, Iteration: 1/2, Loss: 0.2556341840274212\n",
      "Epoch: 969, Iteration: 1/2, Loss: 0.24940368644060334\n",
      "Epoch: 970, Iteration: 1/2, Loss: 0.25756011600832684\n",
      "Epoch: 971, Iteration: 1/2, Loss: 0.2514073012440342\n",
      "Epoch: 972, Iteration: 1/2, Loss: 0.13950731216084042\n",
      "Epoch: 973, Iteration: 1/2, Loss: 0.3526976238771704\n",
      "Epoch: 974, Iteration: 1/2, Loss: 0.36926193797206364\n",
      "Epoch: 975, Iteration: 1/2, Loss: 0.12211338165498142\n",
      "Epoch: 976, Iteration: 1/2, Loss: 0.14021169893965\n",
      "Epoch: 977, Iteration: 1/2, Loss: 0.3542245945536173\n",
      "Epoch: 978, Iteration: 1/2, Loss: 0.25541298705889615\n",
      "Epoch: 979, Iteration: 1/2, Loss: 0.2536741654489041\n",
      "Epoch: 980, Iteration: 1/2, Loss: 0.365963283276298\n",
      "Epoch: 981, Iteration: 1/2, Loss: 0.23942120162032005\n",
      "Epoch: 982, Iteration: 1/2, Loss: 0.23740312695205526\n",
      "Epoch: 983, Iteration: 1/2, Loss: 0.12644101539027217\n",
      "Epoch: 984, Iteration: 1/2, Loss: 0.24880111849753545\n",
      "Epoch: 985, Iteration: 1/2, Loss: 0.2553209840292825\n",
      "Epoch: 986, Iteration: 1/2, Loss: 0.2544565292101654\n",
      "Epoch: 987, Iteration: 1/2, Loss: 0.25432152693153115\n",
      "Epoch: 988, Iteration: 1/2, Loss: 0.2489148406301474\n",
      "Epoch: 989, Iteration: 1/2, Loss: 0.14218233018840734\n",
      "Epoch: 990, Iteration: 1/2, Loss: 0.35341312100963335\n",
      "Epoch: 991, Iteration: 1/2, Loss: 0.1378022377585257\n",
      "Epoch: 992, Iteration: 1/2, Loss: 0.4690995704450375\n",
      "Epoch: 993, Iteration: 1/2, Loss: 0.12464295062030374\n",
      "Epoch: 994, Iteration: 1/2, Loss: 0.3671026533338677\n",
      "Epoch: 995, Iteration: 1/2, Loss: 0.007387265561597285\n",
      "Epoch: 996, Iteration: 1/2, Loss: 0.35522122299385467\n",
      "Epoch: 997, Iteration: 1/2, Loss: 0.24830823137314356\n",
      "Epoch: 998, Iteration: 1/2, Loss: 0.2569051316656883\n",
      "Epoch: 999, Iteration: 1/2, Loss: 0.25454702698395415\n",
      "Epoch: 1000, Iteration: 1/2, Loss: 0.13652348526476768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# configurations\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "# preprocess\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word2idx, idx2word = preprocess([text])\n",
    "contexts, target = create_contexts_target(corpus[0], window_size)\n",
    "target = convert_one_hot(target, len(word2idx))\n",
    "contexts = convert_one_hot(contexts, len(word2idx))\n",
    "\n",
    "# define model\n",
    "cbow_model = CBOW(vocab_size=len(word2idx), hidden_size=hidden_size, window_size=window_size)\n",
    "sgd_optimizer = SGD()\n",
    "trainer = Trainer(cbow_model, sgd_optimizer)\n",
    "\n",
    "# start training\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i [ 1.5014184   0.55395555  1.1085889  -1.1253431  -0.6633211 ]\n",
      "say [-0.290205   -0.8290099  -2.147023    0.49414074 -0.77702194]\n",
      "you [-0.42495802  1.3441554   2.0160549   0.17584291  1.9980527 ]\n",
      ". [ 1.9007728   0.09555145 -1.1272005  -0.3289489  -1.6700785 ]\n",
      "goodbye [ 1.5192126   0.5612364   1.084368   -1.1263704  -0.66816455]\n",
      "and [-2.1903782  -0.93125004 -1.015991    0.8274492   0.9169905 ]\n",
      "hello [-0.42036694  1.3293594   2.0003753   0.16550265  1.9935621 ]\n"
     ]
    }
   ],
   "source": [
    "# check word2vec results\n",
    "word_vecs = cbow_model.word_vecs\n",
    "for word_id, word in idx2word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Derivative Equation\n",
    "\n",
    "1. **Softmax with Cross Entropy:**\n",
    "$$\n",
    "\\begin{align}\n",
    "Softmax &= \\frac{e^{z_i}}{\\sum_j^ke^{z_j}} \\\\\n",
    "Cross\\ Entropy &= -\\frac{1}{N}\\sum_j^N\\sum_i^c(\\hat{y_i}logy_i)\n",
    "\\end{align}\n",
    "$$\n",
    "在進行分類問題時，很常使用的output層函數是softmax，而此函數也經常與cross entropy的損失函數一起使用(學員可以參考此[文章](https://medium.com/jarvis-toward-intelligence/%E6%AF%94%E8%BC%83-cross-entropy-%E8%88%87-mean-squared-error-8bebc0255f5)了解為何分類為題常使用cross entropy當損失函數)。 在word2vec模型的訓練，也可以看成是一種分類問題(分類字詞)，因此訓練時的輸出層也是使用softmax與cross entropy。 相關的導函數推導可以參考此兩篇文章 [reference-1](https://medium.com/hoskiss-stand/backpropagation-with-softmax-cross-entropy-d60983b7b245), [reference-2](https://zhuanlan.zhihu.com/p/25723112)\n",
    "\n",
    "\n",
    "2. **Dense layer:**\n",
    "\n",
    "$$\n",
    "y = xW\n",
    "$$\n",
    "\n",
    "在導函數的推導上，以舉例進行解說。\n",
    "\n",
    "Ex:\n",
    "$$\n",
    "x= \\left[\n",
    "\\begin{matrix}\n",
    "    x_1&x_2&x_3\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "$$\n",
    "W= \\left[\n",
    "\\begin{matrix}\n",
    "    W_{11}&W_{12}&W_{13} \\\\\n",
    "    W_{21}&W_{22}&W_{23} \\\\\n",
    "    W_{31}&W_{32}&W_{331}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = xW = \\left[\n",
    "\\begin{matrix}\n",
    "    W_{11}x_1+W_{21}x_2+W_{31}x_3&W_{12}x_1+W_{22}x_2+W_{32}x_3\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{y}}{\\partial{x}} = \\left[\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial{y_1}}{\\partial{x_1}}&\\frac{\\partial{y_1}}{\\partial{x_2}}&\\frac{\\partial{y_1}}{\\partial{x_3}} \\\\\n",
    "\\frac{\\partial{y_2}}{\\partial{x_1}}&\\frac{\\partial{y_2}}{\\partial{x_2}}&\\frac{\\partial{y_2}}{\\partial{x_3}}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{y}}{\\partial{x}} = \\left[\n",
    "\\begin{matrix}\n",
    "    W_{11}&W_{21}&W_{31} \\\\\n",
    "    W_{12}&W_{22}&W_{32}\n",
    "\\end{matrix}\n",
    "\\right] = W^T\n",
    "$$\n",
    "\n",
    "因此損失函數L對x的微分為\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{y}}W^T\n",
    "$$\n",
    "\n",
    "同理損失函數L對y的微分為\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{W}} = x^T\\frac{\\partial{L}}{\\partial{y}}\n",
    "$$\n",
    "\n",
    "對於矩陣微分想要更瞭解的學員可以參考此[文章](https://zhuanlan.zhihu.com/p/34826167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupoy_env",
   "language": "python",
   "name": "cupoy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
