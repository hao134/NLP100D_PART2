{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "cupoy_env",
      "language": "python",
      "name": "cupoy_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "實作簡易word2vec模型_作業.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH6QymbL73N4"
      },
      "source": [
        "### 作業目的: 實作word2vec Skip-gram模型\n",
        "在課程中了解如何搭建CBOW模型，這次的作業目的在於透過搭建Skip-gram模型來了解另外一種word2vec的架構。\n",
        "\n",
        "Hint_1: 學員可以善用課程中以搭建好的function模組\n",
        "Hint_2: Skip_gram所需的輸入資料與目標跟CBOW有些許不同，Skip_gram是由中間字詞預測上下文"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMKnFUws-xIj",
        "outputId": "347c00d2-f56f-4cc9-f372-ee391fdaad61"
      },
      "source": [
        "!unzip utils.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  utils.zip\n",
            "   creating: utils/\n",
            "  inflating: utils/utility.py        \n",
            "replace __MACOSX/utils/._utility.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/utils/._utility.py  \n",
            "  inflating: utils/__init__.py       \n",
            "replace __MACOSX/utils/.___init__.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/utils/.___init__.py  \n",
            "   creating: utils/__pycache__/\n",
            "  inflating: utils/optimizer.py      \n",
            "  inflating: __MACOSX/utils/._optimizer.py  \n",
            "  inflating: utils/layers.py         \n",
            "  inflating: __MACOSX/utils/._layers.py  \n",
            "  inflating: utils/__pycache__/optimizer.cpython-37.pyc  \n",
            "  inflating: __MACOSX/utils/__pycache__/._optimizer.cpython-37.pyc  \n",
            "  inflating: utils/__pycache__/layers.cpython-37.pyc  \n",
            "  inflating: __MACOSX/utils/__pycache__/._layers.cpython-37.pyc  \n",
            "  inflating: utils/__pycache__/utility.cpython-37.pyc  \n",
            "replace __MACOSX/utils/__pycache__/._utility.cpython-37.pyc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/utils/__pycache__/._utility.cpython-37.pyc  \n",
            "  inflating: utils/__pycache__/__init__.cpython-37.pyc  \n",
            "replace __MACOSX/utils/__pycache__/.___init__.cpython-37.pyc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/utils/__pycache__/.___init__.cpython-37.pyc  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7Ms7N2w73N7"
      },
      "source": [
        "# import modules\n",
        "import numpy as np\n",
        "from typing import List\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from utils.utility import clip_grads, remove_duplicate #Trainer#, convert_one_hot, preprocess,\n",
        "from utils.layers import Dense, SoftmaxWithCrossEntropy\n",
        "from utils.optimizer import SGD"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTA8L-XI_vGt"
      },
      "source": [
        "def convert_one_hot(corpus, vocab_size):\n",
        "    N = corpus.shape[0]\n",
        "\n",
        "    if corpus.ndim == 1:\n",
        "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
        "        for idx, word_id in enumerate(corpus):\n",
        "            one_hot[idx, word_id] = 1\n",
        "    elif corpus.ndim == 2:\n",
        "        C = corpus.shape[1]\n",
        "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
        "        for idx_0, word_ids in enumerate(corpus):\n",
        "            for idx_1, word_id in enumerate(word_ids):\n",
        "                one_hot[idx_0, idx_1, word_id] = 1\n",
        "\n",
        "    return one_hot"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_wpoH8ZAGXm"
      },
      "source": [
        "#define preprocess function\n",
        "def preprocess(corpus: List[str], only_word: bool = False):\n",
        "    '''Function to do preprocess of input corpus\n",
        "    Parameters\n",
        "    -----------\n",
        "    corpus: str\n",
        "        input corpus to be processed\n",
        "    only_word: bool\n",
        "        whether to filter out non-word\n",
        "    '''\n",
        "    word_dic = set()\n",
        "    processed_sentence = []\n",
        "    \n",
        "    for sentence in corpus:\n",
        "        #將所有字詞轉為小寫\n",
        "        sentence = sentence.lower()\n",
        "\n",
        "        #移除標點符號(可以依據使用狀況決定是否要移除標點符號)\n",
        "        if only_word:\n",
        "            pattern = r'[^\\W_]+'\n",
        "            sentence = re.findall(pattern, sentence)\n",
        "        else:\n",
        "            punctuation_list = ['.', ',', '!', '?']\n",
        "            for pun in punctuation_list:\n",
        "                sentence = sentence.replace(pun, ' '+pun)\n",
        "            sentence = sentence.split(' ')\n",
        "        \n",
        "        #添加字詞到字典中\n",
        "        word_dic |= set(sentence)\n",
        "        processed_sentence.append(sentence)\n",
        "    \n",
        "    \n",
        "    #建立字詞ID清單\n",
        "    word2idx = dict()\n",
        "    idx2word = dict()\n",
        "    for word in word_dic:\n",
        "        if word not in word2idx:\n",
        "            idx = len(word2idx)\n",
        "            word2idx[word] = idx\n",
        "            idx2word[idx] = word\n",
        "\n",
        "    #將文本轉為ID型式\n",
        "    id_mapping = lambda x: word2idx[x]\n",
        "    \n",
        "    corpus = np.array([list(map(id_mapping, sentence)) for sentence in processed_sentence])\n",
        "\n",
        "    return corpus, word2idx, idx2word"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su__pfogAGk3"
      },
      "source": [
        "# define trainer for training purpose\n",
        "class Trainer:\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_list = []\n",
        "        self.eval_interval = None\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
        "        data_size = len(x)\n",
        "        max_iters = data_size // batch_size\n",
        "        self.eval_interval = eval_interval\n",
        "        model, optimizer = self.model, self.optimizer\n",
        "        total_loss = 0\n",
        "        loss_count = 0\n",
        "\n",
        "        for epoch in tqdm.tqdm(range(max_epoch)):\n",
        "            # shuffling\n",
        "            idx = np.random.permutation(np.arange(data_size))\n",
        "            x = x[idx]\n",
        "            t = t[idx]\n",
        "\n",
        "            for iters in range(max_iters):\n",
        "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "                # calculate loss and update weights\n",
        "                loss = model.forward(batch_x, batch_t)\n",
        "                model.backward()\n",
        "                # remove duplicate weights (for weights sharing purpose)\n",
        "                params, grads = remove_duplicate(model.params, model.grads) \n",
        "                # for gradient clipping purpose\n",
        "                if max_grad is not None:\n",
        "                    clip_grads(grads, max_grad)\n",
        "                    \n",
        "                optimizer.update(params, grads)\n",
        "                total_loss += loss\n",
        "                loss_count += 1\n",
        "\n",
        "                # evaluation\n",
        "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "                    avg_loss = total_loss / loss_count\n",
        "                    print(f\"Epoch: {self.current_epoch+1}, Iteration: {iters+1}/{max_iters}, Loss: {avg_loss}\")\n",
        "                    self.loss_list.append(float(avg_loss))\n",
        "                    total_loss, loss_count = 0, 0\n",
        "\n",
        "            self.current_epoch += 1\n",
        "\n",
        "    def plot(self, ylim=None):\n",
        "        x = list(range(len(self.loss_list)))\n",
        "        \n",
        "        if ylim is not None:\n",
        "            plt.ylim(*ylim)\n",
        "        plt.plot(x, self.loss_list, label='train')\n",
        "        plt.xlabel(f\"iterations (x{self.eval_interval})\")\n",
        "        plt.ylabel(f\"loss\")\n",
        "        plt.show()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE5o5JXi73N8",
        "outputId": "3581b43b-9f75-400d-96f1-70ab0849daf2"
      },
      "source": [
        "# use the same corpus as in the lecture\n",
        "text = \"I am studying Natural Language Processing now.\"\n",
        "\n",
        "# define create_contexts_target function\n",
        "def create_contexts_target(corpus: List, window_size: int=1):\n",
        "\n",
        "    contexts = corpus[window_size:-window_size]\n",
        "    targets = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                # skip target word itself\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        targets.append(cs)\n",
        "\n",
        "    return np.array(contexts), np.array(targets)\n",
        "\n",
        "# transform corpus to contexts and targets pair\n",
        "corpus, word2idx, idx2word = preprocess([text])\n",
        "contexts, targets= create_contexts_target(corpus[0], window_size=1)\n",
        "contexts, targets"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([6, 4, 1, 7, 0, 2]), array([[5, 4],\n",
              "        [6, 1],\n",
              "        [4, 7],\n",
              "        [1, 0],\n",
              "        [7, 2],\n",
              "        [0, 3]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjf61d-h73N9",
        "outputId": "d55e08df-ea76-44fd-dc1d-0cdf50035c8f"
      },
      "source": [
        "# transform contexts and targets to one-hot encoding\n",
        "### <your code> ###\n",
        "contexts = convert_one_hot(contexts, len(word2idx))\n",
        "targets = convert_one_hot(targets, len(word2idx))\n",
        "contexts, targets"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0, 0, 0, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 1],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0, 0, 0, 0]], dtype=int32),\n",
              " array([[[0, 0, 0, 0, 0, 1, 0, 0],\n",
              "         [0, 0, 0, 0, 1, 0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0, 0, 0, 0, 1, 0],\n",
              "         [0, 1, 0, 0, 0, 0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0, 0, 1, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 1]],\n",
              " \n",
              "        [[0, 1, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 0, 0, 0, 0, 0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0, 0, 0, 0, 0, 1],\n",
              "         [0, 0, 1, 0, 0, 0, 0, 0]],\n",
              " \n",
              "        [[1, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 1, 0, 0, 0, 0]]], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3sfGXc73N-"
      },
      "source": [
        "# define Skip-gram model\n",
        "class SkipGram:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "        self.window_size = window_size\n",
        "\n",
        "        # initialize weights\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "\n",
        "        # create layers\n",
        "        self.in_layer = Dense(W_in)\n",
        "        self.out_layer = Dense(W_out)\n",
        "        self.loss_layers = [SoftmaxWithCrossEntropy() for i in range(window_size * 2)]\n",
        "        \n",
        "\n",
        "        layers = [self.in_layer, self.out_layer]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        \n",
        "        # word vector matrix\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, targets):\n",
        "        h = self.in_layer.forward(contexts)\n",
        "        s = self.out_layer.forward(h)\n",
        "        \n",
        "        loss = sum([self.loss_layers[i].forward(s, targets[:, i]) for i in range(self.window_size*2)])\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        \n",
        "        ds = sum([self.loss_layers[i].backward(dout) for i in range(self.window_size*2)])\n",
        "        dh = self.out_layer.backward(ds)\n",
        "        self.in_layer.backward(dh)\n",
        "        \n",
        "        return None"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlhCRHZz73N_",
        "outputId": "4ecf7ac0-36ad-4ae2-c42d-3e7cfbb441d7"
      },
      "source": [
        "# start training\n",
        "\n",
        "# configurations\n",
        "window_size = 1\n",
        "hidden_size = 5\n",
        "batch_size = 3\n",
        "max_epoch = 1000\n",
        "\n",
        "# define model\n",
        "skip_gram = SkipGram(vocab_size=len(word2idx), hidden_size=hidden_size, window_size=window_size)\n",
        "sgd_optimizer = SGD()\n",
        "trainer = Trainer(skip_gram, sgd_optimizer)\n",
        "\n",
        "# start training\n",
        "trainer.fit(contexts, targets, max_epoch, batch_size)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 104/1000 [00:00<00:00, 1034.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Iteration: 1/2, Loss: 4.158945601023943\n",
            "Epoch: 2, Iteration: 1/2, Loss: 4.158737681064907\n",
            "Epoch: 3, Iteration: 1/2, Loss: 4.158895468833011\n",
            "Epoch: 4, Iteration: 1/2, Loss: 4.158772875863358\n",
            "Epoch: 5, Iteration: 1/2, Loss: 4.158886831715259\n",
            "Epoch: 6, Iteration: 1/2, Loss: 4.158715594362699\n",
            "Epoch: 7, Iteration: 1/2, Loss: 4.158640247030278\n",
            "Epoch: 8, Iteration: 1/2, Loss: 4.158488632237213\n",
            "Epoch: 9, Iteration: 1/2, Loss: 4.15874086879626\n",
            "Epoch: 10, Iteration: 1/2, Loss: 4.158536237491145\n",
            "Epoch: 11, Iteration: 1/2, Loss: 4.158414917627194\n",
            "Epoch: 12, Iteration: 1/2, Loss: 4.1585784663871275\n",
            "Epoch: 13, Iteration: 1/2, Loss: 4.158313006996372\n",
            "Epoch: 14, Iteration: 1/2, Loss: 4.158584551427376\n",
            "Epoch: 15, Iteration: 1/2, Loss: 4.158029207316288\n",
            "Epoch: 16, Iteration: 1/2, Loss: 4.158164109661529\n",
            "Epoch: 17, Iteration: 1/2, Loss: 4.158381135425107\n",
            "Epoch: 18, Iteration: 1/2, Loss: 4.158213125425037\n",
            "Epoch: 19, Iteration: 1/2, Loss: 4.157798495700236\n",
            "Epoch: 20, Iteration: 1/2, Loss: 4.1576365477437545\n",
            "Epoch: 21, Iteration: 1/2, Loss: 4.158074089976386\n",
            "Epoch: 22, Iteration: 1/2, Loss: 4.157754309992125\n",
            "Epoch: 23, Iteration: 1/2, Loss: 4.157565963456166\n",
            "Epoch: 24, Iteration: 1/2, Loss: 4.156892070814961\n",
            "Epoch: 25, Iteration: 1/2, Loss: 4.157521612647499\n",
            "Epoch: 26, Iteration: 1/2, Loss: 4.157164854157543\n",
            "Epoch: 27, Iteration: 1/2, Loss: 4.15673834722257\n",
            "Epoch: 28, Iteration: 1/2, Loss: 4.15702257329292\n",
            "Epoch: 29, Iteration: 1/2, Loss: 4.156336270668238\n",
            "Epoch: 30, Iteration: 1/2, Loss: 4.156305741900599\n",
            "Epoch: 31, Iteration: 1/2, Loss: 4.1555651731945\n",
            "Epoch: 32, Iteration: 1/2, Loss: 4.155763919427777\n",
            "Epoch: 33, Iteration: 1/2, Loss: 4.154641193488919\n",
            "Epoch: 34, Iteration: 1/2, Loss: 4.154422503197894\n",
            "Epoch: 35, Iteration: 1/2, Loss: 4.153718089976749\n",
            "Epoch: 36, Iteration: 1/2, Loss: 4.154843707047571\n",
            "Epoch: 37, Iteration: 1/2, Loss: 4.153416259700153\n",
            "Epoch: 38, Iteration: 1/2, Loss: 4.153045121011281\n",
            "Epoch: 39, Iteration: 1/2, Loss: 4.1507021734779315\n",
            "Epoch: 40, Iteration: 1/2, Loss: 4.149998968021183\n",
            "Epoch: 41, Iteration: 1/2, Loss: 4.152521680646403\n",
            "Epoch: 42, Iteration: 1/2, Loss: 4.147515650181209\n",
            "Epoch: 43, Iteration: 1/2, Loss: 4.149587587267553\n",
            "Epoch: 44, Iteration: 1/2, Loss: 4.145094610772573\n",
            "Epoch: 45, Iteration: 1/2, Loss: 4.144930939833898\n",
            "Epoch: 46, Iteration: 1/2, Loss: 4.147639001340783\n",
            "Epoch: 47, Iteration: 1/2, Loss: 4.142385419335269\n",
            "Epoch: 48, Iteration: 1/2, Loss: 4.139904989140945\n",
            "Epoch: 49, Iteration: 1/2, Loss: 4.135002270144012\n",
            "Epoch: 50, Iteration: 1/2, Loss: 4.139827324075014\n",
            "Epoch: 51, Iteration: 1/2, Loss: 4.134621443190568\n",
            "Epoch: 52, Iteration: 1/2, Loss: 4.129732297824914\n",
            "Epoch: 53, Iteration: 1/2, Loss: 4.128277518467431\n",
            "Epoch: 54, Iteration: 1/2, Loss: 4.119076269815549\n",
            "Epoch: 55, Iteration: 1/2, Loss: 4.119030054895232\n",
            "Epoch: 56, Iteration: 1/2, Loss: 4.115474573474581\n",
            "Epoch: 57, Iteration: 1/2, Loss: 4.114607684038158\n",
            "Epoch: 58, Iteration: 1/2, Loss: 4.096445509294739\n",
            "Epoch: 59, Iteration: 1/2, Loss: 4.099889140919567\n",
            "Epoch: 60, Iteration: 1/2, Loss: 4.101585975441893\n",
            "Epoch: 61, Iteration: 1/2, Loss: 4.080304168505998\n",
            "Epoch: 62, Iteration: 1/2, Loss: 4.0784121478326245\n",
            "Epoch: 63, Iteration: 1/2, Loss: 4.047193962759909\n",
            "Epoch: 64, Iteration: 1/2, Loss: 4.0625091172328744\n",
            "Epoch: 65, Iteration: 1/2, Loss: 4.054842640261814\n",
            "Epoch: 66, Iteration: 1/2, Loss: 4.0305925390795\n",
            "Epoch: 67, Iteration: 1/2, Loss: 3.9948414298101573\n",
            "Epoch: 68, Iteration: 1/2, Loss: 4.008260612058792\n",
            "Epoch: 69, Iteration: 1/2, Loss: 3.9798416483210253\n",
            "Epoch: 70, Iteration: 1/2, Loss: 3.9624922784675114\n",
            "Epoch: 71, Iteration: 1/2, Loss: 3.965887705530604\n",
            "Epoch: 72, Iteration: 1/2, Loss: 3.8918475236335324\n",
            "Epoch: 73, Iteration: 1/2, Loss: 3.880315768629135\n",
            "Epoch: 74, Iteration: 1/2, Loss: 3.8796273914117685\n",
            "Epoch: 75, Iteration: 1/2, Loss: 3.8517474057267953\n",
            "Epoch: 76, Iteration: 1/2, Loss: 3.785495657689679\n",
            "Epoch: 77, Iteration: 1/2, Loss: 3.8070421219946207\n",
            "Epoch: 78, Iteration: 1/2, Loss: 3.731164228755481\n",
            "Epoch: 79, Iteration: 1/2, Loss: 3.635643347621029\n",
            "Epoch: 80, Iteration: 1/2, Loss: 3.7366512707255737\n",
            "Epoch: 81, Iteration: 1/2, Loss: 3.5865198135580494\n",
            "Epoch: 82, Iteration: 1/2, Loss: 3.548610477664262\n",
            "Epoch: 83, Iteration: 1/2, Loss: 3.4533253262544745\n",
            "Epoch: 84, Iteration: 1/2, Loss: 3.5405596220088666\n",
            "Epoch: 85, Iteration: 1/2, Loss: 3.4431695622404015\n",
            "Epoch: 86, Iteration: 1/2, Loss: 3.308140221623291\n",
            "Epoch: 87, Iteration: 1/2, Loss: 3.361629875079228\n",
            "Epoch: 88, Iteration: 1/2, Loss: 3.1254667481094103\n",
            "Epoch: 89, Iteration: 1/2, Loss: 3.1761723468444405\n",
            "Epoch: 90, Iteration: 1/2, Loss: 3.133744547442701\n",
            "Epoch: 91, Iteration: 1/2, Loss: 3.11385336708062\n",
            "Epoch: 92, Iteration: 1/2, Loss: 3.0054631596758528\n",
            "Epoch: 93, Iteration: 1/2, Loss: 3.0107219719419085\n",
            "Epoch: 94, Iteration: 1/2, Loss: 3.0322177170270566\n",
            "Epoch: 95, Iteration: 1/2, Loss: 2.8457509969741697\n",
            "Epoch: 96, Iteration: 1/2, Loss: 2.7381074262989546\n",
            "Epoch: 97, Iteration: 1/2, Loss: 2.853331738644753\n",
            "Epoch: 98, Iteration: 1/2, Loss: 2.6264432185918993\n",
            "Epoch: 99, Iteration: 1/2, Loss: 2.795116439344377\n",
            "Epoch: 100, Iteration: 1/2, Loss: 2.6067854036988485\n",
            "Epoch: 101, Iteration: 1/2, Loss: 2.5300866275636706\n",
            "Epoch: 102, Iteration: 1/2, Loss: 2.5914918776659706\n",
            "Epoch: 103, Iteration: 1/2, Loss: 2.5649114527730235\n",
            "Epoch: 104, Iteration: 1/2, Loss: 2.401961361080811\n",
            "Epoch: 105, Iteration: 1/2, Loss: 2.547840167918086\n",
            "Epoch: 106, Iteration: 1/2, Loss: 2.3393901765093887\n",
            "Epoch: 107, Iteration: 1/2, Loss: 2.4023944255799465\n",
            "Epoch: 108, Iteration: 1/2, Loss: 2.292045104563401\n",
            "Epoch: 109, Iteration: 1/2, Loss: 2.352749904333077\n",
            "Epoch: 110, Iteration: 1/2, Loss: 2.2713362964126373\n",
            "Epoch: 111, Iteration: 1/2, Loss: 2.313956127085966\n",
            "Epoch: 112, Iteration: 1/2, Loss: 2.118129701037785\n",
            "Epoch: 113, Iteration: 1/2, Loss: 2.192300848409931\n",
            "Epoch: 114, Iteration: 1/2, Loss: 2.1480720451432838\n",
            "Epoch: 115, Iteration: 1/2, Loss: 2.1654657148712366\n",
            "Epoch: 116, Iteration: 1/2, Loss: 2.0730464433631712\n",
            "Epoch: 117, Iteration: 1/2, Loss: 2.03589117646641\n",
            "Epoch: 118, Iteration: 1/2, Loss: 2.0558617394260317\n",
            "Epoch: 119, Iteration: 1/2, Loss: 2.0536624969995785\n",
            "Epoch: 120, Iteration: 1/2, Loss: 2.0614018794648503\n",
            "Epoch: 121, Iteration: 1/2, Loss: 1.9434213618107061\n",
            "Epoch: 122, Iteration: 1/2, Loss: 1.9845574013272875\n",
            "Epoch: 123, Iteration: 1/2, Loss: 1.9618159256341687\n",
            "Epoch: 124, Iteration: 1/2, Loss: 1.897546447173871\n",
            "Epoch: 125, Iteration: 1/2, Loss: 1.891076659411734\n",
            "Epoch: 126, Iteration: 1/2, Loss: 1.8957919091536448\n",
            "Epoch: 127, Iteration: 1/2, Loss: 1.9170475428127642\n",
            "Epoch: 128, Iteration: 1/2, Loss: 1.8145595581363112\n",
            "Epoch: 129, Iteration: 1/2, Loss: 1.8862450836311515\n",
            "Epoch: 130, Iteration: 1/2, Loss: 1.8256254930478724\n",
            "Epoch: 131, Iteration: 1/2, Loss: 1.805716317888082\n",
            "Epoch: 132, Iteration: 1/2, Loss: 1.8102656978975502\n",
            "Epoch: 133, Iteration: 1/2, Loss: 1.789567289941557\n",
            "Epoch: 134, Iteration: 1/2, Loss: 1.7501628568568317\n",
            "Epoch: 135, Iteration: 1/2, Loss: 1.7776082328881389\n",
            "Epoch: 136, Iteration: 1/2, Loss: 1.7543056143323459\n",
            "Epoch: 137, Iteration: 1/2, Loss: 1.7433269637974445\n",
            "Epoch: 138, Iteration: 1/2, Loss: 1.736589706258448\n",
            "Epoch: 139, Iteration: 1/2, Loss: 1.7117852742652941\n",
            "Epoch: 140, Iteration: 1/2, Loss: 1.7199357151503767\n",
            "Epoch: 141, Iteration: 1/2, Loss: 1.7193011209072337\n",
            "Epoch: 142, Iteration: 1/2, Loss: 1.6714644283798794\n",
            "Epoch: 143, Iteration: 1/2, Loss: 1.7181268220210422\n",
            "Epoch: 144, Iteration: 1/2, Loss: 1.6786028174262384\n",
            "Epoch: 145, Iteration: 1/2, Loss: 1.6513920687035981\n",
            "Epoch: 146, Iteration: 1/2, Loss: 1.6565106034331116\n",
            "Epoch: 147, Iteration: 1/2, Loss: 1.6712408800202412\n",
            "Epoch: 148, Iteration: 1/2, Loss: 1.6570598840306179\n",
            "Epoch: 149, Iteration: 1/2, Loss: 1.6130996039973078\n",
            "Epoch: 150, Iteration: 1/2, Loss: 1.6631124374096637\n",
            "Epoch: 151, Iteration: 1/2, Loss: 1.6188265958160677\n",
            "Epoch: 152, Iteration: 1/2, Loss: 1.6043351118876725\n",
            "Epoch: 153, Iteration: 1/2, Loss: 1.6167017940509743\n",
            "Epoch: 154, Iteration: 1/2, Loss: 1.6088700601362724\n",
            "Epoch: 155, Iteration: 1/2, Loss: 1.636441113451291\n",
            "Epoch: 156, Iteration: 1/2, Loss: 1.5872384034383005\n",
            "Epoch: 157, Iteration: 1/2, Loss: 1.5979085818864647\n",
            "Epoch: 158, Iteration: 1/2, Loss: 1.5788887394594173\n",
            "Epoch: 159, Iteration: 1/2, Loss: 1.588514497870671\n",
            "Epoch: 160, Iteration: 1/2, Loss: 1.5894081950872418\n",
            "Epoch: 161, Iteration: 1/2, Loss: 1.5699528006514165\n",
            "Epoch: 162, Iteration: 1/2, Loss: 1.5693749868605122\n",
            "Epoch: 163, Iteration: 1/2, Loss: 1.5693654305441613\n",
            "Epoch: 164, Iteration: 1/2, Loss: 1.5722751087031677\n",
            "Epoch: 165, Iteration: 1/2, Loss: 1.5672958641674506\n",
            "Epoch: 166, Iteration: 1/2, Loss: 1.546941965405581\n",
            "Epoch: 167, Iteration: 1/2, Loss: 1.5581284894672303\n",
            "Epoch: 168, Iteration: 1/2, Loss: 1.5555351184636106\n",
            "Epoch: 169, Iteration: 1/2, Loss: 1.5381092806473755\n",
            "Epoch: 170, Iteration: 1/2, Loss: 1.5367560005770144\n",
            "Epoch: 171, Iteration: 1/2, Loss: 1.5443008618501162\n",
            "Epoch: 172, Iteration: 1/2, Loss: 1.5387586901993315\n",
            "Epoch: 173, Iteration: 1/2, Loss: 1.5422935327761844\n",
            "Epoch: 174, Iteration: 1/2, Loss: 1.5145630680943887\n",
            "Epoch: 175, Iteration: 1/2, Loss: 1.5314458002933309\n",
            "Epoch: 176, Iteration: 1/2, Loss: 1.5175003896745753\n",
            "Epoch: 177, Iteration: 1/2, Loss: 1.5359865514814555\n",
            "Epoch: 178, Iteration: 1/2, Loss: 1.5171971164282194\n",
            "Epoch: 179, Iteration: 1/2, Loss: 1.52105848478941\n",
            "Epoch: 180, Iteration: 1/2, Loss: 1.5243964062177435\n",
            "Epoch: 181, Iteration: 1/2, Loss: 1.4949061440319256\n",
            "Epoch: 182, Iteration: 1/2, Loss: 1.5294352373260118\n",
            "Epoch: 183, Iteration: 1/2, Loss: 1.5016135304168545\n",
            "Epoch: 184, Iteration: 1/2, Loss: 1.5027106476016918\n",
            "Epoch: 185, Iteration: 1/2, Loss: 1.5085803287670696\n",
            "Epoch: 186, Iteration: 1/2, Loss: 1.4949873199821937\n",
            "Epoch: 187, Iteration: 1/2, Loss: 1.5039498314695896\n",
            "Epoch: 188, Iteration: 1/2, Loss: 1.50028677452018\n",
            "Epoch: 189, Iteration: 1/2, Loss: 1.499900489622872\n",
            "Epoch: 190, Iteration: 1/2, Loss: 1.4908357353954331\n",
            "Epoch: 191, Iteration: 1/2, Loss: 1.4941715701256295\n",
            "Epoch: 192, Iteration: 1/2, Loss: 1.4838417607884775\n",
            "Epoch: 193, Iteration: 1/2, Loss: 1.497010994791927\n",
            "Epoch: 194, Iteration: 1/2, Loss: 1.4889964309553028\n",
            "Epoch: 195, Iteration: 1/2, Loss: 1.4936412759294164\n",
            "Epoch: 196, Iteration: 1/2, Loss: 1.4851737919621568\n",
            "Epoch: 197, Iteration: 1/2, Loss: 1.485698993511162\n",
            "Epoch: 198, Iteration: 1/2, Loss: 1.4735393105299575\n",
            "Epoch: 199, Iteration: 1/2, Loss: 1.4885314180177163\n",
            "Epoch: 200, Iteration: 1/2, Loss: 1.4719470758615274\n",
            "Epoch: 201, Iteration: 1/2, Loss: 1.474162612190578\n",
            "Epoch: 202, Iteration: 1/2, Loss: 1.4879938003572626\n",
            "Epoch: 203, Iteration: 1/2, Loss: 1.4612251431280288\n",
            "Epoch: 204, Iteration: 1/2, Loss: 1.4736893594371887\n",
            "Epoch: 205, Iteration: 1/2, Loss: 1.4748792332666296\n",
            "Epoch: 206, Iteration: 1/2, Loss: 1.476097290582465\n",
            "Epoch: 207, Iteration: 1/2, Loss: 1.46446759405283\n",
            "Epoch: 208, Iteration: 1/2, Loss: 1.4748886979596203\n",
            "Epoch: 209, Iteration: 1/2, Loss: 1.4661231528687413\n",
            "Epoch: 210, Iteration: 1/2, Loss: 1.470134167052994\n",
            "Epoch: 211, Iteration: 1/2, Loss: 1.4606297476550005\n",
            "Epoch: 212, Iteration: 1/2, Loss: 1.4753194037650355\n",
            "Epoch: 213, Iteration: 1/2, Loss: 1.4553885558422142\n",
            "Epoch: 214, Iteration: 1/2, Loss: 1.4675977122017003\n",
            "Epoch: 215, Iteration: 1/2, Loss: 1.4568174998182357\n",
            "Epoch: 216, Iteration: 1/2, Loss: 1.4663168960952158\n",
            "Epoch: 217, Iteration: 1/2, Loss: 1.4535527968095674\n",
            "Epoch: 218, Iteration: 1/2, Loss: 1.4630108827400838\n",
            "Epoch: 219, Iteration: 1/2, Loss: 1.4513457081071524\n",
            "Epoch: 220, Iteration: 1/2, Loss: 1.4632077582400291\n",
            "Epoch: 221, Iteration: 1/2, Loss: 1.461356196421638\n",
            "Epoch: 222, Iteration: 1/2, Loss: 1.4549452608083238\n",
            "Epoch: 223, Iteration: 1/2, Loss: 1.4438664632778526\n",
            "Epoch: 224, Iteration: 1/2, Loss: 1.4591383275468441\n",
            "Epoch: 225, Iteration: 1/2, Loss: 1.4523596204524023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 473/1000 [00:00<00:00, 1154.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 226, Iteration: 1/2, Loss: 1.4528382342947166\n",
            "Epoch: 227, Iteration: 1/2, Loss: 1.4544431820899786\n",
            "Epoch: 228, Iteration: 1/2, Loss: 1.447092570543492\n",
            "Epoch: 229, Iteration: 1/2, Loss: 1.4476422125743373\n",
            "Epoch: 230, Iteration: 1/2, Loss: 1.4511405058674838\n",
            "Epoch: 231, Iteration: 1/2, Loss: 1.4516027454647231\n",
            "Epoch: 232, Iteration: 1/2, Loss: 1.4450853609690657\n",
            "Epoch: 233, Iteration: 1/2, Loss: 1.4466445241074348\n",
            "Epoch: 234, Iteration: 1/2, Loss: 1.446229851428281\n",
            "Epoch: 235, Iteration: 1/2, Loss: 1.4466337735986226\n",
            "Epoch: 236, Iteration: 1/2, Loss: 1.44096225760434\n",
            "Epoch: 237, Iteration: 1/2, Loss: 1.448742829876291\n",
            "Epoch: 238, Iteration: 1/2, Loss: 1.4412982540986774\n",
            "Epoch: 239, Iteration: 1/2, Loss: 1.443627744731091\n",
            "Epoch: 240, Iteration: 1/2, Loss: 1.4377350782391927\n",
            "Epoch: 241, Iteration: 1/2, Loss: 1.4481573052446466\n",
            "Epoch: 242, Iteration: 1/2, Loss: 1.4422106231688756\n",
            "Epoch: 243, Iteration: 1/2, Loss: 1.4372271354561126\n",
            "Epoch: 244, Iteration: 1/2, Loss: 1.4409671478986488\n",
            "Epoch: 245, Iteration: 1/2, Loss: 1.439943466529475\n",
            "Epoch: 246, Iteration: 1/2, Loss: 1.436217463865604\n",
            "Epoch: 247, Iteration: 1/2, Loss: 1.4433917721269434\n",
            "Epoch: 248, Iteration: 1/2, Loss: 1.4366085087650569\n",
            "Epoch: 249, Iteration: 1/2, Loss: 1.4343805285196378\n",
            "Epoch: 250, Iteration: 1/2, Loss: 1.441200231575471\n",
            "Epoch: 251, Iteration: 1/2, Loss: 1.4364234857298106\n",
            "Epoch: 252, Iteration: 1/2, Loss: 1.4330821793065724\n",
            "Epoch: 253, Iteration: 1/2, Loss: 1.4373825573814616\n",
            "Epoch: 254, Iteration: 1/2, Loss: 1.436836369724693\n",
            "Epoch: 255, Iteration: 1/2, Loss: 1.4312276326507878\n",
            "Epoch: 256, Iteration: 1/2, Loss: 1.4335848068839903\n",
            "Epoch: 257, Iteration: 1/2, Loss: 1.4317137017301098\n",
            "Epoch: 258, Iteration: 1/2, Loss: 1.4352466114933957\n",
            "Epoch: 259, Iteration: 1/2, Loss: 1.4326520636450906\n",
            "Epoch: 260, Iteration: 1/2, Loss: 1.4285083588305016\n",
            "Epoch: 261, Iteration: 1/2, Loss: 1.437301714442048\n",
            "Epoch: 262, Iteration: 1/2, Loss: 1.4294382204994205\n",
            "Epoch: 263, Iteration: 1/2, Loss: 1.4308301725260282\n",
            "Epoch: 264, Iteration: 1/2, Loss: 1.434039665511266\n",
            "Epoch: 265, Iteration: 1/2, Loss: 1.4272313401623238\n",
            "Epoch: 266, Iteration: 1/2, Loss: 1.429429400684871\n",
            "Epoch: 267, Iteration: 1/2, Loss: 1.4261306469785764\n",
            "Epoch: 268, Iteration: 1/2, Loss: 1.4321435504536348\n",
            "Epoch: 269, Iteration: 1/2, Loss: 1.4271225785253834\n",
            "Epoch: 270, Iteration: 1/2, Loss: 1.4270147627380112\n",
            "Epoch: 271, Iteration: 1/2, Loss: 1.4304924010624294\n",
            "Epoch: 272, Iteration: 1/2, Loss: 1.42823740980256\n",
            "Epoch: 273, Iteration: 1/2, Loss: 1.4274035558562692\n",
            "Epoch: 274, Iteration: 1/2, Loss: 1.424747585517201\n",
            "Epoch: 275, Iteration: 1/2, Loss: 1.43107866934161\n",
            "Epoch: 276, Iteration: 1/2, Loss: 1.4219139163456473\n",
            "Epoch: 277, Iteration: 1/2, Loss: 1.4275916055494005\n",
            "Epoch: 278, Iteration: 1/2, Loss: 1.4246593640293415\n",
            "Epoch: 279, Iteration: 1/2, Loss: 1.4283974094729148\n",
            "Epoch: 280, Iteration: 1/2, Loss: 1.4248337253283399\n",
            "Epoch: 281, Iteration: 1/2, Loss: 1.4230744140336131\n",
            "Epoch: 282, Iteration: 1/2, Loss: 1.4226971531263426\n",
            "Epoch: 283, Iteration: 1/2, Loss: 1.424495964642829\n",
            "Epoch: 284, Iteration: 1/2, Loss: 1.424501473157159\n",
            "Epoch: 285, Iteration: 1/2, Loss: 1.4202397477247373\n",
            "Epoch: 286, Iteration: 1/2, Loss: 1.424657826119996\n",
            "Epoch: 287, Iteration: 1/2, Loss: 1.422675736490309\n",
            "Epoch: 288, Iteration: 1/2, Loss: 1.4253897673842393\n",
            "Epoch: 289, Iteration: 1/2, Loss: 1.422139315736052\n",
            "Epoch: 290, Iteration: 1/2, Loss: 1.4208070781168414\n",
            "Epoch: 291, Iteration: 1/2, Loss: 1.420815906662606\n",
            "Epoch: 292, Iteration: 1/2, Loss: 1.4204825342743712\n",
            "Epoch: 293, Iteration: 1/2, Loss: 1.4221748993835903\n",
            "Epoch: 294, Iteration: 1/2, Loss: 1.4187088524932974\n",
            "Epoch: 295, Iteration: 1/2, Loss: 1.421035112847961\n",
            "Epoch: 296, Iteration: 1/2, Loss: 1.4218083064193918\n",
            "Epoch: 297, Iteration: 1/2, Loss: 1.4186792706180742\n",
            "Epoch: 298, Iteration: 1/2, Loss: 1.4224798959579754\n",
            "Epoch: 299, Iteration: 1/2, Loss: 1.4180969591370858\n",
            "Epoch: 300, Iteration: 1/2, Loss: 1.4204475148602829\n",
            "Epoch: 301, Iteration: 1/2, Loss: 1.417827871603711\n",
            "Epoch: 302, Iteration: 1/2, Loss: 1.4168285131967937\n",
            "Epoch: 303, Iteration: 1/2, Loss: 1.4193286781159926\n",
            "Epoch: 304, Iteration: 1/2, Loss: 1.416855894855066\n",
            "Epoch: 305, Iteration: 1/2, Loss: 1.4195548380654184\n",
            "Epoch: 306, Iteration: 1/2, Loss: 1.4182174010299942\n",
            "Epoch: 307, Iteration: 1/2, Loss: 1.417279221522397\n",
            "Epoch: 308, Iteration: 1/2, Loss: 1.4161751103939548\n",
            "Epoch: 309, Iteration: 1/2, Loss: 1.4208271842353668\n",
            "Epoch: 310, Iteration: 1/2, Loss: 1.4124786986302325\n",
            "Epoch: 311, Iteration: 1/2, Loss: 1.418957794149559\n",
            "Epoch: 312, Iteration: 1/2, Loss: 1.4151108746415542\n",
            "Epoch: 313, Iteration: 1/2, Loss: 1.4180638641625216\n",
            "Epoch: 314, Iteration: 1/2, Loss: 1.4150896452403496\n",
            "Epoch: 315, Iteration: 1/2, Loss: 1.4156607317698784\n",
            "Epoch: 316, Iteration: 1/2, Loss: 1.4167703933356148\n",
            "Epoch: 317, Iteration: 1/2, Loss: 1.4143023575991465\n",
            "Epoch: 318, Iteration: 1/2, Loss: 1.4175302290965832\n",
            "Epoch: 319, Iteration: 1/2, Loss: 1.4140150799730251\n",
            "Epoch: 320, Iteration: 1/2, Loss: 1.41408458415413\n",
            "Epoch: 321, Iteration: 1/2, Loss: 1.413748047376565\n",
            "Epoch: 322, Iteration: 1/2, Loss: 1.4145902539873725\n",
            "Epoch: 323, Iteration: 1/2, Loss: 1.4144039931108745\n",
            "Epoch: 324, Iteration: 1/2, Loss: 1.4140479162176802\n",
            "Epoch: 325, Iteration: 1/2, Loss: 1.4150103556024622\n",
            "Epoch: 326, Iteration: 1/2, Loss: 1.4151180937161905\n",
            "Epoch: 327, Iteration: 1/2, Loss: 1.412577124062099\n",
            "Epoch: 328, Iteration: 1/2, Loss: 1.4141368790864357\n",
            "Epoch: 329, Iteration: 1/2, Loss: 1.411556875191172\n",
            "Epoch: 330, Iteration: 1/2, Loss: 1.417207436325854\n",
            "Epoch: 331, Iteration: 1/2, Loss: 1.4108054916294928\n",
            "Epoch: 332, Iteration: 1/2, Loss: 1.4140171791425034\n",
            "Epoch: 333, Iteration: 1/2, Loss: 1.4118390258297593\n",
            "Epoch: 334, Iteration: 1/2, Loss: 1.4115653558205148\n",
            "Epoch: 335, Iteration: 1/2, Loss: 1.4128519096465615\n",
            "Epoch: 336, Iteration: 1/2, Loss: 1.4140307896007205\n",
            "Epoch: 337, Iteration: 1/2, Loss: 1.410110011522613\n",
            "Epoch: 338, Iteration: 1/2, Loss: 1.4136886484333493\n",
            "Epoch: 339, Iteration: 1/2, Loss: 1.4092881509965038\n",
            "Epoch: 340, Iteration: 1/2, Loss: 1.4131707897562595\n",
            "Epoch: 341, Iteration: 1/2, Loss: 1.4102128215507506\n",
            "Epoch: 342, Iteration: 1/2, Loss: 1.4124763580280968\n",
            "Epoch: 343, Iteration: 1/2, Loss: 1.4099733817797073\n",
            "Epoch: 344, Iteration: 1/2, Loss: 1.412002197156908\n",
            "Epoch: 345, Iteration: 1/2, Loss: 1.410672044454425\n",
            "Epoch: 346, Iteration: 1/2, Loss: 1.4097861222745078\n",
            "Epoch: 347, Iteration: 1/2, Loss: 1.4092556846569038\n",
            "Epoch: 348, Iteration: 1/2, Loss: 1.411295119382263\n",
            "Epoch: 349, Iteration: 1/2, Loss: 1.4102531826938582\n",
            "Epoch: 350, Iteration: 1/2, Loss: 1.4095650831073754\n",
            "Epoch: 351, Iteration: 1/2, Loss: 1.4095505300289348\n",
            "Epoch: 352, Iteration: 1/2, Loss: 1.4113298801962362\n",
            "Epoch: 353, Iteration: 1/2, Loss: 1.4073285129571234\n",
            "Epoch: 354, Iteration: 1/2, Loss: 1.411504595147158\n",
            "Epoch: 355, Iteration: 1/2, Loss: 1.4097271755709688\n",
            "Epoch: 356, Iteration: 1/2, Loss: 1.4077652580237898\n",
            "Epoch: 357, Iteration: 1/2, Loss: 1.4101433632869238\n",
            "Epoch: 358, Iteration: 1/2, Loss: 1.4073674408629955\n",
            "Epoch: 359, Iteration: 1/2, Loss: 1.4106519934104027\n",
            "Epoch: 360, Iteration: 1/2, Loss: 1.407979908803921\n",
            "Epoch: 361, Iteration: 1/2, Loss: 1.4082670820771253\n",
            "Epoch: 362, Iteration: 1/2, Loss: 1.4094846593540789\n",
            "Epoch: 363, Iteration: 1/2, Loss: 1.4073228349439362\n",
            "Epoch: 364, Iteration: 1/2, Loss: 1.4082706787457528\n",
            "Epoch: 365, Iteration: 1/2, Loss: 1.409642991204767\n",
            "Epoch: 366, Iteration: 1/2, Loss: 1.4074111090276087\n",
            "Epoch: 367, Iteration: 1/2, Loss: 1.4081047075311597\n",
            "Epoch: 368, Iteration: 1/2, Loss: 1.4081672728061259\n",
            "Epoch: 369, Iteration: 1/2, Loss: 1.4066237953521223\n",
            "Epoch: 370, Iteration: 1/2, Loss: 1.4064872986659887\n",
            "Epoch: 371, Iteration: 1/2, Loss: 1.408097497361409\n",
            "Epoch: 372, Iteration: 1/2, Loss: 1.4081282349352662\n",
            "Epoch: 373, Iteration: 1/2, Loss: 1.4071409748984778\n",
            "Epoch: 374, Iteration: 1/2, Loss: 1.407112171504028\n",
            "Epoch: 375, Iteration: 1/2, Loss: 1.4071984336387955\n",
            "Epoch: 376, Iteration: 1/2, Loss: 1.406389894452454\n",
            "Epoch: 377, Iteration: 1/2, Loss: 1.4072738058614889\n",
            "Epoch: 378, Iteration: 1/2, Loss: 1.4065436578321608\n",
            "Epoch: 379, Iteration: 1/2, Loss: 1.405193600329997\n",
            "Epoch: 380, Iteration: 1/2, Loss: 1.408168385341264\n",
            "Epoch: 381, Iteration: 1/2, Loss: 1.4066503525002907\n",
            "Epoch: 382, Iteration: 1/2, Loss: 1.4058384914681685\n",
            "Epoch: 383, Iteration: 1/2, Loss: 1.4054228539994593\n",
            "Epoch: 384, Iteration: 1/2, Loss: 1.4075837939873748\n",
            "Epoch: 385, Iteration: 1/2, Loss: 1.405961478202546\n",
            "Epoch: 386, Iteration: 1/2, Loss: 1.404945660247455\n",
            "Epoch: 387, Iteration: 1/2, Loss: 1.4071318466322542\n",
            "Epoch: 388, Iteration: 1/2, Loss: 1.403510379936779\n",
            "Epoch: 389, Iteration: 1/2, Loss: 1.406878639302911\n",
            "Epoch: 390, Iteration: 1/2, Loss: 1.4054683015618392\n",
            "Epoch: 391, Iteration: 1/2, Loss: 1.4049172720739\n",
            "Epoch: 392, Iteration: 1/2, Loss: 1.4050907259928895\n",
            "Epoch: 393, Iteration: 1/2, Loss: 1.4055033578145066\n",
            "Epoch: 394, Iteration: 1/2, Loss: 1.4051536822371262\n",
            "Epoch: 395, Iteration: 1/2, Loss: 1.4056362039737955\n",
            "Epoch: 396, Iteration: 1/2, Loss: 1.4028617534969559\n",
            "Epoch: 397, Iteration: 1/2, Loss: 1.4056359267782814\n",
            "Epoch: 398, Iteration: 1/2, Loss: 1.4045578628008808\n",
            "Epoch: 399, Iteration: 1/2, Loss: 1.4066170072084891\n",
            "Epoch: 400, Iteration: 1/2, Loss: 1.4031541615062744\n",
            "Epoch: 401, Iteration: 1/2, Loss: 1.4033078778182744\n",
            "Epoch: 402, Iteration: 1/2, Loss: 1.4051759827287207\n",
            "Epoch: 403, Iteration: 1/2, Loss: 1.4034011603964327\n",
            "Epoch: 404, Iteration: 1/2, Loss: 1.4043494122981484\n",
            "Epoch: 405, Iteration: 1/2, Loss: 1.404735063321831\n",
            "Epoch: 406, Iteration: 1/2, Loss: 1.4046921933883625\n",
            "Epoch: 407, Iteration: 1/2, Loss: 1.4045050833785102\n",
            "Epoch: 408, Iteration: 1/2, Loss: 1.4044638801530722\n",
            "Epoch: 409, Iteration: 1/2, Loss: 1.4025112111103195\n",
            "Epoch: 410, Iteration: 1/2, Loss: 1.4042532741204683\n",
            "Epoch: 411, Iteration: 1/2, Loss: 1.4043864280903595\n",
            "Epoch: 412, Iteration: 1/2, Loss: 1.4022412056563414\n",
            "Epoch: 413, Iteration: 1/2, Loss: 1.4029143416705017\n",
            "Epoch: 414, Iteration: 1/2, Loss: 1.4042683234441717\n",
            "Epoch: 415, Iteration: 1/2, Loss: 1.402073913151777\n",
            "Epoch: 416, Iteration: 1/2, Loss: 1.4050690020036203\n",
            "Epoch: 417, Iteration: 1/2, Loss: 1.4026183212471537\n",
            "Epoch: 418, Iteration: 1/2, Loss: 1.402299928224874\n",
            "Epoch: 419, Iteration: 1/2, Loss: 1.4037507989452536\n",
            "Epoch: 420, Iteration: 1/2, Loss: 1.4041266458712491\n",
            "Epoch: 421, Iteration: 1/2, Loss: 1.4028727704717463\n",
            "Epoch: 422, Iteration: 1/2, Loss: 1.4015837774516409\n",
            "Epoch: 423, Iteration: 1/2, Loss: 1.4032754906511262\n",
            "Epoch: 424, Iteration: 1/2, Loss: 1.401947366733229\n",
            "Epoch: 425, Iteration: 1/2, Loss: 1.4026306613986614\n",
            "Epoch: 426, Iteration: 1/2, Loss: 1.4025138166477829\n",
            "Epoch: 427, Iteration: 1/2, Loss: 1.401867585197682\n",
            "Epoch: 428, Iteration: 1/2, Loss: 1.4029138220311592\n",
            "Epoch: 429, Iteration: 1/2, Loss: 1.401930860306325\n",
            "Epoch: 430, Iteration: 1/2, Loss: 1.4026103929406868\n",
            "Epoch: 431, Iteration: 1/2, Loss: 1.402319417147544\n",
            "Epoch: 432, Iteration: 1/2, Loss: 1.401425121379749\n",
            "Epoch: 433, Iteration: 1/2, Loss: 1.4032306087148247\n",
            "Epoch: 434, Iteration: 1/2, Loss: 1.4002944686914047\n",
            "Epoch: 435, Iteration: 1/2, Loss: 1.4024188336086585\n",
            "Epoch: 436, Iteration: 1/2, Loss: 1.4031235705481588\n",
            "Epoch: 437, Iteration: 1/2, Loss: 1.4011148829154896\n",
            "Epoch: 438, Iteration: 1/2, Loss: 1.402053779114111\n",
            "Epoch: 439, Iteration: 1/2, Loss: 1.4007713551257275\n",
            "Epoch: 440, Iteration: 1/2, Loss: 1.4017238037684454\n",
            "Epoch: 441, Iteration: 1/2, Loss: 1.4023667340040968\n",
            "Epoch: 442, Iteration: 1/2, Loss: 1.399835109223854\n",
            "Epoch: 443, Iteration: 1/2, Loss: 1.4026034340224616\n",
            "Epoch: 444, Iteration: 1/2, Loss: 1.4010634091746805\n",
            "Epoch: 445, Iteration: 1/2, Loss: 1.4023926096810733\n",
            "Epoch: 446, Iteration: 1/2, Loss: 1.3997250791247837\n",
            "Epoch: 447, Iteration: 1/2, Loss: 1.4006505104402762\n",
            "Epoch: 448, Iteration: 1/2, Loss: 1.4020595655152643\n",
            "Epoch: 449, Iteration: 1/2, Loss: 1.400044526015472\n",
            "Epoch: 450, Iteration: 1/2, Loss: 1.4013063426680472\n",
            "Epoch: 451, Iteration: 1/2, Loss: 1.401528048894531\n",
            "Epoch: 452, Iteration: 1/2, Loss: 1.4008300344850804\n",
            "Epoch: 453, Iteration: 1/2, Loss: 1.4001719432970927\n",
            "Epoch: 454, Iteration: 1/2, Loss: 1.4019498494983942\n",
            "Epoch: 455, Iteration: 1/2, Loss: 1.3991206386880783\n",
            "Epoch: 456, Iteration: 1/2, Loss: 1.4015578231731438\n",
            "Epoch: 457, Iteration: 1/2, Loss: 1.4008738205839424\n",
            "Epoch: 458, Iteration: 1/2, Loss: 1.4007690409459124\n",
            "Epoch: 459, Iteration: 1/2, Loss: 1.4007729689742139\n",
            "Epoch: 460, Iteration: 1/2, Loss: 1.3985382494148526\n",
            "Epoch: 461, Iteration: 1/2, Loss: 1.4021736077286433\n",
            "Epoch: 462, Iteration: 1/2, Loss: 1.3999219938442717\n",
            "Epoch: 463, Iteration: 1/2, Loss: 1.400119443397601\n",
            "Epoch: 464, Iteration: 1/2, Loss: 1.4002946121670237\n",
            "Epoch: 465, Iteration: 1/2, Loss: 1.399079893832782\n",
            "Epoch: 466, Iteration: 1/2, Loss: 1.3996115379249718\n",
            "Epoch: 467, Iteration: 1/2, Loss: 1.4007776893260075\n",
            "Epoch: 468, Iteration: 1/2, Loss: 1.3996421484670318\n",
            "Epoch: 469, Iteration: 1/2, Loss: 1.4003430049653525\n",
            "Epoch: 470, Iteration: 1/2, Loss: 1.3993088160614051\n",
            "Epoch: 471, Iteration: 1/2, Loss: 1.4000417754292989\n",
            "Epoch: 472, Iteration: 1/2, Loss: 1.3997510398174806\n",
            "Epoch: 473, Iteration: 1/2, Loss: 1.4003069933519787\n",
            "Epoch: 474, Iteration: 1/2, Loss: 1.3992397908407601\n",
            "Epoch: 475, Iteration: 1/2, Loss: 1.3998038071921106\n",
            "Epoch: 476, Iteration: 1/2, Loss: 1.3997438386561707\n",
            "Epoch: 477, Iteration: 1/2, Loss: 1.3990948597436743\n",
            "Epoch: 478, Iteration: 1/2, Loss: 1.3994492495210138\n",
            "Epoch: 479, Iteration: 1/2, Loss: 1.400451149935606\n",
            "Epoch: 480, Iteration: 1/2, Loss: 1.3982794960713583\n",
            "Epoch: 481, Iteration: 1/2, Loss: 1.3985842136501323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 729/1000 [00:00<00:00, 1213.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 482, Iteration: 1/2, Loss: 1.4006477316249941\n",
            "Epoch: 483, Iteration: 1/2, Loss: 1.3981481817571892\n",
            "Epoch: 484, Iteration: 1/2, Loss: 1.3995838572148402\n",
            "Epoch: 485, Iteration: 1/2, Loss: 1.3997525195005953\n",
            "Epoch: 486, Iteration: 1/2, Loss: 1.3993633120547342\n",
            "Epoch: 487, Iteration: 1/2, Loss: 1.3982208115418922\n",
            "Epoch: 488, Iteration: 1/2, Loss: 1.39894255906225\n",
            "Epoch: 489, Iteration: 1/2, Loss: 1.3980203346383782\n",
            "Epoch: 490, Iteration: 1/2, Loss: 1.400207870867634\n",
            "Epoch: 491, Iteration: 1/2, Loss: 1.3984551133109653\n",
            "Epoch: 492, Iteration: 1/2, Loss: 1.398486600698405\n",
            "Epoch: 493, Iteration: 1/2, Loss: 1.3990572426047878\n",
            "Epoch: 494, Iteration: 1/2, Loss: 1.3991784955056557\n",
            "Epoch: 495, Iteration: 1/2, Loss: 1.398171761992233\n",
            "Epoch: 496, Iteration: 1/2, Loss: 1.3994080507140914\n",
            "Epoch: 497, Iteration: 1/2, Loss: 1.3968919795672017\n",
            "Epoch: 498, Iteration: 1/2, Loss: 1.3989072230364585\n",
            "Epoch: 499, Iteration: 1/2, Loss: 1.399398219222943\n",
            "Epoch: 500, Iteration: 1/2, Loss: 1.398073714832769\n",
            "Epoch: 501, Iteration: 1/2, Loss: 1.3981075236816787\n",
            "Epoch: 502, Iteration: 1/2, Loss: 1.398729529465097\n",
            "Epoch: 503, Iteration: 1/2, Loss: 1.398489196355088\n",
            "Epoch: 504, Iteration: 1/2, Loss: 1.3983976683672115\n",
            "Epoch: 505, Iteration: 1/2, Loss: 1.3981136075040586\n",
            "Epoch: 506, Iteration: 1/2, Loss: 1.39707061833102\n",
            "Epoch: 507, Iteration: 1/2, Loss: 1.3998682107998133\n",
            "Epoch: 508, Iteration: 1/2, Loss: 1.3972376063438248\n",
            "Epoch: 509, Iteration: 1/2, Loss: 1.3981182918920263\n",
            "Epoch: 510, Iteration: 1/2, Loss: 1.3979152172753557\n",
            "Epoch: 511, Iteration: 1/2, Loss: 1.3984658239302508\n",
            "Epoch: 512, Iteration: 1/2, Loss: 1.3974639126220554\n",
            "Epoch: 513, Iteration: 1/2, Loss: 1.3980048712769988\n",
            "Epoch: 514, Iteration: 1/2, Loss: 1.3978031387744156\n",
            "Epoch: 515, Iteration: 1/2, Loss: 1.3979517637977377\n",
            "Epoch: 516, Iteration: 1/2, Loss: 1.3982388477035834\n",
            "Epoch: 517, Iteration: 1/2, Loss: 1.3970455572893363\n",
            "Epoch: 518, Iteration: 1/2, Loss: 1.3974987719708718\n",
            "Epoch: 519, Iteration: 1/2, Loss: 1.3981831821241655\n",
            "Epoch: 520, Iteration: 1/2, Loss: 1.3984400597991655\n",
            "Epoch: 521, Iteration: 1/2, Loss: 1.3967717299094387\n",
            "Epoch: 522, Iteration: 1/2, Loss: 1.3971221772734301\n",
            "Epoch: 523, Iteration: 1/2, Loss: 1.3979988366635847\n",
            "Epoch: 524, Iteration: 1/2, Loss: 1.3976079939089376\n",
            "Epoch: 525, Iteration: 1/2, Loss: 1.3974613017554658\n",
            "Epoch: 526, Iteration: 1/2, Loss: 1.3975625749455491\n",
            "Epoch: 527, Iteration: 1/2, Loss: 1.397136012465572\n",
            "Epoch: 528, Iteration: 1/2, Loss: 1.39726419397845\n",
            "Epoch: 529, Iteration: 1/2, Loss: 1.3974973207094625\n",
            "Epoch: 530, Iteration: 1/2, Loss: 1.39797656055143\n",
            "Epoch: 531, Iteration: 1/2, Loss: 1.3964626322015739\n",
            "Epoch: 532, Iteration: 1/2, Loss: 1.3967615661502133\n",
            "Epoch: 533, Iteration: 1/2, Loss: 1.3971648870145883\n",
            "Epoch: 534, Iteration: 1/2, Loss: 1.3971433337822208\n",
            "Epoch: 535, Iteration: 1/2, Loss: 1.3976466172255748\n",
            "Epoch: 536, Iteration: 1/2, Loss: 1.397215088627989\n",
            "Epoch: 537, Iteration: 1/2, Loss: 1.396781332612372\n",
            "Epoch: 538, Iteration: 1/2, Loss: 1.3965883851521466\n",
            "Epoch: 539, Iteration: 1/2, Loss: 1.397743043677871\n",
            "Epoch: 540, Iteration: 1/2, Loss: 1.3965024466742664\n",
            "Epoch: 541, Iteration: 1/2, Loss: 1.3973281873802101\n",
            "Epoch: 542, Iteration: 1/2, Loss: 1.3965336278647422\n",
            "Epoch: 543, Iteration: 1/2, Loss: 1.3968086871850471\n",
            "Epoch: 544, Iteration: 1/2, Loss: 1.3968256411685733\n",
            "Epoch: 545, Iteration: 1/2, Loss: 1.3965326112849938\n",
            "Epoch: 546, Iteration: 1/2, Loss: 1.397712385511279\n",
            "Epoch: 547, Iteration: 1/2, Loss: 1.395293463693358\n",
            "Epoch: 548, Iteration: 1/2, Loss: 1.3980790616323082\n",
            "Epoch: 549, Iteration: 1/2, Loss: 1.3954801258456593\n",
            "Epoch: 550, Iteration: 1/2, Loss: 1.3966006478623396\n",
            "Epoch: 551, Iteration: 1/2, Loss: 1.3967814030829373\n",
            "Epoch: 552, Iteration: 1/2, Loss: 1.3967848419567757\n",
            "Epoch: 553, Iteration: 1/2, Loss: 1.396398751143131\n",
            "Epoch: 554, Iteration: 1/2, Loss: 1.3964942562054468\n",
            "Epoch: 555, Iteration: 1/2, Loss: 1.3958900243142294\n",
            "Epoch: 556, Iteration: 1/2, Loss: 1.3964235004980639\n",
            "Epoch: 557, Iteration: 1/2, Loss: 1.397028232828755\n",
            "Epoch: 558, Iteration: 1/2, Loss: 1.396044874357143\n",
            "Epoch: 559, Iteration: 1/2, Loss: 1.3966311936550153\n",
            "Epoch: 560, Iteration: 1/2, Loss: 1.3967276181415476\n",
            "Epoch: 561, Iteration: 1/2, Loss: 1.3958741542911755\n",
            "Epoch: 562, Iteration: 1/2, Loss: 1.3962162001326601\n",
            "Epoch: 563, Iteration: 1/2, Loss: 1.3958147403499943\n",
            "Epoch: 564, Iteration: 1/2, Loss: 1.3965972317030426\n",
            "Epoch: 565, Iteration: 1/2, Loss: 1.3963466738428196\n",
            "Epoch: 566, Iteration: 1/2, Loss: 1.3958929483023002\n",
            "Epoch: 567, Iteration: 1/2, Loss: 1.3960234223474997\n",
            "Epoch: 568, Iteration: 1/2, Loss: 1.3958372568091528\n",
            "Epoch: 569, Iteration: 1/2, Loss: 1.3960346432606914\n",
            "Epoch: 570, Iteration: 1/2, Loss: 1.396297546874282\n",
            "Epoch: 571, Iteration: 1/2, Loss: 1.3966824899109338\n",
            "Epoch: 572, Iteration: 1/2, Loss: 1.394570816704475\n",
            "Epoch: 573, Iteration: 1/2, Loss: 1.397219684190823\n",
            "Epoch: 574, Iteration: 1/2, Loss: 1.3952016887720962\n",
            "Epoch: 575, Iteration: 1/2, Loss: 1.3958647739035035\n",
            "Epoch: 576, Iteration: 1/2, Loss: 1.3963037655329866\n",
            "Epoch: 577, Iteration: 1/2, Loss: 1.3958111686570214\n",
            "Epoch: 578, Iteration: 1/2, Loss: 1.3954138953200537\n",
            "Epoch: 579, Iteration: 1/2, Loss: 1.3957342394059404\n",
            "Epoch: 580, Iteration: 1/2, Loss: 1.3964050481107488\n",
            "Epoch: 581, Iteration: 1/2, Loss: 1.3956998953239992\n",
            "Epoch: 582, Iteration: 1/2, Loss: 1.3950935713538652\n",
            "Epoch: 583, Iteration: 1/2, Loss: 1.3951734664430533\n",
            "Epoch: 584, Iteration: 1/2, Loss: 1.395713818264965\n",
            "Epoch: 585, Iteration: 1/2, Loss: 1.3956889488572186\n",
            "Epoch: 586, Iteration: 1/2, Loss: 1.3951059972718916\n",
            "Epoch: 587, Iteration: 1/2, Loss: 1.3963986428275363\n",
            "Epoch: 588, Iteration: 1/2, Loss: 1.39569792238892\n",
            "Epoch: 589, Iteration: 1/2, Loss: 1.3949170715544905\n",
            "Epoch: 590, Iteration: 1/2, Loss: 1.3960298181477488\n",
            "Epoch: 591, Iteration: 1/2, Loss: 1.3948755112409583\n",
            "Epoch: 592, Iteration: 1/2, Loss: 1.3955999410517936\n",
            "Epoch: 593, Iteration: 1/2, Loss: 1.3956779655988993\n",
            "Epoch: 594, Iteration: 1/2, Loss: 1.395360345300568\n",
            "Epoch: 595, Iteration: 1/2, Loss: 1.3956208059936968\n",
            "Epoch: 596, Iteration: 1/2, Loss: 1.3948280634655135\n",
            "Epoch: 597, Iteration: 1/2, Loss: 1.39511670497657\n",
            "Epoch: 598, Iteration: 1/2, Loss: 1.395710324458174\n",
            "Epoch: 599, Iteration: 1/2, Loss: 1.3948891117010778\n",
            "Epoch: 600, Iteration: 1/2, Loss: 1.3952782545850817\n",
            "Epoch: 601, Iteration: 1/2, Loss: 1.3954749230379564\n",
            "Epoch: 602, Iteration: 1/2, Loss: 1.3951650682301884\n",
            "Epoch: 603, Iteration: 1/2, Loss: 1.394783146377056\n",
            "Epoch: 604, Iteration: 1/2, Loss: 1.3954651428369478\n",
            "Epoch: 605, Iteration: 1/2, Loss: 1.3946591578197542\n",
            "Epoch: 606, Iteration: 1/2, Loss: 1.3955053260233106\n",
            "Epoch: 607, Iteration: 1/2, Loss: 1.394673305026075\n",
            "Epoch: 608, Iteration: 1/2, Loss: 1.3950386203084257\n",
            "Epoch: 609, Iteration: 1/2, Loss: 1.3944432100244022\n",
            "Epoch: 610, Iteration: 1/2, Loss: 1.3961361826336638\n",
            "Epoch: 611, Iteration: 1/2, Loss: 1.3942586544313333\n",
            "Epoch: 612, Iteration: 1/2, Loss: 1.395009725060025\n",
            "Epoch: 613, Iteration: 1/2, Loss: 1.3949032895362272\n",
            "Epoch: 614, Iteration: 1/2, Loss: 1.3942656291384696\n",
            "Epoch: 615, Iteration: 1/2, Loss: 1.3952872772524585\n",
            "Epoch: 616, Iteration: 1/2, Loss: 1.395360671850535\n",
            "Epoch: 617, Iteration: 1/2, Loss: 1.3950730914335336\n",
            "Epoch: 618, Iteration: 1/2, Loss: 1.3941763929481916\n",
            "Epoch: 619, Iteration: 1/2, Loss: 1.3942476850322056\n",
            "Epoch: 620, Iteration: 1/2, Loss: 1.39519738169923\n",
            "Epoch: 621, Iteration: 1/2, Loss: 1.3944631404094614\n",
            "Epoch: 622, Iteration: 1/2, Loss: 1.3948678592498203\n",
            "Epoch: 623, Iteration: 1/2, Loss: 1.3954490403018212\n",
            "Epoch: 624, Iteration: 1/2, Loss: 1.394143743454419\n",
            "Epoch: 625, Iteration: 1/2, Loss: 1.3947310263195045\n",
            "Epoch: 626, Iteration: 1/2, Loss: 1.3939193767638518\n",
            "Epoch: 627, Iteration: 1/2, Loss: 1.3957009678761194\n",
            "Epoch: 628, Iteration: 1/2, Loss: 1.3943231809972054\n",
            "Epoch: 629, Iteration: 1/2, Loss: 1.3942866887328322\n",
            "Epoch: 630, Iteration: 1/2, Loss: 1.3946248431090045\n",
            "Epoch: 631, Iteration: 1/2, Loss: 1.3945907839006981\n",
            "Epoch: 632, Iteration: 1/2, Loss: 1.3945616668471201\n",
            "Epoch: 633, Iteration: 1/2, Loss: 1.3942089956587944\n",
            "Epoch: 634, Iteration: 1/2, Loss: 1.3948791748755154\n",
            "Epoch: 635, Iteration: 1/2, Loss: 1.3939249634463482\n",
            "Epoch: 636, Iteration: 1/2, Loss: 1.3940607439820911\n",
            "Epoch: 637, Iteration: 1/2, Loss: 1.3945473043463905\n",
            "Epoch: 638, Iteration: 1/2, Loss: 1.3942797974845098\n",
            "Epoch: 639, Iteration: 1/2, Loss: 1.3945806148990822\n",
            "Epoch: 640, Iteration: 1/2, Loss: 1.3946552289727754\n",
            "Epoch: 641, Iteration: 1/2, Loss: 1.3934116469945605\n",
            "Epoch: 642, Iteration: 1/2, Loss: 1.3945197813088386\n",
            "Epoch: 643, Iteration: 1/2, Loss: 1.394578982425271\n",
            "Epoch: 644, Iteration: 1/2, Loss: 1.3939280440667667\n",
            "Epoch: 645, Iteration: 1/2, Loss: 1.3945341522210057\n",
            "Epoch: 646, Iteration: 1/2, Loss: 1.3945325804265396\n",
            "Epoch: 647, Iteration: 1/2, Loss: 1.393931364457545\n",
            "Epoch: 648, Iteration: 1/2, Loss: 1.3938466625717196\n",
            "Epoch: 649, Iteration: 1/2, Loss: 1.3943751202511883\n",
            "Epoch: 650, Iteration: 1/2, Loss: 1.394292807345304\n",
            "Epoch: 651, Iteration: 1/2, Loss: 1.3937804154089455\n",
            "Epoch: 652, Iteration: 1/2, Loss: 1.3943159694865128\n",
            "Epoch: 653, Iteration: 1/2, Loss: 1.3943080610169654\n",
            "Epoch: 654, Iteration: 1/2, Loss: 1.393250118451314\n",
            "Epoch: 655, Iteration: 1/2, Loss: 1.3950545114996267\n",
            "Epoch: 656, Iteration: 1/2, Loss: 1.3937754015273314\n",
            "Epoch: 657, Iteration: 1/2, Loss: 1.3931992511225593\n",
            "Epoch: 658, Iteration: 1/2, Loss: 1.394606463423514\n",
            "Epoch: 659, Iteration: 1/2, Loss: 1.393802056028351\n",
            "Epoch: 660, Iteration: 1/2, Loss: 1.3939327334147176\n",
            "Epoch: 661, Iteration: 1/2, Loss: 1.3942164584039012\n",
            "Epoch: 662, Iteration: 1/2, Loss: 1.3932759955974183\n",
            "Epoch: 663, Iteration: 1/2, Loss: 1.3937181670469947\n",
            "Epoch: 664, Iteration: 1/2, Loss: 1.3943388810373398\n",
            "Epoch: 665, Iteration: 1/2, Loss: 1.3941402461979342\n",
            "Epoch: 666, Iteration: 1/2, Loss: 1.3934453945356555\n",
            "Epoch: 667, Iteration: 1/2, Loss: 1.3939449391980554\n",
            "Epoch: 668, Iteration: 1/2, Loss: 1.3937097838911445\n",
            "Epoch: 669, Iteration: 1/2, Loss: 1.3935476029358789\n",
            "Epoch: 670, Iteration: 1/2, Loss: 1.3939738850560128\n",
            "Epoch: 671, Iteration: 1/2, Loss: 1.3935108091699946\n",
            "Epoch: 672, Iteration: 1/2, Loss: 1.3937916852229675\n",
            "Epoch: 673, Iteration: 1/2, Loss: 1.3933289308763315\n",
            "Epoch: 674, Iteration: 1/2, Loss: 1.393744189947904\n",
            "Epoch: 675, Iteration: 1/2, Loss: 1.393971117261679\n",
            "Epoch: 676, Iteration: 1/2, Loss: 1.3940956057175002\n",
            "Epoch: 677, Iteration: 1/2, Loss: 1.3938460674151276\n",
            "Epoch: 678, Iteration: 1/2, Loss: 1.3933927807027757\n",
            "Epoch: 679, Iteration: 1/2, Loss: 1.393002516047655\n",
            "Epoch: 680, Iteration: 1/2, Loss: 1.3940970083473387\n",
            "Epoch: 681, Iteration: 1/2, Loss: 1.3936328280034216\n",
            "Epoch: 682, Iteration: 1/2, Loss: 1.3932561118231404\n",
            "Epoch: 683, Iteration: 1/2, Loss: 1.393813376315756\n",
            "Epoch: 684, Iteration: 1/2, Loss: 1.392769783500123\n",
            "Epoch: 685, Iteration: 1/2, Loss: 1.3939321773130837\n",
            "Epoch: 686, Iteration: 1/2, Loss: 1.3939273900684044\n",
            "Epoch: 687, Iteration: 1/2, Loss: 1.3927244273617405\n",
            "Epoch: 688, Iteration: 1/2, Loss: 1.3938810234246235\n",
            "Epoch: 689, Iteration: 1/2, Loss: 1.3933475637656612\n",
            "Epoch: 690, Iteration: 1/2, Loss: 1.3933509542998772\n",
            "Epoch: 691, Iteration: 1/2, Loss: 1.3933798117542522\n",
            "Epoch: 692, Iteration: 1/2, Loss: 1.3934503749940004\n",
            "Epoch: 693, Iteration: 1/2, Loss: 1.3937916574573461\n",
            "Epoch: 694, Iteration: 1/2, Loss: 1.392354537226109\n",
            "Epoch: 695, Iteration: 1/2, Loss: 1.3942587458842173\n",
            "Epoch: 696, Iteration: 1/2, Loss: 1.3928867805739613\n",
            "Epoch: 697, Iteration: 1/2, Loss: 1.393283663718123\n",
            "Epoch: 698, Iteration: 1/2, Loss: 1.3937785456697092\n",
            "Epoch: 699, Iteration: 1/2, Loss: 1.3926354777740397\n",
            "Epoch: 700, Iteration: 1/2, Loss: 1.3938788063158942\n",
            "Epoch: 701, Iteration: 1/2, Loss: 1.3927526188172643\n",
            "Epoch: 702, Iteration: 1/2, Loss: 1.393206216268225\n",
            "Epoch: 703, Iteration: 1/2, Loss: 1.3933488750760803\n",
            "Epoch: 704, Iteration: 1/2, Loss: 1.3932583308532447\n",
            "Epoch: 705, Iteration: 1/2, Loss: 1.3934446850203668\n",
            "Epoch: 706, Iteration: 1/2, Loss: 1.3921941892927086\n",
            "Epoch: 707, Iteration: 1/2, Loss: 1.393415330609203\n",
            "Epoch: 708, Iteration: 1/2, Loss: 1.3934714757041067\n",
            "Epoch: 709, Iteration: 1/2, Loss: 1.3929686148988007\n",
            "Epoch: 710, Iteration: 1/2, Loss: 1.3933773710607182\n",
            "Epoch: 711, Iteration: 1/2, Loss: 1.3925314730428346\n",
            "Epoch: 712, Iteration: 1/2, Loss: 1.393816999642949\n",
            "Epoch: 713, Iteration: 1/2, Loss: 1.3925142549636407\n",
            "Epoch: 714, Iteration: 1/2, Loss: 1.3933732102438003\n",
            "Epoch: 715, Iteration: 1/2, Loss: 1.3926863999264971\n",
            "Epoch: 716, Iteration: 1/2, Loss: 1.3930065762384491\n",
            "Epoch: 717, Iteration: 1/2, Loss: 1.393118464976976\n",
            "Epoch: 718, Iteration: 1/2, Loss: 1.3926435437846847\n",
            "Epoch: 719, Iteration: 1/2, Loss: 1.3929648786706537\n",
            "Epoch: 720, Iteration: 1/2, Loss: 1.393353147611431\n",
            "Epoch: 721, Iteration: 1/2, Loss: 1.3923934980507835\n",
            "Epoch: 722, Iteration: 1/2, Loss: 1.3930592569302531\n",
            "Epoch: 723, Iteration: 1/2, Loss: 1.392907341265197\n",
            "Epoch: 724, Iteration: 1/2, Loss: 1.393291911189344\n",
            "Epoch: 725, Iteration: 1/2, Loss: 1.3926834150146727\n",
            "Epoch: 726, Iteration: 1/2, Loss: 1.3929188948581075\n",
            "Epoch: 727, Iteration: 1/2, Loss: 1.3929154541208488\n",
            "Epoch: 728, Iteration: 1/2, Loss: 1.3923723412018716\n",
            "Epoch: 729, Iteration: 1/2, Loss: 1.393160531958849\n",
            "Epoch: 730, Iteration: 1/2, Loss: 1.392807404258514\n",
            "Epoch: 731, Iteration: 1/2, Loss: 1.3929837369170817\n",
            "Epoch: 732, Iteration: 1/2, Loss: 1.3925886917855625\n",
            "Epoch: 733, Iteration: 1/2, Loss: 1.3928318621301656\n",
            "Epoch: 734, Iteration: 1/2, Loss: 1.3931964544687703\n",
            "Epoch: 735, Iteration: 1/2, Loss: 1.392737273687558\n",
            "Epoch: 736, Iteration: 1/2, Loss: 1.3925352344634774\n",
            "Epoch: 737, Iteration: 1/2, Loss: 1.392257235611866\n",
            "Epoch: 738, Iteration: 1/2, Loss: 1.3928947465498895\n",
            "Epoch: 739, Iteration: 1/2, Loss: 1.3930693993972947\n",
            "Epoch: 740, Iteration: 1/2, Loss: 1.3918887694770974\n",
            "Epoch: 741, Iteration: 1/2, Loss: 1.3930421914110624\n",
            "Epoch: 742, Iteration: 1/2, Loss: 1.3926428761592162\n",
            "Epoch: 743, Iteration: 1/2, Loss: 1.3925749541982744\n",
            "Epoch: 744, Iteration: 1/2, Loss: 1.3926167262029752\n",
            "Epoch: 745, Iteration: 1/2, Loss: 1.3927222664261611\n",
            "Epoch: 746, Iteration: 1/2, Loss: 1.3922762882933784\n",
            "Epoch: 747, Iteration: 1/2, Loss: 1.392957596032899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 1207.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 748, Iteration: 1/2, Loss: 1.3924451131913003\n",
            "Epoch: 749, Iteration: 1/2, Loss: 1.3923584625429877\n",
            "Epoch: 750, Iteration: 1/2, Loss: 1.3926026797890025\n",
            "Epoch: 751, Iteration: 1/2, Loss: 1.3925260616834034\n",
            "Epoch: 752, Iteration: 1/2, Loss: 1.3926402001336333\n",
            "Epoch: 753, Iteration: 1/2, Loss: 1.3929276176911825\n",
            "Epoch: 754, Iteration: 1/2, Loss: 1.3921132902498976\n",
            "Epoch: 755, Iteration: 1/2, Loss: 1.3924210013443519\n",
            "Epoch: 756, Iteration: 1/2, Loss: 1.3924616994971926\n",
            "Epoch: 757, Iteration: 1/2, Loss: 1.3924394059675378\n",
            "Epoch: 758, Iteration: 1/2, Loss: 1.3921918965496651\n",
            "Epoch: 759, Iteration: 1/2, Loss: 1.3926165204209549\n",
            "Epoch: 760, Iteration: 1/2, Loss: 1.392096559416581\n",
            "Epoch: 761, Iteration: 1/2, Loss: 1.392347489519855\n",
            "Epoch: 762, Iteration: 1/2, Loss: 1.392817513194258\n",
            "Epoch: 763, Iteration: 1/2, Loss: 1.3926156078334289\n",
            "Epoch: 764, Iteration: 1/2, Loss: 1.3919974878178816\n",
            "Epoch: 765, Iteration: 1/2, Loss: 1.3922880017961672\n",
            "Epoch: 766, Iteration: 1/2, Loss: 1.392460481288451\n",
            "Epoch: 767, Iteration: 1/2, Loss: 1.3926281779749754\n",
            "Epoch: 768, Iteration: 1/2, Loss: 1.3924360205208055\n",
            "Epoch: 769, Iteration: 1/2, Loss: 1.3921793728434677\n",
            "Epoch: 770, Iteration: 1/2, Loss: 1.3919886503762564\n",
            "Epoch: 771, Iteration: 1/2, Loss: 1.3924513401777379\n",
            "Epoch: 772, Iteration: 1/2, Loss: 1.3920914547776035\n",
            "Epoch: 773, Iteration: 1/2, Loss: 1.3918957695846197\n",
            "Epoch: 774, Iteration: 1/2, Loss: 1.3930185536897806\n",
            "Epoch: 775, Iteration: 1/2, Loss: 1.391872541334638\n",
            "Epoch: 776, Iteration: 1/2, Loss: 1.3918018769652607\n",
            "Epoch: 777, Iteration: 1/2, Loss: 1.3926741955747166\n",
            "Epoch: 778, Iteration: 1/2, Loss: 1.3921343341371046\n",
            "Epoch: 779, Iteration: 1/2, Loss: 1.3921920710022428\n",
            "Epoch: 780, Iteration: 1/2, Loss: 1.3924050167665036\n",
            "Epoch: 781, Iteration: 1/2, Loss: 1.3919284065829387\n",
            "Epoch: 782, Iteration: 1/2, Loss: 1.391968753579441\n",
            "Epoch: 783, Iteration: 1/2, Loss: 1.3923067925107016\n",
            "Epoch: 784, Iteration: 1/2, Loss: 1.391775193371699\n",
            "Epoch: 785, Iteration: 1/2, Loss: 1.392463604257579\n",
            "Epoch: 786, Iteration: 1/2, Loss: 1.3920515993830822\n",
            "Epoch: 787, Iteration: 1/2, Loss: 1.3924992353223735\n",
            "Epoch: 788, Iteration: 1/2, Loss: 1.3917324179611335\n",
            "Epoch: 789, Iteration: 1/2, Loss: 1.3916639183444977\n",
            "Epoch: 790, Iteration: 1/2, Loss: 1.3926932310792095\n",
            "Epoch: 791, Iteration: 1/2, Loss: 1.3920472777562192\n",
            "Epoch: 792, Iteration: 1/2, Loss: 1.3915686904312814\n",
            "Epoch: 793, Iteration: 1/2, Loss: 1.392606393986687\n",
            "Epoch: 794, Iteration: 1/2, Loss: 1.3916697616882243\n",
            "Epoch: 795, Iteration: 1/2, Loss: 1.3919935209755891\n",
            "Epoch: 796, Iteration: 1/2, Loss: 1.3918271912616382\n",
            "Epoch: 797, Iteration: 1/2, Loss: 1.392317543371208\n",
            "Epoch: 798, Iteration: 1/2, Loss: 1.3918057691022527\n",
            "Epoch: 799, Iteration: 1/2, Loss: 1.39156033341705\n",
            "Epoch: 800, Iteration: 1/2, Loss: 1.392173680180869\n",
            "Epoch: 801, Iteration: 1/2, Loss: 1.3923337228910588\n",
            "Epoch: 802, Iteration: 1/2, Loss: 1.3916448749448156\n",
            "Epoch: 803, Iteration: 1/2, Loss: 1.391972311744013\n",
            "Epoch: 804, Iteration: 1/2, Loss: 1.3916748753082775\n",
            "Epoch: 805, Iteration: 1/2, Loss: 1.3919561809732253\n",
            "Epoch: 806, Iteration: 1/2, Loss: 1.3924367172571133\n",
            "Epoch: 807, Iteration: 1/2, Loss: 1.3918729584213798\n",
            "Epoch: 808, Iteration: 1/2, Loss: 1.3914234802143628\n",
            "Epoch: 809, Iteration: 1/2, Loss: 1.3917407686855814\n",
            "Epoch: 810, Iteration: 1/2, Loss: 1.3917815938698004\n",
            "Epoch: 811, Iteration: 1/2, Loss: 1.3922814472257359\n",
            "Epoch: 812, Iteration: 1/2, Loss: 1.3919309108009283\n",
            "Epoch: 813, Iteration: 1/2, Loss: 1.3914734661699462\n",
            "Epoch: 814, Iteration: 1/2, Loss: 1.39185519362614\n",
            "Epoch: 815, Iteration: 1/2, Loss: 1.3917417382190378\n",
            "Epoch: 816, Iteration: 1/2, Loss: 1.3917798167731152\n",
            "Epoch: 817, Iteration: 1/2, Loss: 1.39187139184064\n",
            "Epoch: 818, Iteration: 1/2, Loss: 1.391601214784663\n",
            "Epoch: 819, Iteration: 1/2, Loss: 1.3920224839570983\n",
            "Epoch: 820, Iteration: 1/2, Loss: 1.391468611339118\n",
            "Epoch: 821, Iteration: 1/2, Loss: 1.3920000893850686\n",
            "Epoch: 822, Iteration: 1/2, Loss: 1.3914494296609528\n",
            "Epoch: 823, Iteration: 1/2, Loss: 1.391980846462261\n",
            "Epoch: 824, Iteration: 1/2, Loss: 1.3911528963193454\n",
            "Epoch: 825, Iteration: 1/2, Loss: 1.392233738010949\n",
            "Epoch: 826, Iteration: 1/2, Loss: 1.3918351972969267\n",
            "Epoch: 827, Iteration: 1/2, Loss: 1.3913474702281226\n",
            "Epoch: 828, Iteration: 1/2, Loss: 1.3919329546216777\n",
            "Epoch: 829, Iteration: 1/2, Loss: 1.3915444404980313\n",
            "Epoch: 830, Iteration: 1/2, Loss: 1.3916385044794963\n",
            "Epoch: 831, Iteration: 1/2, Loss: 1.391362956100989\n",
            "Epoch: 832, Iteration: 1/2, Loss: 1.3914573868349938\n",
            "Epoch: 833, Iteration: 1/2, Loss: 1.3915059287954064\n",
            "Epoch: 834, Iteration: 1/2, Loss: 1.3917061309226955\n",
            "Epoch: 835, Iteration: 1/2, Loss: 1.391909227898236\n",
            "Epoch: 836, Iteration: 1/2, Loss: 1.3912659560827094\n",
            "Epoch: 837, Iteration: 1/2, Loss: 1.3914702323643553\n",
            "Epoch: 838, Iteration: 1/2, Loss: 1.3915639446872836\n",
            "Epoch: 839, Iteration: 1/2, Loss: 1.3918684963534456\n",
            "Epoch: 840, Iteration: 1/2, Loss: 1.39154194529823\n",
            "Epoch: 841, Iteration: 1/2, Loss: 1.391439382129636\n",
            "Epoch: 842, Iteration: 1/2, Loss: 1.3914119828673597\n",
            "Epoch: 843, Iteration: 1/2, Loss: 1.391726829922499\n",
            "Epoch: 844, Iteration: 1/2, Loss: 1.391195383630024\n",
            "Epoch: 845, Iteration: 1/2, Loss: 1.3921707395701484\n",
            "Epoch: 846, Iteration: 1/2, Loss: 1.390769604222808\n",
            "Epoch: 847, Iteration: 1/2, Loss: 1.3921497348946898\n",
            "Epoch: 848, Iteration: 1/2, Loss: 1.3909471719186701\n",
            "Epoch: 849, Iteration: 1/2, Loss: 1.391676287149806\n",
            "Epoch: 850, Iteration: 1/2, Loss: 1.3918023894942262\n",
            "Epoch: 851, Iteration: 1/2, Loss: 1.3908786464618106\n",
            "Epoch: 852, Iteration: 1/2, Loss: 1.3919499432398024\n",
            "Epoch: 853, Iteration: 1/2, Loss: 1.391469527310944\n",
            "Epoch: 854, Iteration: 1/2, Loss: 1.391056084835435\n",
            "Epoch: 855, Iteration: 1/2, Loss: 1.3917631947230347\n",
            "Epoch: 856, Iteration: 1/2, Loss: 1.3908384187575928\n",
            "Epoch: 857, Iteration: 1/2, Loss: 1.391445155135876\n",
            "Epoch: 858, Iteration: 1/2, Loss: 1.3915754130012594\n",
            "Epoch: 859, Iteration: 1/2, Loss: 1.3915292202405958\n",
            "Epoch: 860, Iteration: 1/2, Loss: 1.391007166532622\n",
            "Epoch: 861, Iteration: 1/2, Loss: 1.3916474473102898\n",
            "Epoch: 862, Iteration: 1/2, Loss: 1.391243237215945\n",
            "Epoch: 863, Iteration: 1/2, Loss: 1.391084295601468\n",
            "Epoch: 864, Iteration: 1/2, Loss: 1.3912276314734249\n",
            "Epoch: 865, Iteration: 1/2, Loss: 1.3914172533339761\n",
            "Epoch: 866, Iteration: 1/2, Loss: 1.391461772407081\n",
            "Epoch: 867, Iteration: 1/2, Loss: 1.39134474019827\n",
            "Epoch: 868, Iteration: 1/2, Loss: 1.391498977599995\n",
            "Epoch: 869, Iteration: 1/2, Loss: 1.3912807006109702\n",
            "Epoch: 870, Iteration: 1/2, Loss: 1.390928817078074\n",
            "Epoch: 871, Iteration: 1/2, Loss: 1.3913609816306551\n",
            "Epoch: 872, Iteration: 1/2, Loss: 1.391259138684671\n",
            "Epoch: 873, Iteration: 1/2, Loss: 1.391497151451706\n",
            "Epoch: 874, Iteration: 1/2, Loss: 1.3910850406356499\n",
            "Epoch: 875, Iteration: 1/2, Loss: 1.391481100167825\n",
            "Epoch: 876, Iteration: 1/2, Loss: 1.3909317932482645\n",
            "Epoch: 877, Iteration: 1/2, Loss: 1.3913529451294626\n",
            "Epoch: 878, Iteration: 1/2, Loss: 1.390970695276668\n",
            "Epoch: 879, Iteration: 1/2, Loss: 1.3912413411705848\n",
            "Epoch: 880, Iteration: 1/2, Loss: 1.391284056665865\n",
            "Epoch: 881, Iteration: 1/2, Loss: 1.391427996380615\n",
            "Epoch: 882, Iteration: 1/2, Loss: 1.390971879658721\n",
            "Epoch: 883, Iteration: 1/2, Loss: 1.3910274944129792\n",
            "Epoch: 884, Iteration: 1/2, Loss: 1.3910042545088066\n",
            "Epoch: 885, Iteration: 1/2, Loss: 1.3913450203362494\n",
            "Epoch: 886, Iteration: 1/2, Loss: 1.3910963544562325\n",
            "Epoch: 887, Iteration: 1/2, Loss: 1.3911322480819752\n",
            "Epoch: 888, Iteration: 1/2, Loss: 1.391121486420311\n",
            "Epoch: 889, Iteration: 1/2, Loss: 1.3909667925012608\n",
            "Epoch: 890, Iteration: 1/2, Loss: 1.3913523891105197\n",
            "Epoch: 891, Iteration: 1/2, Loss: 1.3910063342682828\n",
            "Epoch: 892, Iteration: 1/2, Loss: 1.3909979557952759\n",
            "Epoch: 893, Iteration: 1/2, Loss: 1.3912178593388629\n",
            "Epoch: 894, Iteration: 1/2, Loss: 1.390884720761783\n",
            "Epoch: 895, Iteration: 1/2, Loss: 1.3912611993679933\n",
            "Epoch: 896, Iteration: 1/2, Loss: 1.3910165114544166\n",
            "Epoch: 897, Iteration: 1/2, Loss: 1.3909027473297\n",
            "Epoch: 898, Iteration: 1/2, Loss: 1.391242601437757\n",
            "Epoch: 899, Iteration: 1/2, Loss: 1.391221965164232\n",
            "Epoch: 900, Iteration: 1/2, Loss: 1.3908424965314272\n",
            "Epoch: 901, Iteration: 1/2, Loss: 1.3908790524107684\n",
            "Epoch: 902, Iteration: 1/2, Loss: 1.3911446374829106\n",
            "Epoch: 903, Iteration: 1/2, Loss: 1.3910963208057279\n",
            "Epoch: 904, Iteration: 1/2, Loss: 1.3907177584481818\n",
            "Epoch: 905, Iteration: 1/2, Loss: 1.3909900997363953\n",
            "Epoch: 906, Iteration: 1/2, Loss: 1.390982390323741\n",
            "Epoch: 907, Iteration: 1/2, Loss: 1.3911213323097775\n",
            "Epoch: 908, Iteration: 1/2, Loss: 1.3911982410894015\n",
            "Epoch: 909, Iteration: 1/2, Loss: 1.3907275751333144\n",
            "Epoch: 910, Iteration: 1/2, Loss: 1.3908050564104586\n",
            "Epoch: 911, Iteration: 1/2, Loss: 1.3913206757949097\n",
            "Epoch: 912, Iteration: 1/2, Loss: 1.390703180614818\n",
            "Epoch: 913, Iteration: 1/2, Loss: 1.3909728525184573\n",
            "Epoch: 914, Iteration: 1/2, Loss: 1.3909657638421074\n",
            "Epoch: 915, Iteration: 1/2, Loss: 1.3911873383496436\n",
            "Epoch: 916, Iteration: 1/2, Loss: 1.3907215181294212\n",
            "Epoch: 917, Iteration: 1/2, Loss: 1.3906799397783438\n",
            "Epoch: 918, Iteration: 1/2, Loss: 1.390788502438312\n",
            "Epoch: 919, Iteration: 1/2, Loss: 1.3908842946046995\n",
            "Epoch: 920, Iteration: 1/2, Loss: 1.3907908123906512\n",
            "Epoch: 921, Iteration: 1/2, Loss: 1.3910573676296214\n",
            "Epoch: 922, Iteration: 1/2, Loss: 1.3907595561502417\n",
            "Epoch: 923, Iteration: 1/2, Loss: 1.391086608584803\n",
            "Epoch: 924, Iteration: 1/2, Loss: 1.3907994347707473\n",
            "Epoch: 925, Iteration: 1/2, Loss: 1.3908836207880155\n",
            "Epoch: 926, Iteration: 1/2, Loss: 1.3907936851356681\n",
            "Epoch: 927, Iteration: 1/2, Loss: 1.390821885537964\n",
            "Epoch: 928, Iteration: 1/2, Loss: 1.3908590523830093\n",
            "Epoch: 929, Iteration: 1/2, Loss: 1.3907694007847098\n",
            "Epoch: 930, Iteration: 1/2, Loss: 1.3908472954791629\n",
            "Epoch: 931, Iteration: 1/2, Loss: 1.3909344025263826\n",
            "Epoch: 932, Iteration: 1/2, Loss: 1.3906071387850614\n",
            "Epoch: 933, Iteration: 1/2, Loss: 1.3905974967590478\n",
            "Epoch: 934, Iteration: 1/2, Loss: 1.3907784689207205\n",
            "Epoch: 935, Iteration: 1/2, Loss: 1.3909932392749749\n",
            "Epoch: 936, Iteration: 1/2, Loss: 1.3908977997496355\n",
            "Epoch: 937, Iteration: 1/2, Loss: 1.3905760357917485\n",
            "Epoch: 938, Iteration: 1/2, Loss: 1.3910069444222497\n",
            "Epoch: 939, Iteration: 1/2, Loss: 1.3903937577898091\n",
            "Epoch: 940, Iteration: 1/2, Loss: 1.3908561556805978\n",
            "Epoch: 941, Iteration: 1/2, Loss: 1.3909473015102432\n",
            "Epoch: 942, Iteration: 1/2, Loss: 1.3904974824378291\n",
            "Epoch: 943, Iteration: 1/2, Loss: 1.3904899627173635\n",
            "Epoch: 944, Iteration: 1/2, Loss: 1.3905833672665862\n",
            "Epoch: 945, Iteration: 1/2, Loss: 1.3911267791115973\n",
            "Epoch: 946, Iteration: 1/2, Loss: 1.3903883521201292\n",
            "Epoch: 947, Iteration: 1/2, Loss: 1.3908602727638244\n",
            "Epoch: 948, Iteration: 1/2, Loss: 1.390804219579248\n",
            "Epoch: 949, Iteration: 1/2, Loss: 1.3907235851233484\n",
            "Epoch: 950, Iteration: 1/2, Loss: 1.390186497223128\n",
            "Epoch: 951, Iteration: 1/2, Loss: 1.3907817041318973\n",
            "Epoch: 952, Iteration: 1/2, Loss: 1.3905276649004081\n",
            "Epoch: 953, Iteration: 1/2, Loss: 1.3910217673264862\n",
            "Epoch: 954, Iteration: 1/2, Loss: 1.3903834227737701\n",
            "Epoch: 955, Iteration: 1/2, Loss: 1.3905479738292406\n",
            "Epoch: 956, Iteration: 1/2, Loss: 1.390877049225227\n",
            "Epoch: 957, Iteration: 1/2, Loss: 1.390541977135406\n",
            "Epoch: 958, Iteration: 1/2, Loss: 1.3909451621164421\n",
            "Epoch: 959, Iteration: 1/2, Loss: 1.39042685696005\n",
            "Epoch: 960, Iteration: 1/2, Loss: 1.390813320998169\n",
            "Epoch: 961, Iteration: 1/2, Loss: 1.390255676647534\n",
            "Epoch: 962, Iteration: 1/2, Loss: 1.3906238713913055\n",
            "Epoch: 963, Iteration: 1/2, Loss: 1.3907533197206692\n",
            "Epoch: 964, Iteration: 1/2, Loss: 1.3903187889235544\n",
            "Epoch: 965, Iteration: 1/2, Loss: 1.3906021056723952\n",
            "Epoch: 966, Iteration: 1/2, Loss: 1.390719988488633\n",
            "Epoch: 967, Iteration: 1/2, Loss: 1.390177698206324\n",
            "Epoch: 968, Iteration: 1/2, Loss: 1.3907573871136605\n",
            "Epoch: 969, Iteration: 1/2, Loss: 1.3907522631112834\n",
            "Epoch: 970, Iteration: 1/2, Loss: 1.3903586525107738\n",
            "Epoch: 971, Iteration: 1/2, Loss: 1.3902761677844333\n",
            "Epoch: 972, Iteration: 1/2, Loss: 1.3906934756640628\n",
            "Epoch: 973, Iteration: 1/2, Loss: 1.3906296400910687\n",
            "Epoch: 974, Iteration: 1/2, Loss: 1.390506965135128\n",
            "Epoch: 975, Iteration: 1/2, Loss: 1.3907096349665051\n",
            "Epoch: 976, Iteration: 1/2, Loss: 1.3899971436906584\n",
            "Epoch: 977, Iteration: 1/2, Loss: 1.3904884938582893\n",
            "Epoch: 978, Iteration: 1/2, Loss: 1.3907305322340981\n",
            "Epoch: 979, Iteration: 1/2, Loss: 1.3907217715569993\n",
            "Epoch: 980, Iteration: 1/2, Loss: 1.3904690144819702\n",
            "Epoch: 981, Iteration: 1/2, Loss: 1.3902169640112139\n",
            "Epoch: 982, Iteration: 1/2, Loss: 1.3904596882538374\n",
            "Epoch: 983, Iteration: 1/2, Loss: 1.3905646555829219\n",
            "Epoch: 984, Iteration: 1/2, Loss: 1.3903251653657138\n",
            "Epoch: 985, Iteration: 1/2, Loss: 1.39027278288259\n",
            "Epoch: 986, Iteration: 1/2, Loss: 1.390599217381324\n",
            "Epoch: 987, Iteration: 1/2, Loss: 1.3904258961405604\n",
            "Epoch: 988, Iteration: 1/2, Loss: 1.3905362479493304\n",
            "Epoch: 989, Iteration: 1/2, Loss: 1.3904133347370538\n",
            "Epoch: 990, Iteration: 1/2, Loss: 1.39032915312489\n",
            "Epoch: 991, Iteration: 1/2, Loss: 1.3901982724689945\n",
            "Epoch: 992, Iteration: 1/2, Loss: 1.390564376317736\n",
            "Epoch: 993, Iteration: 1/2, Loss: 1.390343764933768\n",
            "Epoch: 994, Iteration: 1/2, Loss: 1.3905410954632371\n",
            "Epoch: 995, Iteration: 1/2, Loss: 1.390222823496221\n",
            "Epoch: 996, Iteration: 1/2, Loss: 1.390368624475885\n",
            "Epoch: 997, Iteration: 1/2, Loss: 1.3903335631845355\n",
            "Epoch: 998, Iteration: 1/2, Loss: 1.3904323838092885\n",
            "Epoch: 999, Iteration: 1/2, Loss: 1.3902812189971845\n",
            "Epoch: 1000, Iteration: 1/2, Loss: 1.3905864937511168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "nIoblH2M73OB",
        "outputId": "dbab7c8c-cf5b-4f2c-c2be-a27fb743b332"
      },
      "source": [
        "trainer.plot()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9YE782Q73OD",
        "outputId": "e9adf90f-8791-4123-daa6-488376354fe4"
      },
      "source": [
        "# check skip-gram results\n",
        "word_vecs = skip_gram.word_vecs\n",
        "for word_id, word in idx2word.items():\n",
        "  print(word, word_vecs[word_id])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing [ 0.21788023  1.503287   -2.3098319  -0.72451067 -0.44643888]\n",
            "natural [ 2.068127    1.7463601  -0.4107805  -0.64879006  1.4489607 ]\n",
            "now [-1.115022   -0.2141742   1.807777    0.65493166 -1.9406198 ]\n",
            ". [ 0.01067478 -0.02115559  0.0028832   0.00228551  0.00550201]\n",
            "studying [-0.8394605 -1.9036411 -1.5561676  0.5159814  1.1317595]\n",
            "i [-0.02095236 -0.00288974  0.00872457 -0.01154802 -0.00090183]\n",
            "am [ 1.9471995  -0.06526342  1.9812766  -0.9485421   0.30893594]\n",
            "language [-2.5698915  -0.6065119   0.49892664  1.6573792   0.4026616 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vY124jQG5uD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}