{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCIvz30AOj-H"
      },
      "source": [
        "# 作業 : 觀察機器翻譯 ATTENTION 內容 \n",
        "- 仔細地觀察機器翻譯 ATTENTION 結果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usP1_X7qOv6F"
      },
      "source": [
        "# [作業目標]\n",
        "- 透過視覺化 注意力 attention 層 了解attention 的作用方式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWGLeN9BOxEF"
      },
      "source": [
        "# [作業重點]\n",
        "- 透過視覺化 注意力 attention 層 了解attention 的作用方式\n",
        "- 原則上只要之前的訓練有跑完，這邊的程式可以執行成功最後只要觀察結果就好\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIBD2Nn-OI-1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQoAR8K-RyHd",
        "outputId": "c2861f72-14a7-4327-bd0b-bdf856497fd6"
      },
      "source": [
        "# Colab 進行matplotlib繪圖時顯示繁體中文\n",
        "# 下載字體並命名taipei_sans_tc_beta.ttf，移至指定路徑\n",
        "!wget -O taipei_sans_tc_beta.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
        "!mv taipei_sans_tc_beta.ttf /usr/local/lib/python3.6/dist-packages/matplotlib//mpl-data/fonts/ttf\n",
        "\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import matplotlib.pyplot as plt \n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "import matplotlib.ticker as ticker\n",
        "# 自定義字體變數\n",
        "myfont = FontProperties(fname=r'/usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data/fonts/ttf/taipei_sans_tc_beta.ttf')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-04 07:59:54--  https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.193.139, 172.217.193.113, 172.217.193.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.193.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6jqlri1gs98147jamhuf9ruc7ecf33pi/1612425525000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_ [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-04 07:59:56--  https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6jqlri1gs98147jamhuf9ruc7ecf33pi/1612425525000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)... 172.217.193.132, 2607:f8b0:400c:c03::84\n",
            "Connecting to doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)|172.217.193.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-font-ttf]\n",
            "Saving to: ‘taipei_sans_tc_beta.ttf’\n",
            "\n",
            "taipei_sans_tc_beta     [  <=>               ]  19.70M  45.0MB/s    in 0.4s    \n",
            "\n",
            "2021-02-04 07:59:57 (45.0 MB/s) - ‘taipei_sans_tc_beta.ttf’ saved [20659344]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrbNQhl5R2MP",
        "outputId": "cf695486-51c1-482d-a510-12f6ef153c86"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5fQ9FY-EdLg",
        "outputId": "a0b43d28-d1a1-42e2-d4ba-b994d7dc9b94"
      },
      "source": [
        "!wget https://www.manythings.org/anki/cmn-eng.zip\n",
        "!unzip cmn-eng.zip\n",
        "!mkdir data\n",
        "!mv cmn.txt ./data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-04 08:04:33--  https://www.manythings.org/anki/cmn-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3031::6815:37de, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1062383 (1.0M) [application/zip]\n",
            "Saving to: ‘cmn-eng.zip’\n",
            "\n",
            "cmn-eng.zip         100%[===================>]   1.01M  4.57MB/s    in 0.2s    \n",
            "\n",
            "2021-02-04 08:04:33 (4.57 MB/s) - ‘cmn-eng.zip’ saved [1062383/1062383]\n",
            "\n",
            "Archive:  cmn-eng.zip\n",
            "  inflating: cmn.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFjVGqegR5U8",
        "outputId": "041e8b1e-8677-43f2-f729-289075844dc2"
      },
      "source": [
        "data_dir = './data/'\n",
        "lines = open(data_dir + 'cmn.txt' , encoding='utf-8').read().strip().split('\\n')\n",
        "trnslt_pairs = [[s for s in l.split('\\t')] for l in lines ]\n",
        "print (\"Sample: \" , trnslt_pairs[1000][0:2] )\n",
        "print (\"Total records:\" , len(trnslt_pairs))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample:  ['He was drowned.', '他被淹死了。']\n",
            "Total records: 24360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwps_d8UGGZ_",
        "outputId": "e671371f-bf1c-4ceb-9f70-1e08b4a61040"
      },
      "source": [
        "trnslt_pairs = [pair for pair in trnslt_pairs if pair[1][0] in ['我','你','他','她']]\n",
        "print (\"Total records after filtering :\" , len(trnslt_pairs))\n",
        "train, test = train_test_split(trnslt_pairs, test_size=0.08)\n",
        "train, val = train_test_split(train, test_size=0.09)\n",
        "print (\"training data:{} , develop data: {} , testing data: {}\".format(len(train),len(val),len(test)))\n",
        "    \n",
        "def write_csv(trn_data, file_path ):\n",
        "    with open(file_path ,'w', newline='', encoding='utf-8') as fout:\n",
        "        writer = csv.writer (fout)\n",
        "        for itm in trn_data: \n",
        "            writer.writerow ([itm[0],itm[1]])\n",
        "            \n",
        "file_path = data_dir + 'train.csv'\n",
        "write_csv(train, file_path )\n",
        "\n",
        "file_path = data_dir + 'val.csv'\n",
        "write_csv(val, file_path )\n",
        "    \n",
        "file_path = data_dir + 'test.csv'\n",
        "write_csv(test, file_path )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total records after filtering : 13420\n",
            "training data:11234 , develop data: 1112 , testing data: 1074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl-KIM-nSA-H",
        "outputId": "cd20366c-7158-4f45-c1a6-96a12b050ac4"
      },
      "source": [
        "# 下載 spacy 的英文模型 幫我們做tokenize\n",
        "model_dir =  './drive/MyDrive/0204_model/'\n",
        "\n",
        "spacy_eng = spacy.load('en_core_web_sm')\n",
        "def tokenize_eng(text):\n",
        "  #清除不需要的字符\n",
        "  text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "  return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "TRG = Field(tokenize = tokenize_eng, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_cmn(text):\n",
        "  #去掉非中文字元\n",
        "  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
        "  text = regex.sub(' ', text)\n",
        "\n",
        "  return [word for word in text if word.strip()]\n",
        "    \n",
        "\n",
        "SRC = Field(tokenize = tokenize_cmn, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            include_lengths = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset, dev_dataset, test_dataset = TabularDataset.splits(\n",
        "    path = data_dir , format = 'csv', skip_header = True,\n",
        "    train='train.csv', validation='val.csv', test='test.csv',\n",
        "    fields=[\n",
        "        ('trg', TRG),\n",
        "        ('src', SRC)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# 讀取之前儲存的 vocabulary\n",
        "SRC.vocab = torch.load(model_dir + 'SRC_vocab.pt')\n",
        "TRG.vocab = torch.load(model_dir + 'TRG_vocab.pt')\n",
        "\n",
        "print (\"中文語料的字元表長度: \" , len(SRC.vocab) , \", 英文的字元表長度: \" ,len(TRG.vocab))\n",
        "print (\"Sample SRC:\", test_dataset[0].src , \"TRG:\", test_dataset[0].trg)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dataset, dev_dataset, test_dataset), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.src),\n",
        "     device = device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "中文語料的字元表長度:  2683 , 英文的字元表長度:  4082\n",
            "Sample SRC: ['她', '忘', '了', '寄', '信'] TRG: ['she', 'forgot', 'to', 'mail', 'the', 'letter', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYoqlKcrq2Z_"
      },
      "source": [
        "# 模型主體 和前面範例程式一樣\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj3ZTHDMSGOF"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    # hidden bz , dec_hid_dim\n",
        "    # encoder_outputs src len, bz , enc_hid_dim x 2\n",
        "    # mask bz , src len\n",
        "    \n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "\n",
        "    hidden = hidden.unsqueeze(1) \n",
        "    # hidden unsqueeze bz , 1 , dec_hid_dim\n",
        "\n",
        "    attention = torch.matmul( hidden , encoder_outputs.permute(1, 2, 0)   )\n",
        "    # attention bz, 1 , src len\n",
        "    \n",
        "    attention = attention.squeeze(1)\n",
        "    # squeeze bz , src len\n",
        "\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "    return F.softmax(attention, dim = 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNzgZqHcS2CX"
      },
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        # 雙向 ＧＲＵ encoder \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        \n",
        "        #src shape [src len, batch size]\n",
        "        #src_len shape [batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded shape [src len, batch size, emb dim]\n",
        "                \n",
        "        # 使用pack_padded_sequence 來壓縮序列        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "\n",
        "        # 使用 pad_packed_sequence 用來展開序列成原本形狀的      \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "            \n",
        "        #outputs shape [src len, batch size, hid dim * num directions]\n",
        "        #hidden shape [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden 堆疊 [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs 是最後一層 \n",
        "        \n",
        "        #hidden [-2, :, : ] 是最後一層 forwards RNN \n",
        "        #hidden [-1, :, : ] 是最後一層 backwards RNN\n",
        "        \n",
        "        # hidden 是最後再過一層 dense layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        # 單向 ＧＲＵ decoder \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "             \n",
        "        #input shape [batch size]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        #encoder_outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #mask shape [batch size, src len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input shape [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded shape [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "                \n",
        "        #a shape [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a shape [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs shape [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted shape [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted shape [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input shape [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output shape [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden shape [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output shape [1, batch size, dec hid dim]\n",
        "        #hidden shape [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction shape [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
        "\n",
        "class Seq2SeqATTN(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "                    \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        #mask = [batch size, src len]\n",
        "                \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
        "            #  and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlITuy6WS47j"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50mOv4N-S8LJ"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW2KIxhxrMGf"
      },
      "source": [
        "# 建立模型和重要參數 請保持和前面訓練時一樣"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybIY0kKGS_gI",
        "outputId": "17b54df2-094d-426c-f378-377d8c02552f"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 256 # 注意 encoder hidden layer 設定 必須為 dec 的一半 \n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "LEARNING_RATE = 0.002\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2SeqATTN(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "\n",
        "def initial_mdl_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(initial_mdl_weights)\n",
        "print (\"模型全部參數量: {:10,d} \".format(sum(p.numel() for p in model.parameters())))\n",
        "model"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "模型全部參數量:  9,982,194 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqATTN(\n",
              "  (encoder): RNNEncoder(\n",
              "    (embedding): Embedding(2683, 256)\n",
              "    (rnn): GRU(256, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): RNNDecoder(\n",
              "    (attention): Attention()\n",
              "    (embedding): Embedding(4082, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=4082, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOpjxQJmTDYU"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, src_len.cpu(), trg, 0) #turn off teacher forcing\n",
        "            \n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukg9t_iOTHlG",
        "outputId": "fdda02ad-b50b-4673-f6d6-32636c95bdc8"
      },
      "source": [
        "model_dir =  './drive/MyDrive/0204_model/'\n",
        "model.load_state_dict(torch.load(model_dir + 'best-model.pt'))\n",
        "#model.load_state_dict(torch.load(model_dir + 'model-7.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 2.252 | Test PPL:   9.510 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-caE1Y1TL5p"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    #if isinstance(sentence, str):\n",
        "    #    nlp = spacy_en = spacy.load('en_core_web_sm')\n",
        "    #    tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
        "    #else:\n",
        "    #    tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "        \n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jhh_5_SYYLT"
      },
      "source": [
        "def display_attention(sentence, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    #fontdict = {\"fontproperties\": zhfont}\n",
        "    \n",
        "    #ax.set_xticks(range(max(max_len_tar, len(predicted_seq))))\n",
        "    #ax.set_xlim(-0.5, max_len_tar -1.5)\n",
        "    \n",
        "    #ax.set_yticks(range(len(sentence) + 2))\n",
        "    #ax.set_xticklabels([subword_encoder_zh.decode([i]) for i in predicted_seq \n",
        "    #                    if i < subword_encoder_zh.vocab_size], \n",
        "    #                   fontdict=fontdict, fontsize=18)\n",
        "    \n",
        "    #plt.rcParams[\"font.family\"]=\"sans-serif\"\n",
        "    #plt.rcParams['font.sans-serif']=['STSong'] #用来正常显示中文标签\n",
        "    \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                       rotation=45 , fontproperties=myfont) #, fontdict=fontdict)\n",
        "    ax.set_yticklabels(['']+translation, fontproperties=myfont) # , fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4n7915Mrcs1"
      },
      "source": [
        "# 作業重點\n",
        "## 請選擇一個好的翻譯結果\n",
        "## 將其 ATTENTION 視覺化 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRYXjqgvYb-E",
        "outputId": "2dbd2a0f-b485-4b37-e040-cdb4638ff885"
      },
      "source": [
        "# 請在這邊自行調整 sample index \n",
        "# 觀察不同句子的 ATTENTION 結果\n",
        "example_idx =499\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['src']\n",
        "trg = vars(train_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['他', '的', '辦', '公', '室', '在', '八', '樓']\n",
            "trg = ['his', 'office', 'is', 'on', 'the', 'eighth', 'floor', '.']\n",
            "predicted trg = ['he', 'was', 'on', 'the', 'coast', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cdlptGJrsfv"
      },
      "source": [
        "# 請觀察翻譯文 和被翻譯文的語意對應"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "H8f8csPSYfkU",
        "outputId": "e6001730-4d17-4f28-fa26-d75a27ca772f"
      },
      "source": [
        "print (\"\".join(src ))\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "他的辦公室在八樓\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ake_IJd2YiG3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}